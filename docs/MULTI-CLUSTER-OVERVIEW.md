# Multi-Cluster Kubernetes Architecture Overview

**Status:** Architecture Finalized âœ… | Implementation: Ready to Begin
**Date:** 2025-10-15
**Architecture:** 2 Talos Clusters (Infra + Apps) with Linkerd Service Mesh + Cilium CNI

---

## ğŸ“‹ Quick Links

| Document | Purpose | Status |
|----------|---------|--------|
| [Best Technical Solution 2025](./best-technical-solution-2025.md) | Why Linkerd wins on technical merit | âœ… Complete |
| [Cluster Mesh Comparison](./cluster-mesh-comparison-2025.md) | Comprehensive solution analysis | âœ… Complete |
| [Linkerd Implementation Plan](./linkerd-implementation-plan.md) | 6-week detailed rollout | ğŸ”„ In Progress |
| [Talos Multi-Cluster Bootstrap](./talos-multi-cluster-bootstrap.md) | Step-by-step conversion guide | âœ… Ready |
| [Architecture Decision Record](./architecture-decision-record.md) | All architectural decisions | ğŸ”„ Updating |
| [Helper Scripts](../scripts/README.md) | Automation scripts | âœ… Ready |

---

## ğŸ¯ Architecture Overview

### Current State
- **Single 6-node cluster** (`k8s`)
- All nodes: `10.25.11.11-16` in controlplane mode
- PodCIDR: `10.244.0.0/16`
- ServiceCIDR: `10.245.0.0/16`

### Target State
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    10.25.11.0/24 Network                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Infra Cluster          â”‚       Apps Cluster            â”‚
â”‚   infra.k8s.monosense.io    â”‚   apps.k8s.monosense.io       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Nodes: 10.25.11.11-13       â”‚ Nodes: 10.25.11.14-16         â”‚
â”‚   - infra-01 (11)           â”‚   - apps-01 (14)              â”‚
â”‚   - infra-02 (12)           â”‚   - apps-02 (15)              â”‚
â”‚   - infra-03 (13)           â”‚   - apps-03 (16)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PodCIDR: 10.244.0.0/16      â”‚ PodCIDR: 10.246.0.0/16        â”‚
â”‚ ServiceCIDR: 10.245.0.0/16  â”‚ ServiceCIDR: 10.247.0.0/16    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LoadBalancer: .100-.149     â”‚ LoadBalancer: .150-.199       â”‚
â”‚                             â”‚                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚  Cilium CNI (Layer 1)   â”‚ â”‚ â”‚  Cilium CNI (Layer 1)   â”‚   â”‚
â”‚ â”‚  eBPF networking        â”‚ â”‚ â”‚  eBPF networking        â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚  Linkerd Mesh (Layer 2) â”‚ â”‚ â”‚  Linkerd Mesh (Layer 2) â”‚   â”‚
â”‚ â”‚  L7 observability/mTLS  â”‚ â”‚ â”‚  L7 observability/mTLS  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
              Linkerd Multi-Cluster Service Mirroring
                  (mTLS-encrypted cross-cluster)
```

### Architecture Layers

**Layer 1 - Networking (Cilium CNI):**
- eBPF-based pod networking and routing
- NetworkPolicies for L3/L4 security
- BGP integration with Juniper SRX320
- Gateway API for ingress

**Layer 2 - Service Mesh (Linkerd):**
- Automatic mTLS between all services
- L7 observability and distributed tracing
- Service mirroring for cross-cluster access
- Minimal resource overhead (Rust micro-proxies)

**Cross-Cluster Communication:**
- Linkerd gateway with automatic service discovery
- Encrypted tunnels over mTLS (safe over public internet)
- Service mirroring: `postgres-rw-infra.svc.cluster.local` on apps cluster
- Automatic failover and locality-aware routing

---

## ğŸ—ï¸ Infra Cluster Workloads

**Platform Services** (Always Running):
- **Storage:** Rook Ceph (3x1TB NVMe), OpenEBS LocalPV (512GB NVMe)
- **Databases:** CloudNativePG (Postgres 15) with MinIO object storage backups, Dragonfly (Redis-compatible cache)
- **Security:** Keycloak, cert-manager, external-secrets (1Password)
- **Observability:** Victoria Metrics Stack, Fluent-bit, Victoria Logs, Kromgo
- **Networking:** External-DNS (Cloudflare), Cloudflare Tunnel
- **GitOps:** FluxCD, Tofu-Controller
- **CI/CD:** GitHub Actions runners

**Resource Allocation:**
- CPU: 36 cores (22.5 platform + 13.5 buffer)
- RAM: 192GB (66GB platform + 44GB workloads + 82GB buffer)
- Storage: 3TB NVMe (Ceph) + 1.5TB NVMe (OpenEBS)

---

## ğŸ“¦ Apps Cluster Workloads

**Application Services** (User-Facing):
- GitLab (with Runners)
- Harbor Registry
- Mattermost Team Chat
- Future application workloads

**Platform Integration:** GitLab consumes the infra-hosted CloudNativePG and Dragonfly services through Linkerd service mirroring with mTLS encryption, keeping state centralized while workloads remain isolated. Services are automatically mirrored as `<service>-infra.svc.cluster.local` on the apps cluster.

**Storage Strategy:**
- Local Rook-Ceph (3Ã—1TB NVMe, replica 3) for stateful workloads
- OpenEBS LocalPV (512GB NVMe hostPath) for cache/ephemeral volumes

---

## ğŸš€ Getting Started

### Prerequisites Checklist
- [ ] Talos Linux 1.11.2 installed on all 6 nodes
- [ ] Node IPs: 10.25.11.11-16 configured
- [ ] DNS access (Cloudflare or internal)
- [ ] 1Password CLI configured
- [ ] Required tools: `talosctl`, `kubectl`, `linkerd`, `op`, `yq`, `minijinja-cli`

### Quick Start (30-minute version)

```bash
# 1. Run conversion script (5 min)
./scripts/convert-to-multicluster.sh

# 2. Generate secrets (2 min)
talosctl gen secrets -o /tmp/infra-secrets.yaml
talosctl gen secrets -o /tmp/apps-secrets.yaml

# 3. Import to 1Password (3 min)
./scripts/extract-talos-secrets.sh /tmp/infra-secrets.yaml infra
# Copy and run the 'op item create' command
./scripts/extract-talos-secrets.sh /tmp/apps-secrets.yaml apps
# Copy and run the 'op item create' command

# 4. Update configs (10 min)
# - Update talos/machineconfig.yaml.j2 (see bootstrap guide Step 1)
# - Update .taskfiles/talos/Taskfile.yaml (see bootstrap guide Step 2)
# - Add DNS records: infra.k8s.monosense.io, apps.k8s.monosense.io

# 5. Deploy infra cluster (5 min)
task talos:apply-node NODE=10.25.11.11 CLUSTER=infra MACHINE_TYPE=controlplane
task talos:apply-node NODE=10.25.11.12 CLUSTER=infra MACHINE_TYPE=controlplane
task talos:apply-node NODE=10.25.11.13 CLUSTER=infra MACHINE_TYPE=controlplane
talosctl bootstrap --nodes 10.25.11.11

# 6. Deploy apps cluster (5 min)
task talos:apply-node NODE=10.25.11.14 CLUSTER=apps MACHINE_TYPE=controlplane
task talos:apply-node NODE=10.25.11.15 CLUSTER=apps MACHINE_TYPE=controlplane
task talos:apply-node NODE=10.25.11.16 CLUSTER=apps MACHINE_TYPE=controlplane
talosctl config context apps --nodes 10.25.11.14,10.25.11.15,10.25.11.16
talosctl bootstrap --nodes 10.25.11.14
```

**Full detailed guide:** [Talos Multi-Cluster Bootstrap](./talos-multi-cluster-bootstrap.md)

---

## ğŸ“… Implementation Phases

### Phase 1: Foundation (Week 1-2)
- âœ… Architecture finalized (Linkerd + Cilium)
- ğŸ”² Bootstrap both Talos clusters
- ğŸ”² Deploy Cilium CNI on both clusters
- ğŸ”² Bootstrap FluxCD
- ğŸ”² Deploy core monitoring (Victoria Metrics)

### Phase 2: Service Mesh & Observability (Week 3)
- ğŸ”² Install Linkerd on both clusters
- ğŸ”² Configure multi-cluster service mirroring
- ğŸ”² Deploy Jaeger for distributed tracing
- ğŸ”² Deploy Linkerd Viz dashboard
- ğŸ”² Validate cross-cluster connectivity

### Phase 3: Storage & Backup (Week 4)
- ğŸ”² Deploy Rook Ceph on both clusters
- ğŸ”² Deploy OpenEBS on both clusters
- ğŸ”² Configure VolSync backup
- ğŸ”² Deploy Velero

### Phase 4: Platform Services (Week 5)
- ğŸ”² Deploy CloudNativePG (infra cluster)
- ğŸ”² Deploy Dragonfly cache (infra cluster)
- ğŸ”² Deploy MinIO (infra cluster)
- ğŸ”² Deploy Keycloak (infra cluster)
- ğŸ”² Export services via Linkerd mirroring

### Phase 5: Security & Networking (Week 6)
- ğŸ”² Configure Cloudflare Tunnel
- ğŸ”² Implement Cilium NetworkPolicies
- ğŸ”² Configure Linkerd authorization policies
- ğŸ”² Deploy cert-manager + certificates
- ğŸ”² Deploy GitHub Actions runners

### Phase 6: Applications & Validation (Week 7)
- ğŸ”² Mesh GitLab namespace (apps cluster)
- ğŸ”² Deploy GitLab (using mirrored DB services)
- ğŸ”² Mesh Harbor namespace
- ğŸ”² Deploy Harbor
- ğŸ”² Mesh Mattermost namespace
- ğŸ”² Deploy Mattermost
- ğŸ”² Validate distributed tracing end-to-end
- ğŸ”² DR testing
- ğŸ”² Performance testing
- ğŸ”² Go-live

**Total Timeline:** 7 weeks (reduced from 10 weeks due to Linkerd simplicity)

**Full detailed plan:** [Linkerd Implementation Plan](./linkerd-implementation-plan.md)

---

## ğŸ”‘ Key Architecture Decisions

| ID | Decision | Rationale |
|----|----------|-----------|
| ADR-001 | All databases on infra cluster | Centralized management, better resource utilization |
| ADR-002 | Cloudflare Tunnel for external access | Zero-trust, no exposed IPs |
| ADR-004 | Velero Day 1 implementation | Cluster-level backup critical from start |
| ADR-006 | 6-hour RPO for VolSync | Balance between protection and overhead |
| ADR-009 | Actions runners on infra cluster | Shared resource, always available |
| ADR-012 | Keycloak for authentication | SSO across all services |
| ADR-013 | External-DNS with Cloudflare | Automatic DNS management |

**Full ADR:** [Architecture Decision Record](./architecture-decision-record.md)

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| OS | Talos Linux | 1.11.2 | Immutable Kubernetes OS |
| Kubernetes | K8s | 1.34.1 | Container orchestration |
| CNI | Cilium | 1.18+ | eBPF networking, NetworkPolicies |
| Service Mesh | Linkerd | 2.18+ | L7 observability, mTLS, multi-cluster |
| GitOps | FluxCD | Latest | Continuous deployment |
| Storage | Rook Ceph | Latest | Block/Object/File storage |
| Storage | OpenEBS LocalPV | Latest | Local persistent volumes |
| Database | CloudNativePG | Latest | PostgreSQL operator |
| Cache | Dragonfly | Latest | Redis-compatible in-memory cache |
| Object Storage | MinIO | Latest | S3-compatible object storage |
| Secrets | External Secrets | Latest | 1Password integration |
| Metrics | Victoria Metrics | Latest | Metrics aggregation |
| Logs | Victoria Logs | Latest | Log aggregation |
| Tracing | Jaeger | Latest | Distributed tracing (OTLP) |
| Auth | Keycloak | Latest | SSO/Identity provider |
| CI/CD | GitHub Actions | N/A | Pipeline automation |
| Backup | VolSync | Latest | PVC replication |
| Backup | Velero | Latest | Cluster backup |

### Architecture Update (October 15, 2025)

**Service Mesh Decision:**
- **Selected:** Linkerd + Cilium CNI (best technical solution)
- **Why:** Highest performance (+8% latency vs +166% Istio sidecar), lowest resource overhead (1/5th of Istio), excellent L7 observability with OTLP/Jaeger, production-proven multi-cluster (GA for 5 years)
- **Alternative considered:** Istio ambient (rejected due to alpha multi-cluster status)

**Repository Structure:**
- Flux multi-cluster layout: `kubernetes/infrastructure` â†’ `kubernetes/workloads` â†’ `kubernetes/clusters/<name>`
- Cilium configured with BGP control plane, Gateway API, `kubeProxyReplacement: false` for Linkerd compatibility
- Linkerd deployed on both clusters with automatic mTLS and service mirroring

**Observability Stack:**
- **Centralized on infra cluster:** Jaeger (traces), Victoria Metrics (metrics), Victoria Logs (logs), Grafana (dashboards)
- **Apps cluster:** Linkerd proxies send OTLP traces to Jaeger collector on infra
- **Cross-cluster:** Service mirroring provides transparent access to infra services

**Storage:**
- Rook-Ceph: 1TB NVMe drives, replica 3, `rook-ceph-block` StorageClass
- OpenEBS: 512GB NVMe hostPath, `openebs-local-nvme` StorageClass

**Multi-Cluster Integration:**
- GitLab (apps cluster) connects to CloudNativePG/Dragonfly/MinIO (infra cluster) via Linkerd service mirroring
- Services automatically available as `<service>-infra.svc.cluster.local` on apps cluster
- mTLS-encrypted cross-cluster communication via Linkerd gateway

---

## ğŸ“Š Resource Planning

### Infra Cluster (3 nodes)
```
Total Capacity:
  CPU: 36 cores (3 Ã— 12 cores)
  RAM: 192 GB (3 Ã— 64 GB)
  Storage: 4.5 TB NVMe (3 Ã— 1.5 TB)

Infrastructure Services:
â”œâ”€ Cilium CNI: ~400m CPU, ~1 GB RAM
â”œâ”€ Linkerd (control plane): ~500m CPU, ~1.1 GB RAM
â”œâ”€ Linkerd (data plane, ~50 pods): ~1 CPU, ~2 GB RAM
â”œâ”€ Jaeger: ~300m CPU, ~768 MB RAM
â”œâ”€ Victoria Metrics: ~1 CPU, ~4 GB RAM
â”œâ”€ FluxCD: ~200m CPU, ~512 MB RAM
â””â”€ Total Infrastructure: ~3.5 CPU, ~9.5 GB RAM

Platform Workloads:
â”œâ”€ CloudNativePG: ~1 CPU, ~4 GB RAM
â”œâ”€ Dragonfly: ~500m CPU, ~2 GB RAM
â”œâ”€ MinIO: ~500m CPU, ~2 GB RAM
â”œâ”€ Keycloak: ~1 CPU, ~2 GB RAM
â”œâ”€ Rook Ceph: ~2 CPU, ~6 GB RAM
â”œâ”€ Actions Runners: ~2 CPU, ~4 GB RAM
â””â”€ Total Platform: ~7 CPU, ~20 GB RAM

Summary:
â”œâ”€ Total Used: ~10.5 CPU, ~29.5 GB RAM (29% CPU, 15% RAM)
â”œâ”€ Available: ~25.5 CPU, ~162.5 GB RAM
â””â”€ Headroom: Excellent

Storage:
  Ceph: 3 TB usable (3Ã—1TB, replica 3)
  OpenEBS: 1.5 TB (3Ã—512GB)
```

### Apps Cluster (3 nodes)
```
Total Capacity:
  CPU: 36 cores (3 Ã— 12 cores)
  RAM: 192 GB (3 Ã— 64 GB)
  Storage: 4.5 TB NVMe (3 Ã— 1.5 TB)

Infrastructure Services:
â”œâ”€ Cilium CNI: ~400m CPU, ~1 GB RAM
â”œâ”€ Linkerd (control plane): ~500m CPU, ~1.1 GB RAM
â”œâ”€ Linkerd (data plane, ~100 pods): ~2 CPU, ~4 GB RAM
â”œâ”€ FluxCD: ~200m CPU, ~512 MB RAM
â””â”€ Total Infrastructure: ~3.1 CPU, ~6.6 GB RAM

Application Workloads (estimated):
â”œâ”€ GitLab: ~4 CPU, ~16 GB RAM
â”œâ”€ Harbor: ~2 CPU, ~8 GB RAM
â”œâ”€ Mattermost: ~2 CPU, ~4 GB RAM
â”œâ”€ Rook Ceph: ~2 CPU, ~6 GB RAM
â””â”€ Total Applications: ~10 CPU, ~34 GB RAM

Summary:
â”œâ”€ Total Used: ~13 CPU, ~40.6 GB RAM (36% CPU, 21% RAM)
â”œâ”€ Available: ~23 CPU, ~151.4 GB RAM
â””â”€ Headroom: Excellent for future growth

Storage:
  Ceph: 3 TB usable (3Ã—1TB, replica 3)
  OpenEBS: 1.5 TB (3Ã—512GB)
```

### Linkerd Resource Impact

**Compared to Istio Sidecar:**
```
Apps Cluster (100 meshed pods):
â”œâ”€ Linkerd: ~2 CPU cores, ~4 GB RAM
â”œâ”€ Istio Sidecar: ~6 CPU cores, ~8 GB RAM
â””â”€ Savings: 4 CPU cores (11%), 4 GB RAM

This saves $200-300/month equivalent in cloud costs!
```

---

## ğŸ” Security Architecture

**Zero-Trust Foundation:**
- **Service-to-Service mTLS:** Linkerd automatic mTLS for all meshed pods (enabled by default)
- **Cross-Cluster mTLS:** Linkerd gateway encrypts all cross-cluster traffic
- **Certificate Management:** Linkerd automatic cert rotation (24h lifetime)
- **Network Policies:** Cilium NetworkPolicy for L3/L4 isolation
- **Authorization Policies:** Linkerd L7 authorization policies per workload

**Identity & Access:**
- **Secret Management:** 1Password via External Secrets Operator
- **Authentication:** Keycloak SSO for all user-facing services
- **External Access:** Cloudflare Tunnel (zero-trust, no exposed IPs)
- **TLS Certificates:** cert-manager with Let's Encrypt
- **Pod Security:** Gradual enforcement (audit â†’ warn â†’ enforce)

**Data Protection:**
- **In-Transit:** Automatic mTLS via Linkerd (no configuration needed)
- **At-Rest:** Rook Ceph encryption, Age-encrypted backups
- **Cross-Cluster:** mTLS-encrypted gateway tunnels
- **Secret Rotation:** Automatic (Linkerd certs), Manual quarterly (1Password)

**Compliance:**
- **CNCF Graduated Project:** Linkerd (industry-standard security)
- **Zero CVEs:** Linkerd core proxy (Rust-based, minimal attack surface)
- **Audit Logs:** Linkerd Viz + Jaeger for request-level tracing

---

## ğŸ“ˆ Monitoring & Observability

**Three Pillars of Observability (Centralized on Infra Cluster):**

### 1. Metrics (Victoria Metrics + Linkerd)

**Linkerd Golden Metrics (Automatic):**
- âœ… **Success Rate:** Percentage of successful requests
- âœ… **Request Rate (RPS):** Requests per second
- âœ… **Latency (p50, p95, p99):** Response time distribution
- âœ… **Per-Service, Per-Route:** Granular visibility

**Infrastructure Metrics:**
- Victoria Metrics (centralized on infra)
- VMAgent on apps cluster (remote-write to infra)
- Prometheus scraping of Linkerd proxies
- Pre-configured Grafana dashboards (Linkerd + custom)

**CLI Access:**
```bash
# Real-time golden metrics
linkerd viz stat deploy -n gitlab

# Top resources by RPS
linkerd viz top deploy -n gitlab
```

### 2. Logs (Fluent-bit + Victoria Logs)

- Centralized Victoria Logs (infra cluster)
- Fluent-bit agents on all pods
- Structured JSON logging
- Correlated with trace IDs from Linkerd

### 3. Distributed Tracing (Jaeger + Linkerd)

**Linkerd â†’ OpenTelemetry â†’ Jaeger Pipeline:**
- âœ… **Automatic context propagation** (W3C Trace Context)
- âœ… **Request-level tracing** across services and clusters
- âœ… **Cross-cluster visibility:** GitLab (apps) â†’ Postgres (infra) traces
- âœ… **Service dependency map** in Jaeger UI
- âœ… **Performance bottleneck identification**

**Trace Flow:**
```
App Request â†’ Linkerd Proxy â†’ OTLP Collector â†’ Jaeger (infra)
                â†“
         Automatic span creation
         Context propagation
         Cross-cluster correlation
```

### Real-Time Service Visibility (Linkerd Viz)

**Linkerd Viz Dashboard Features:**
- âœ… Live service topology graph
- âœ… Real-time success rates and latencies
- âœ… TLS status indicators (mTLS verification)
- âœ… Traffic split visualization
- âœ… Tap: Live request inspection (like HTTP Wireshark)

**Access:**
```bash
# Launch dashboard
linkerd viz dashboard

# Live traffic inspection
linkerd viz tap deploy/gitlab --to deploy/postgres-rw-infra
# See LIVE requests with headers, methods, status codes!
```

### Alerting (Alertmanager + Linkerd)

- AlertManager (infra cluster)
- Linkerd SLO-based alerts (success rate, latency)
- Critical alerts to GitHub issues
- Multi-channel support (Slack, PagerDuty, etc.)
- Silence operator for maintenance

### Health Checks

- **Gatus:** External endpoint monitoring
- **Linkerd Health:** Service mesh health checks
- **Multi-cluster validation:** Cross-cluster connectivity monitoring

---

## ğŸš¨ Disaster Recovery

### Backup Strategy
- **VolSync:** PVC replication, 6-hour RPO
- **Velero:** Cluster resources, weekly full backup
- **Destinations:** MinIO on Synology NAS
- **Encryption:** Age encryption for all backups

### Recovery Scenarios
1. **Pod Failure:** Kubernetes self-healing (< 1 min)
2. **Node Failure:** Pod rescheduling (< 5 min)
3. **Cluster Failure:** Restore from backups (< 4 hours)
4. **Complete Disaster:** Rebuild from GitOps repo (< 8 hours)

### Testing Schedule
- **Tier 1 (Pod/PVC):** Weekly
- **Tier 2 (Node/Cluster):** Monthly
- **Tier 3 (Complete DR):** Quarterly

---

## ğŸ“š Documentation Structure

```
docs/
â”œâ”€â”€ MULTI-CLUSTER-OVERVIEW.md          # This file - high-level overview
â”œâ”€â”€ talos-multi-cluster-bootstrap.md   # Step-by-step conversion guide
â”œâ”€â”€ architecture-decision-record.md    # All 20 ADR decisions
â”œâ”€â”€ technical-deep-dive.md             # Production configurations
â”œâ”€â”€ implementation-timeline.md         # 10-week rollout plan
â””â”€â”€ brainstorming-session-results.md   # Initial research & analysis

scripts/
â”œâ”€â”€ README.md                          # Helper scripts documentation
â”œâ”€â”€ convert-to-multicluster.sh         # Automate node reorganization
â””â”€â”€ extract-talos-secrets.sh           # Extract secrets for 1Password
```

---

## ğŸ“ Learning Resources

### Linkerd (Service Mesh)
- [Official Documentation](https://linkerd.io/2.18/overview/)
- [Getting Started (10 min)](https://linkerd.io/2.18/getting-started/)
- [Multi-cluster Guide](https://linkerd.io/2.18/tasks/multicluster/)
- [Distributed Tracing](https://linkerd.io/2.18/tasks/distributed-tracing/)
- [Linkerd + Cilium Integration](https://buoyant.io/blog/kubernetes-network-policies-with-cilium-and-linkerd)
- [Practical Linkerd eBook](https://buoyant.io/books) (Free)
- [Linkerd Slack Community](https://slack.linkerd.io)

### Talos Linux
- [Official Docs](https://www.talos.dev/latest/)
- [Multi-Cluster Setup](https://www.talos.dev/latest/advanced/multi-cluster/)
- [CNI Configuration](https://www.talos.dev/latest/kubernetes-guides/network/)

### Cilium (CNI)
- [Installation Guide](https://docs.cilium.io/en/stable/installation/)
- [BGP Control Plane](https://docs.cilium.io/en/stable/network/bgp-control-plane/)
- [Gateway API](https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/)
- [NetworkPolicies](https://docs.cilium.io/en/stable/security/policy/)

### FluxCD (GitOps)
- [Multi-Cluster GitOps](https://fluxcd.io/flux/use-cases/multi-cluster/)
- [Helm Controller](https://fluxcd.io/flux/components/helm/)
- [Best Practices](https://fluxcd.io/flux/guides/repository-structure/)

### Observability
- [Jaeger Documentation](https://www.jaegertracing.io/docs/latest/)
- [OpenTelemetry](https://opentelemetry.io/docs/)
- [Victoria Metrics](https://docs.victoriametrics.com/)
- [Grafana Dashboards](https://grafana.com/grafana/dashboards/)

---

## âœ… Pre-Flight Checklist

Before starting implementation:

**Documentation:**
- [ ] Read talos-multi-cluster-bootstrap.md completely
- [ ] Review architecture-decision-record.md
- [ ] Understand implementation-timeline.md phases
- [ ] Review helper scripts in scripts/README.md

**Infrastructure:**
- [ ] All 6 nodes powered on and accessible
- [ ] Network connectivity verified (10.25.11.11-16)
- [ ] DNS provider access (Cloudflare or internal)
- [ ] 1Password vault access
- [ ] Juniper SRX320 accessible for BGP configuration

**Tools Installed:**
- [ ] talosctl (v1.11.2+)
- [ ] kubectl (v1.34.1+)
- [ ] linkerd CLI (v2.18+) - `curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh`
- [ ] op (1Password CLI)
- [ ] yq (YAML processor)
- [ ] minijinja-cli (template processor)
- [ ] flux CLI
- [ ] helm
- [ ] helmfile

**Git Repository:**
- [ ] All changes committed
- [ ] Backup of current configs
- [ ] Branch created for multi-cluster work

---

## ğŸ¤ Support & Troubleshooting

### Common Issues
- **Node not bootstrapping:** Check talosctl logs and etcd health
- **PodCIDR overlap:** Verify different CIDRs in both clusters
- **1Password injection failing:** Verify op CLI authentication

### Troubleshooting Guides
- See "Troubleshooting" section in talos-multi-cluster-bootstrap.md
- See "Testing Checklist" in implementation-timeline.md
- See "Disaster Recovery" section in technical-deep-dive.md

### Getting Help
- Review documentation in `docs/` directory
- Check scripts output and error messages
- Validate prerequisites before proceeding

---

## ğŸ“ Notes

- This is a **greenfield** multi-cluster deployment plan
- Total estimated implementation time: **7 weeks** (reduced from 10 weeks)
- **Service mesh:** Linkerd (best technical solution: performance + observability + simplicity)
- **CNI:** Cilium (eBPF networking, NetworkPolicies, BGP)
- Conversion scripts automate ~70% of manual work
- All architectural decisions documented and ratified
- Ready to begin Phase 1: Foundation

---

## ğŸš€ Quick Start Commands

```bash
# Install Linkerd CLI
curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh
export PATH=$PATH:$HOME/.linkerd2/bin

# Verify prerequisites
linkerd version
kubectl version
talosctl version

# Test Linkerd on existing cluster (optional)
linkerd check --pre
linkerd install --crds | kubectl apply -f -
linkerd install | kubectl apply -f -
linkerd check

# View live dashboard
linkerd viz install | kubectl apply -f -
linkerd viz dashboard
```

---

*Multi-Cluster Overview - v2.0*
*Last Updated: 2025-10-15*
*Architecture: Linkerd Service Mesh + Cilium CNI*
