# üèóÔ∏è Multi-Cluster GitOps Architecture (v4)

<div align="center">

![Status](https://img.shields.io/badge/Status-Implementing-blue)
![Owner](https://img.shields.io/badge/Owner-Platform_Engineering-orange)
![Last Updated](https://img.shields.io/badge/Updated-2025--10--21-green)
![Version](https://img.shields.io/badge/Version-4.0-purple)

**Modern cloud-native platform built on Talos Linux ‚Ä¢ GitOps-powered ‚Ä¢ Multi-cluster**

</div>

---

## üìã Table of Contents

1. [üìñ Context & Goals](#1-context--goals)
2. [ü™ñ Talos Linux 1.11.2](#-talos-linux-1112---api-managed-kubernetes-operating-system)
3. [‚ò∏Ô∏è Kubernetes 1.34.1](#Ô∏è-kubernetes-1341---of-wind--will-release)
4. [üéØ Design Principles](#3-design-principles)
5. [üåê Target Topology](#4-target-topology)
6. [üìÅ Repository Layout](#5-repository-layout)
7. [‚öôÔ∏è Flux Model & Convergence](#6-flux-model--convergence)
8. [üöÄ Bootstrap Architecture](#7-bootstrap-architecture)
9. [üîß Cluster Settings & Substitution](#8-cluster-settings--substitution)
10. [üîê Secrets Management](#9-secrets-management)
11. [üåê Networking (Cilium)](#10-networking-cilium)
12. [üíæ Storage](#11-storage)
13. [üìä Observability](#12-observability)
14. [üîÑ CI/CD & Policy](#13-cicd--policy)
15. [üè¢ Multi-Tenancy](#14-multi-tenancy)
16. [üîß Operations & Runbooks](#15-operations--runbooks)
17. [üìÖ Phased Implementation](#16-phased-implementation)
18. [üìù Decisions & Rationale](#17-decisions--rationale)
19. [‚ö†Ô∏è Risks & Mitigations](#18-risks--mitigations)
20. [‚úÖ Acceptance Criteria](#19-acceptance-criteria--metrics)
21. [üõ†Ô∏è Workloads & Versions](#20-workloads--versions)
22. [üîß CRD & API Standardization Matrix](#21-crd--api-standardization-matrix)
23. [üîó Cilium ClusterMesh + SPIRE](#22-cilium-clustermesh--spire)
24. [üõ°Ô∏è Security & Network Policy](#23-security--network-policy-baseline)
25. [üîÑ Multi-Cluster Mesh Options](#24-multi-cluster-mesh-options---decision-matrix)
26. [üåê DNS, ExternalDNS, and Cloudflare Tunnel](#25-dns-externaldns-and-cloudflare-tunnel)

---

## 1. üìñ Context & Goals

> **üèõÔ∏è Overview**
>
> We run two Talos-based Kubernetes clusters ‚Äì an infrastructure cluster ("infra") and an application cluster ("apps"). We manage the platform via GitOps using Flux, Helm, and Kustomize. This document defines the target multi-cluster architecture, repository structure, bootstrap approach, security posture, and phased rollout plan.

---

## ü™ñ Talos Linux 1.11.2 - API-Managed Kubernetes Operating System

> **üîí Secure by Design ‚Ä¢ Immutable ‚Ä¢ API-First**

Talos Linux 1.11.2 serves as the foundation of our Kubernetes infrastructure, providing an immutable, minimal, and secure operating system designed specifically for running Kubernetes clusters.

### üèóÔ∏è Core Architecture Principles

| Principle | Implementation | Benefits |
|---|---|---|
| **üîí Immutable Infrastructure** | Ephemeral root filesystem mounted read-only | Eliminates configuration drift, ensures consistency |
| **üîß API-First Management** | All operations via Talos API (no SSH access) | Programmable infrastructure, audit trail |
| **‚ö° Minimal Attack Surface** | Essential services only (no shells, package managers) | Reduced security vulnerabilities |
| **üîÑ Automated Lifecycle** | Zero-downtime upgrades via A/B nodes | Simplified operations, predictable maintenance |

### üöÄ Key Features & Capabilities

#### **Core Platform Features**
- **Linux Kernel 6.12.48**: Latest stable kernel with enhanced security patches
- **Kubernetes 1.34.1 Integration**: Native support with validated compatibility
- **containerd Runtime**: Optimized container runtime with CRI compliance
- **etcd v3.5.18**: Distributed key-value store for cluster state
- **runc 1.3.1**: OCI-compliant container runtime

#### **Enhanced Security Model**
```yaml
# Talos security posture
security:
  kernel_hardening: true
  selinux_enforcing: true
  read_only_filesystem: true
  minimal_attack_surface: true
  api_authentication: mTLS
  disk_encryption: AES-256-GCM
```

#### **Advanced Networking Capabilities**
- **eBPF Support**: Native eBPF integration for Cilium and advanced networking
- **WireGuard Integration**: Built-in VPN functionality for secure node communication
- **Multi-Interface Support**: Complex network topology management
- **BGP Peering**: Native BGP support for network fabric integration

#### **Platform Management APIs**

| API Endpoint | Purpose | Usage |
|---|---|---|
| **`/machineconfig`** | Node configuration management | Apply configuration changes |
| **`/health`** | System health monitoring | Validate node status |
| **`/reset`** | Factory reset operations | Secure node decommissioning |
| **`/resources`** | Resource monitoring | System resource visibility |
| **`/osinfo`** | Platform information | Version and capabilities |

### üîß Configuration Management

#### **Production Machine Configuration Structure**

Our production Talos configurations demonstrate advanced networking and performance optimizations:

```yaml
# Infrastructure Cluster Control Plane (10.25.11.11)
---
machine:
  network:
    hostname: infra-01
    interfaces:
      - interface: bond0
        bond:
          deviceSelectors: [{ hardwareAddr: "f8:f2:1e:20:57:*", driver: i40e }]
          mode: 802.3ad
          xmitHashPolicy: layer3+4
          lacpRate: fast
          miimon: 100
        dhcp: false
        mtu: 9000
        addresses: [10.25.11.11/24]
        routes: [{ network: "0.0.0.0/0", gateway: "10.25.11.1" }]
        vlans:
          - vlanId: 2512
            mtu: 1500
            dhcp: false
            dhcpOptions:
              routeMetric: 4096
---
apiVersion: v1alpha1
kind: EthernetConfig
name: enp1s0f0np0
rings:
  rx: 8160
  tx: 8160
---
apiVersion: v1alpha1
kind: EthernetConfig
name: enp1s0f1np1
rings:
  rx: 8160
  tx: 8160
```

#### **Advanced Network Configuration Features**

| Feature | Implementation | Benefits |
|---|---|---|
| **üîó LACP Bonding** | 802.3ad with layer3+4 hashing | High availability and link aggregation |
| **üì° High MTU** | 9000 bytes on bond interface | Jumbo frames for better throughput |
| **üè∑Ô∏è VLAN Segmentation** | VLAN 2512 for management traffic | Network isolation and organization |
| **‚ö° Performance Tuning** | 8160 ring buffers per interface | Optimized network I/O performance |

#### **Production Performance Optimizations**

Our custom schematic includes targeted performance optimizations:

```yaml
# talos/schematic.yaml - Production performance profile
---
customization:
  extraKernelArgs:
    # Hardware compatibility
    - module_blacklist=e1000e

    # Performance optimizations (trade-offs for speed)
    - -init_on_alloc                      # Faster memory allocation
    - -selinux                            # Reduced security overhead
    - apparmor=0                          # Disabled for performance
    - init_on_alloc=0                     # Faster memory allocation
    - init_on_free=0                      # Faster memory deallocation
    - mitigations=off                     # Disable security mitigations
    - security=none                       # Maximum performance
    - talos.auditd.disabled=1             # Disable audit daemon

  systemExtensions:
    officialExtensions:
      - siderolabs/intel-ucode            # Intel microcode updates
      - siderolabs/i915-ucode            # Intel GPU microcode
      - siderolabs/iscsi-tools           # iSCSI support
      - siderolabs/nfsrahead             # NFS performance optimization
```

#### **Cluster Topology**

| Cluster | Node IPs | Hardware | Network Config |
|---|---|---|---|
| **üè≠ Infra** | 10.25.11.11-13 | Intel i40e NICs | Bond0 + VLAN 2512 |
| **üéØ Apps** | 10.25.11.14-16 | Intel i40e NICs | Bond0 + VLAN 2512 |

#### **Configuration Management Strategy**

**üîÑ Configuration Hierarchy:**
1. **Base Configuration**: Network interfaces and bonding
2. **Cluster Settings**: Hostname and IP assignments
3. **Performance Profile**: Kernel optimizations via schematic
4. **System Extensions**: Hardware-specific modules

**üõ°Ô∏è Security vs Performance Trade-offs:**
Our production configuration prioritizes performance for bare-metal deployment:
- **Disabled Security Features**: SELinux, AppArmor, memory initialization
- **Mitigation Disabled**: CPU vulnerability mitigations for maximum performance
- **Audit Disabled**: Audit daemon disabled to reduce overhead
- **Justification**: Controlled environment with physical security and network isolation

#### **API Configuration Updates**
- **Dynamic Reconfiguration**: Live configuration updates without reboot
- **Validation Framework**: Pre-flight validation of configuration changes
- **Rollback Capabilities**: Automatic rollback on configuration failures
- **Multi-Node Coordination**: Synchronized configuration across cluster nodes

### üöÄ Performance Optimizations

#### **Boot Time Performance**
- **Cold Start**: < 60 seconds to Kubernetes API availability
- **Memory Efficiency**: < 200MiB base memory footprint
- **Storage Optimization**: Minimal disk usage with efficient layering

#### **Runtime Performance**
- **Container Startup**: < 2 seconds average container start time
- **Network Latency**: < 1ms intra-node network latency
- **I/O Performance**: NVMe-optimized storage operations

### üîí Security Enhancements

#### **Zero-Trust Architecture**
```yaml
# Talos security configuration
security:
  # Kernel security modules
  selinux: enforcing
  apparmor: enabled

  # API security
  api_certificates:
    validity_period: "8760h"  # 1 year
    renewal_window: "720h"     # 30 days before expiry

  # Disk encryption
  disk_encryption:
    method: luks2
    cipher: aes-xts-plain64
    key_size: 512
```

#### **Compliance & Standards**
- **NIST 800-53**: Security controls compliance
- **CIS Benchmarks**: Kubernetes hardening guidelines
- **FedRAMP**: Government security requirements alignment
- **SOC 2 Type II**: Security and availability controls

### üõ†Ô∏è Operational Excellence

#### **Maintenance & Updates**
- **Zero-Downtime Upgrades**: Rolling updates with health validation
- **Automated Rollback**: Failure detection and automatic recovery
- **Health Monitoring**: Comprehensive system health metrics
- **Log Aggregation**: Centralized logging for troubleshooting

#### **Debugging & Observability**
```bash
# Talos debugging commands
talosctl health                    # Cluster health status
talosctl version                   # Version information
talosctl get resources             # System resource overview
talosctl logs --follow             # Real-time log streaming
talosctl dmesg                     # Kernel message buffer
talosctl dashboard                 # Interactive dashboard
```

### üìä Integration with Kubernetes 1.34.1

#### **Kubernetes Feature Compatibility**
| Feature | Status | Implementation |
|---|---|---|
| **Dynamic Resource Allocation** | ‚úÖ GA | Native support for resource allocation |
| **Structured Authentication** | ‚úÖ GA | Enhanced API server authentication |
| **VolumeAttributesClass** | ‚úÖ GA | Advanced volume management |
| **CPU Manager Uncore Cache** | üîÑ Beta | CPU performance optimization |
| **Pod Security Standards** | ‚úÖ GA | Built-in security policy enforcement |

#### **Platform-Specific Optimizations**
- **eBPF Integration**: Native eBPF support for Cilium CNI
- **Container Runtime**: Optimized containerd configuration
- **Storage Drivers**: Enhanced storage driver support
- **Network Plugins**: Optimized CNI integration

### üîÑ Upgrade Path Management

#### **Supported Upgrade Methods**
1. **Sequential Node Upgrade**: One node at a time with validation
2. **Parallel Upgrade**: Multiple nodes simultaneously (with quorum protection)
3. **Blue-Green Deployment**: Full cluster replacement with validation

#### **Upgrade Safety Features**
- **Health Checks**: Pre-upgrade and post-upgrade validation
- **Rollback Protection**: Automatic rollback on upgrade failures
- **Quorum Protection**: Prevents cluster split-brain scenarios
- **Configuration Validation**: Ensures compatibility before applying

### üìà Monitoring & Metrics

#### **Platform Metrics**
```yaml
# Key Talos metrics
talos:
  boot_time_seconds: 45.2
  memory_usage_bytes: 134217728  # 128MiB
  cpu_usage_percent: 2.5
  disk_usage_bytes: 1073741824   # 1GiB
  network_rx_bytes: 1048576000   # 1GB
  network_tx_bytes: 524288000    # 500MB
```

#### **Alerting Integration**
- **Prometheus Integration**: Native metrics export
- **Alertmanager**: Alert configuration for critical failures
- **Grafana Dashboards**: Pre-built platform monitoring dashboards
- **Health Status**: Real-time cluster health indicators

---

## ‚ò∏Ô∏è Kubernetes 1.34.1 - "Of Wind & Will" Release

> **üöÄ Production-Ready Innovation ‚Ä¢ Developer Experience ‚Ä¢ Cloud-Native Excellence**

Kubernetes 1.34.1 represents a significant milestone in container orchestration, bringing 58 enhancements including 13 features moving to General Availability, with a focus on developer experience, security, and operational efficiency.

### üéØ Release Highlights

#### **General Availability Features**
- **Dynamic Resource Allocation (DRA)**: Revolutionary resource management for specialized hardware
- **VolumeAttributesClass**: Advanced volume configuration and modification capabilities
- **Structured Authentication Configuration**: Enhanced API server authentication management
- **Finer-Grained Authorization Selectors**: Improved RBAC with precise policy control
- **Delayed Job Pod Replacement**: Intelligent job execution with resource optimization
- **Volume Expansion Failure Recovery**: Resilient storage operations with automatic recovery

#### **Beta Features**
- **Projected ServiceAccount Tokens for Kubelet**: Enhanced security with short-lived tokens
- **Mutable CSI Node Allocatable**: Dynamic resource allocation adjustments
- **PSI (Pressure Stall Information) Metrics**: Advanced system performance monitoring

### üîß Core Platform Enhancements

#### **Dynamic Resource Allocation (DRA)**
```yaml
# DRA example for GPU allocation
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: gpu-claim
spec:
  devices:
    requests:
      - deviceClassName: gpu.example.com
        results:
          - name: gpu
            claimName: gpu-claim
```

**Key Benefits:**
- **Hardware Agnostic**: Supports GPUs, FPGAs, NICs, and specialized accelerators
- **Fine-Grained Control**: Precise resource allocation and sharing
- **Multi-Tenant Support**: Secure resource isolation between workloads
- **Dynamic Scheduling**: Intelligent resource placement and migration

#### **VolumeAttributesClass Innovation**
```yaml
# VolumeAttributesClass for dynamic volume modification
apiVersion: storage.k8s.io/v1beta1
kind: VolumeAttributesClass
metadata:
  name: fast-ssd
parameters:
  throughput: "5000MiB/s"
  iops: "30000"
  encryption: "true"
  compression: "lz4"
```

**Capabilities:**
- **Runtime Modification**: Change volume characteristics without pod restart
- **Performance Tuning**: Dynamic performance parameter adjustment
- **Cost Optimization**: Optimize storage costs based on workload requirements
- **Multi-Cloud Support**: Consistent experience across cloud providers

#### **Structured Authentication Configuration**
```yaml
# Enhanced authentication configuration
apiVersion: v1
kind: Configuration
authentication:
  webhook:
    config:
      # Structured configuration with validation
      - name: oidc-provider
        type: OIDC
        config:
          issuer: "https://oidc.example.com"
          audiences: ["kubernetes"]
          claimMappings:
            usernameClaim: "sub"
            groupsClaim: ["groups"]
```

### üõ°Ô∏è Security & Compliance Enhancements

#### **Pod Security Standards GA**
- **Baseline Policy**: Default security posture for workloads
- **Restricted Policy**: Enhanced security for sensitive applications
- **Privileged Policy**: Full access for system workloads
- **Policy Validation**: Automated security policy enforcement

#### **Authentication & Authorization Improvements**
```yaml
# Fine-grained authorization example
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: namespace-reader
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "watch"]
  resourceNames: ["*"]  # Selective resource access
```

#### **Certificate Management**
- **Short-Lived Certificates**: Automatic rotation with 24-hour validity
- **Multi-CA Support**: Support for multiple certificate authorities
- **Enhanced Validation**: Improved certificate chain validation
- **Security Policies**: Automated security policy enforcement

### üìä Performance & Scalability

#### **Scheduling Enhancements**
- **Asynchronous API Handling**: Improved scheduler performance with prioritized queues
- **Request Deduplication**: Reduced API server load through intelligent caching
- **Bin Packing Optimization**: Improved resource utilization through advanced scheduling algorithms

#### **Resource Management**
- **Memory Optimization**: Reduced memory footprint for large clusters
- **CPU Efficiency**: Enhanced CPU utilization and scheduling
- **Network Performance**: Improved network throughput and latency

#### **Monitoring & Observability**
```yaml
# PSI metrics example
node_pressure_conditions:
  - type: MemoryPressure
    status: "False"
    reason: "KubeletHasSufficientMemory"
  - type: DiskPressure
    status: "False"
    reason: "KubeletHasNoDiskPressure"
  - type: PIDPressure
    status: "False"
    reason: "KubeletHasSufficientPID"
```

### üöÄ Developer Experience Improvements

#### **Enhanced kubectl Functionality**
- **Improved Error Messages**: More descriptive and actionable error reporting
- **Enhanced Debugging**: Better debugging capabilities with detailed status information
- **Resource Validation**: Pre-creation validation with comprehensive error reporting

#### **API Server Enhancements**
- **Structured Logging**: Improved log formatting and filtering capabilities
- **Performance Metrics**: Enhanced metrics for API server performance monitoring
- **Request Tracing**: Distributed tracing for API request analysis

### üîß Networking Enhancements

#### **Gateway API Improvements**
```yaml
# Enhanced Gateway API configuration
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: web-gateway
spec:
  gatewayClassName: cilium
  listeners:
    - name: http
      protocol: HTTP
      port: 80
      allowedRoutes:
        kinds:
          - kind: HTTPRoute
```

#### **Network Policy Enhancements**
- **Policy Validation**: Enhanced network policy validation and error reporting
- **Performance Optimization**: Improved network policy enforcement performance
- **Multi-Network Support**: Enhanced support for multiple network interfaces

### üíæ Storage Innovations

#### **CSI Driver Improvements**
- **Enhanced Volume Management**: Improved volume lifecycle management
- **Performance Monitoring**: Better storage performance metrics and monitoring
- **Multi-Path Support**: Enhanced support for multi-path storage configurations

#### **Storage Class Enhancements**
```yaml
# Enhanced StorageClass with volume binding mode
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

### üîÑ Upgrade & Migration Considerations

#### **Compatibility Matrix**
| Component | Minimum Version | Recommended Version | Notes |
|---|---|---|---|
| **kubectl** | 1.34.0 | 1.34.1 | Client compatibility required |
| **CNI Plugins** | 1.2.0 | 1.3.0 | Enhanced networking features |
| **CSI Drivers** | Latest | Latest | Recommended for DRA support |
| **Ingress Controllers** | Latest | Latest | Enhanced Gateway API support |

#### **Migration Path**
1. **Pre-Upgrade Validation**: Cluster health and compatibility checks
2. **Control Plane Upgrade**: Sequential upgrade of control plane components
3. **Worker Node Upgrade**: Rolling upgrade of worker nodes
4. **Post-Upgrade Validation**: Comprehensive health and functionality testing

### üìà Operational Excellence

#### **Cluster Autoscaling**
- **Node Auto-Discovery**: Enhanced node discovery and registration
- **Resource Optimization**: Improved resource utilization and cost optimization
- **Multi-Cloud Support**: Consistent autoscaling across cloud providers

#### **Monitoring & Alerting**
```yaml
# Enhanced metrics collection
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-config
data:
  metrics: |
    # Kubernetes 1.34.1 specific metrics
    scheduler_scheduling_latency_seconds
    api_server_request_total
    controller_manager_workqueue_adds_total
    kubelet_volume_stats_capacity_bytes
```

### üîÆ Future-Proofing Considerations

#### **Extensibility**
- **Custom Resource Definitions**: Enhanced CRD validation and support
- **Webhook Enhancements**: Improved webhook framework and validation
- **Operator Framework**: Better support for custom operators

#### **Multi-Cluster Management**
- **Cluster API**: Enhanced multi-cluster management capabilities
- **Federation Improvements**: Better cross-cluster resource sharing
- **Service Discovery**: Enhanced service discovery across clusters

### üìö API Reference & Documentation

#### **Key API Groups**
```yaml
# Core API groups with 1.34.1 enhancements
apiVersion:
  - group: "apps/v1"            # Enhanced deployment strategies
  - group: "batch/v1"           # Improved job management
  - group: "networking.k8s.io/v1"  # Gateway API enhancements
  - group: "storage.k8s.io/v1"    # DRA and VolumeAttributesClass
  - group: "resource.k8s.io/v1alpha2"  # Dynamic resource allocation
  - group: "policy/v1"          # Pod security standards
```

### üéØ Objectives

| ‚úÖ Primary Goals | ‚ùå Non-Goals |
| :--- | :--- |
| ‚ö° **Repeatable, fast cluster bring-up** with minimal manual steps | üîß Managing Talos OS lifecycle (covered by Talos docs/scripts) |
| üîÑ **Clear separation of concerns** between infrastructure and workloads | üìö Full application SRE runbooks (app teams own those) |
| ‚öôÔ∏è **Deterministic ordering** using Flux `Kustomization.dependsOn`, health checks, and timeouts | |
| üîê **Zero plaintext secrets** via External Secrets (1Password Connect) | |
| üìà **Scalable to additional environments** (apps-dev, apps-stg, apps-prod) and optional multi-tenancy | |

## 2. üéØ Design Principles

> **üèõÔ∏è Core Philosophy**
>
> Our architecture follows GitOps best practices with security and reliability at the forefront.

| Principle | Description | Impact |
| :--- | :--- | :--- |
| üîÑ **Git as Single Source of Truth** | Controllers converge the desired state from Git repository | Eliminates configuration drift |
| üèóÔ∏è **Hierarchical Flux Structure** | One entry Kustomization per cluster, fanning into ordered Kustomizations with `dependsOn` | Ensures deterministic deployment order |
| üîí **Hermetic Builds** | Avoid remote bases; prefer local Git/OCI sources. No cross-namespace references for multi-tenant areas | Improves security and reproducibility |
| ‚úÖ **Complete Kustomization Settings** | All `Kustomization` have `prune: true`, `wait: true`, `timeout`, `healthChecks`/`healthCheckExprs` | Ensures reliable deployments |
| üîê **Secure Secrets Management** | External Secrets with 1Password Connect for all secrets (bootstrap and runtime) | Zero plaintext secrets in Git |
| üöÄ **Minimal Bootstrap** | Bootstrap installs only what is required to let Flux take over; manifests are authored first and applied post‚Äëbootstrap (v3) | Fast, reliable cluster bring-up |

---

## 3. üåê Target Topology

> **üèóÔ∏è Multi-Cluster Architecture**
>
> Our platform consists of two specialized clusters designed for optimal performance and separation of concerns.

### üè≠ Cluster Overview

| Cluster | Purpose | Key Components | Network Features |
| :--- | :--- | :--- | :--- |
| **üè≠ infra** | Platform services & storage | üóÑÔ∏è Rook-Ceph ‚Ä¢ üêò CloudNativePG ‚Ä¢ üìä VictoriaMetrics/Logs ‚Ä¢ üîê Security ‚Ä¢ üì¶ Registry | Core Cilium + BGP + Gateway API |
| **üéØ apps** | Application workloads | ‚ö° User applications ‚Ä¢ üöÄ CI/CD runners ‚Ä¢ üì¨ Messaging systems | Core Cilium + ClusterMesh connectivity |

### üåê Network Architecture

```mermaid
graph TB
    subgraph "Infrastructure Cluster"
        C1[Cilium CNI]
        BGP1[BGP Peering]
        GW1[Gateway API]
        Storage[Storage Services]
        DB[Database Services]
    end

    subgraph "Application Cluster"
        C2[Cilium CNI]
        CM[ClusterMesh]
        Apps[Application Workloads]
    end

    C1 <--> CM
    C2 <--> CM
    Storage -.-> |Cross-cluster services| Apps
    DB -.-> |Database access| Apps
```

**Key Network Features:**
- üåê **Cilium as CNI** - Core installed during bootstrap, managed via GitOps thereafter
- üß∞ **Networking Features (Git‚ÄëManaged)** - BGP, Gateway API, ClusterMesh secrets, IPAM pools defined as manifests and reconciled by Flux
- üîó **ClusterMesh** - Secure cross-cluster connectivity for service discovery

### üíæ Storage Architecture

| Cluster | Storage Solution | Use Case | Performance |
| :--- | :--- | :--- | :--- |
| **üè≠ infra** | üóÑÔ∏è Rook-Ceph + OpenEBS LocalPV | Block/file storage, databases (Postgres, Dragonfly) | High throughput NVMe |
| **üéØ apps** | üóÑÔ∏è Dedicated Rook-Ceph + OpenEBS LocalPV (default) | Application storage, local workloads | Multi‚ÄëGB/s NVMe local |

**Why Dedicated Storage for Apps Cluster?**
- üö´ **Avoids 1 Gbps router bottleneck** - Keeps Ceph traffic local to apps cluster
- ‚ö° **Better performance** - Local replication/backfill within cluster
- üîß **Operational simplicity** - Aligned Ceph versions across clusters

### üìä Observability Strategy

| Cluster | Stack Components | Data Flow |
| :--- | :--- | :--- |
| **üè≠ infra** | üìà VictoriaMetrics global ‚Ä¢ üìù VictoriaLogs ‚Ä¢ üìä Exporters ‚Ä¢ üö® Flux Alerts/Receivers | Full observability stack |
| **üéØ apps** | üì° vmagent ‚Ä¢ üìä kube-state-metrics ‚Ä¢ üìã node-exporter ‚Ä¢ üì§ Fluent Bit | **Remote write/export to infra only** |

**Benefits:**
- üí∞ **Cost efficient** - Single observability stack
- üöÄ **High performance** - Local aggregation, remote forwarding
- üîß **Simplified operations** - Centralized monitoring and logging

## 4. üìÅ Repository Layout (End‚ÄëState)

> **üèõÔ∏è Flux-Optimized Structure**
>
> We keep the established `kubernetes/` layout and align it with Flux best practices for optimal GitOps workflows.

### üìÇ Directory Structure

```
üì¶ k8s-gitops/
‚î£‚îÅ üìÅ kubernetes/
‚îÉ  ‚î£‚îÅ üìÅ clusters/
‚îÉ  ‚îÉ  ‚î£‚îÅ üè≠ infra/
‚îÉ  ‚îÉ  ‚îÉ  ‚î£‚îÅ üîÑ flux-system/               # GitRepository + cluster Kustomizations (entrypoint)
‚îÉ  ‚îÉ  ‚îÉ  ‚î£‚îÅ ‚öôÔ∏è cluster-settings.yaml      # ConfigMap; used by postBuild.substituteFrom
‚îÉ  ‚îÉ  ‚îÉ  ‚î£‚îÅ üèóÔ∏è infrastructure.yaml        # Ordered platform Kustomizations for infra cluster
‚îÉ  ‚îÉ  ‚îÉ  ‚îó‚îÅ üöÄ workloads.yaml             # Platform workloads (observability, registry, etc.)
‚îÉ  ‚îÉ  ‚î£‚îÅ üéØ apps/
‚îÉ  ‚îÉ  ‚îÉ  ‚î£‚îÅ üîÑ flux-system/
‚îÉ  ‚îÉ  ‚îÉ  ‚î£‚îÅ ‚öôÔ∏è cluster-settings.yaml
‚îÉ  ‚îÉ  ‚îÉ  ‚î£‚îÅ üèóÔ∏è infrastructure.yaml        # Platform needed on apps cluster (e.g., external-secrets, issuers)
‚îÉ  ‚îÉ  ‚îÉ  ‚îó‚îÅ üöÄ workloads.yaml             # Tenants/platform apps on apps cluster
‚îÉ  ‚îÉ  ‚îó‚îÅ üåê apps-dev/, apps-stg/, apps-prod/ (optional overlays)
‚îÉ  ‚î£‚îÅ üìÅ infrastructure/
‚îÉ  ‚îÉ  ‚î£‚îÅ üåê networking/                  # networking manifests (cilium core, bgp, gateway, clustermesh, ipam)
‚îÉ  ‚îÉ  ‚î£‚îÅ üîê security/                    # external-secrets, cert-manager issuers, RBAC bundles
‚îÉ  ‚îÉ  ‚î£‚îÅ üíæ storage/                     # rook-ceph, openebs (infra cluster only)
‚îÉ  ‚îÉ  ‚îó‚îÅ üîÑ gitops/                      # legacy flux-operator/instance charts (reference only)
‚îÉ  ‚î£‚îÅ üìÅ workloads/
‚îÉ  ‚îÉ  ‚î£‚îÅ üèóÔ∏è platform/                    # platform apps (observability, registry, CICD, databases)
‚îÉ  ‚îÉ  ‚îó‚îÅ üë• tenants/                     # optional multi‚Äëtenant applications
‚îÉ  ‚î£‚îÅ üìÅ components/                     # reusable Kustomize components (namespaced building blocks)
‚îÉ  ‚îÉ  ‚î£‚îÅ namespace/                      # standard namespace component with labels/PSS
‚îÉ  ‚îÉ  ‚îó‚îÅ volsync/                        # app-level VolSync pieces: ExternalSecret, Replication{Source,Destination}, PVC restore
‚î£‚îÅ üìÅ bootstrap/
‚îÉ  ‚î£‚îÅ üìÑ helmfile.d/00-crds.yaml       # CRD-only phase
‚îÉ  ‚î£‚îÅ üìÑ helmfile.d/01-apps.yaml       # ordered bootstrap charts (cilium‚Üícoredns‚Üíspegel‚Üícert-manager‚Üíflux-operator‚Üíflux-instance)
‚îÉ  ‚î£‚îÅ üìÑ helmfile.d/templates/values.yaml.gotmpl  # reads HelmRelease .spec.values to keep one source of truth
‚îÉ  ‚îó‚îÅ üìÑ resources.yaml                # namespaces + initial Secret(s) for secret-store
‚îó‚îÅ üìÅ .taskfiles/
   ‚î£‚îÅ üìÑ bootstrap/Taskfile.yaml       # task bootstrap:talos, bootstrap:apps
   ‚îó‚îÅ üìÑ talos/Taskfile.yaml           # node lifecycle helpers
```

### üéØ Key Design Decisions

| Decision | Rationale | Impact |
| :--- | :--- | :--- |
| **üö´ Removed aggregator `ks.yaml`** | Avoid duplicating cluster wiring | Cleaner, more direct Flux reconciliations |
| **üîß Helmfile bootstrap** | Predictable, idempotent installation | Reliable cluster bring-up |
| **üìÅ Cluster-specific settings** | `cluster-settings.yaml` per cluster | Environment-specific configuration |
| **üîÑ Git as source of truth** | Flux reconciles directories directly | No configuration drift |
| **üß© Components pattern** | Reusable, namespaced Kustomize components for app teams (e.g., VolSync) | Promotes consistency; no cluster-scoped side effects |

### üîß Manifests‚ÄëFirst v3 Model

| Stage | What Happens | Tools |
| :--- | :--- | :--- |
| **1) Author & Validate Manifests (Stories 1‚Äë41)** | Write all YAML (networking, security, storage, etc.) under `kubernetes/**`. Validate with `kustomize`, `flux build`, and `kubeconform`. No cluster required. | kustomize, flux, kubeconform |
| **2) Bootstrap & Deploy (Stories 42‚Äë50)** | Create clusters (Talos), bootstrap minimal core, then apply and reconcile manifests. Validate end‚Äëto‚Äëend via validation stories. | Helmfile + Task, Flux |

This section aligns with `docs/resequencing-v3-summary.md` (v3 ‚Äî Manifests‚ÄëFirst, Bootstrap‚ÄëLast).

### üß© Components Usage

- Components live under `kubernetes/components/` and are imported by app/team overlays as Kustomize components.
- Examples:
  - `components/namespace`: standard namespace with labels and PSS defaults
  - `components/volsync`: VolSync app artifacts (ExternalSecret with MinIO S3 env vars, ReplicationSource/ReplicationDestination, optional PVC restore)
- Components are namespaced and safe to apply per‚Äëteam; they do not create cluster‚Äëscoped controllers. Controllers (e.g., VolSync operator, snapshot-controller) are deployed under `workloads/platform/**` and wired by cluster Kustomizations.

---

## 5. ‚öôÔ∏è Flux Model & Convergence

> **üîÑ Declarative GitOps Engine**
>
> Flux ensures reliable, ordered deployment of infrastructure and workloads with built-in health checking.

### üéØ Entry Kustomization (Per Cluster)

Each cluster has a single entry point that orchestrates the entire platform:

```yaml
# kubernetes/clusters/<cluster>/flux-system/kustomization.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cluster-<name>
  namespace: flux-system
spec:
  interval: 10m
  path: ./kubernetes/clusters/<name>
  prune: true
  wait: true
  timeout: 15m
  sourceRef:
    kind: GitRepository
    name: flux-system
```

**Key Features:**
- üìÅ **Reconciles cluster directory** - Includes `cluster-settings.yaml`, `infrastructure.yaml`, and `workloads.yaml`
- üîß **Variable substitution** - Uses `postBuild.substituteFrom` to inject cluster-specific settings
- ‚úÖ **Health validation** - Built-in health checks ensure reliable deployments

### üîÑ Ordering & Dependencies

```mermaid
graph TD
    A[flux-repositories] --> B[infrastructure]
    B --> C[workloads]

    B --> D[networking]
    B --> E[security]
    B --> F[storage]

    D --> G[workloads]
    E --> G
    F --> G
```

**Dependency Chain:**
1. **üîÑ flux-repositories** - Helm repositories and OCI sources
2. **üèóÔ∏è infrastructure** - Core platform components (networking, security, storage)
3. **üöÄ workloads** - Applications and services

### ‚úÖ Health Checking Strategy

| Component | Health Check | Timeout | Success Criteria |
|---|---|---|---|
| **ClusterIssuer** | Ready condition | 5m | Certificate authority ready |
| **DaemonSet** | Available pods | 10m | All nodes running pods |
| **Deployment** | Available replicas | 5m | Desired replicas ready |
| **StatefulSet** | Ready replicas | 15m | All replicas ready |
| **PVC** | Bound status | 2m | Volume successfully bound |

**Configuration Example:**
```yaml
spec:
  dependsOn:
    - name: flux-repositories
  interval: 10m
  path: ./kubernetes/infrastructure
  prune: true
  wait: true
  timeout: 10m
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: external-secrets
      namespace: external-secrets
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
```

Example (trimmed)
```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cluster-infra-infrastructure
  namespace: flux-system
spec:
  dependsOn:
    - name: flux-repositories
  interval: 10m
  path: ./kubernetes/infrastructure
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: flux-system
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
```

## 6. üöÄ Bootstrap Architecture (Helmfile + Task)

> **üèóÔ∏è Two-Phase Bootstrap Strategy**
>
> We use Helmfile for predictable, idempotent bootstrap before Flux controllers exist. This ensures consistent cluster bring-up every time.

### üéØ Why Helmfile?

| Benefit | Description | Impact |
|---|---|---|
| **üîÑ Idempotent** | Same result every run | Reliable bootstrap process |
| **‚ö° Fast** | Direct cluster installation | No waiting for controllers |
| **üìù Consistent values** | Reuses same values as Flux `HelmRelease` | Zero configuration drift |
| **üîß Two-phase approach** | CRDs first, then apps | Proper dependency ordering |

### üîÑ Bootstrap Phases (applied in Stories 42‚Äë44)

```mermaid
graph LR
    A[Phase 0: CRDs] --> B[Phase 1: Core Apps]
    B --> C[Phase 2: Flux Handover]
    C --> D[GitOps Reconciliation]

    subgraph "Phase 0"
        A1[00-crds.yaml]
        A2[Extract CRDs only]
        A3[Apply CRDs first]
    end

    subgraph "Phase 1"
        B1[01-apps.yaml]
        B2[Ordered releases]
        B3[Cilium ‚Üí CoreDNS ‚Üí Spegel ‚Üí cert-manager ‚Üí Flux]
    end
```

#### Phase 0: CRD Foundation (`bootstrap/helmfile.d/00-crds.yaml`)

```bash
helmfile -f bootstrap/helmfile.d/00-crds.yaml -e <cluster> template \
  | yq ea 'select(.kind == "CustomResourceDefinition")' \
  | kubectl apply -f -
```

**Key Features:**
- üîß **`helmDefaults.args: [--include-crds, --no-hooks]`** - Clean CRD extraction
- üìã **PostRenderer with `yq`** - Filters CRDs only
- ‚úÖ **Prerequisite validation** - Ensures CRDs exist before consumers

#### Phase 1: Core Applications (`bootstrap/helmfile.d/01-core.yaml.gotmpl`)

**üîó Dependency Chain:**
```
Cilium (CNI) ‚Üí CoreDNS (DNS) ‚Üí Spegel (Image Mirror) ‚Üí cert-manager (TLS) ‚Üí flux-operator ‚Üí flux-instance
```

**üéØ Key Features:**
- üìã **Ordered releases with `needs`** - Proper startup sequence
- üîÑ **Template values from Git** - `values.yaml.gotmpl` reads `HelmRelease` specs
- üìù **Single source of truth** - Same values for bootstrap and Flux

### üîê Early Secrets & Namespaces

**Bootstrap Resources (`bootstrap/resources.yaml`):**

| Resource | Purpose | Created When |
|---|---|---|
| **external-secrets namespace** | Secret management operator | Bootstrap Phase 1 |
| **1Password Connect Secret** | External secrets access | Before Flux starts |
| **1Password Connect token** | Bootstrap access token for External Secrets | Bootstrap Phase 0 |

### üõ†Ô∏è Bootstrap Tasks (Taskfile Canonical)

| Task | Command | Function |
|---|---|---|
| **üîß Talos Bootstrap (Phase ‚àí1)** | `task bootstrap:talos` | Apply first control plane, `talosctl bootstrap`, remaining CPs, export kubeconfig |
| **üì¶ Prereqs (Phase 0)** | `task :bootstrap:phase:0 CLUSTER=<cluster>` | Namespaces and initial secrets (e.g., 1Password) |
| **üîß CRDs (Phase 1)** | `task :bootstrap:phase:1 CLUSTER=<cluster>` | Install CRDs only (extracted from charts) |
| **üöÄ Core (Phase 2)** | `task :bootstrap:phase:2 CLUSTER=<cluster>` | Cilium, CoreDNS, cert‚Äëmanager (CRDs disabled), External Secrets (CRDs disabled), Flux |
| **‚úÖ Validate (Phase 3)** | `task :bootstrap:phase:3 CLUSTER=<cluster>` | Readiness, Flux health, status summary |

### ‚úÖ Phase Guards
- Phase 0 must emit only CustomResourceDefinition kinds; audit with kinds filter.
- Phase 1 installs controllers with CRD installation disabled in chart values (CRDs were pre‚Äëinstalled in Phase 0).

### üß≠ Handover Criteria (Authoritative)
- flux‚Äëoperator Ready; flux‚Äëinstance Ready; GitRepository source connected; all initial Kustomizations Ready; `kustomize build` + `kubeconform` clean for the cluster root.

### üß© Talos Role‚ÄëAware Convention (optional)
```
talos/
 ‚îî‚îÄ <cluster>/
     ‚îú‚îÄ controlplane/   # first CP used for etcd bootstrap; then remaining CPs
     ‚îî‚îÄ worker/         # workers joined after API is responding
```
Behavior:
- Prefer `controlplane/*.yaml` first; then remaining CPs in `<cluster>/*.yaml`.
- After API is Ready, apply `worker/*.yaml` using `:talos:apply-node ... MACHINE_TYPE=worker`.
- Safe detector: if first CP already healthy (`talosctl get machineconfig` and `etcd status` OK), skip `talosctl bootstrap`.

### üß™ CI Dry‚ÄëRun (non‚Äëblocking to start)
- Run `task bootstrap:dry-run CLUSTER=infra` in validation CI to surface template/values drift. Emit a short summary. Can become gating later.

### ‚è±Ô∏è Time‚Äëto‚ÄëReady Targets (baseline)
- Talos control plane ‚â§ 7 minutes; CRDs ‚â§ 2 minutes; Core ‚â§ 6 minutes; total ‚â§ 20 minutes per cluster.

### üîÑ GitOps Reconciliation Window (post‚Äëbootstrap)

```mermaid
graph TD
    A[Bootstrap Complete] --> B[flux-operator Ready]
    B --> C[flux-instance Ready]
    C --> D[Flux Takes Over]
    D --> E[Continuous GitOps Management]

    E --> F[Networking: BGP/Gateway/ClusterMesh]
    E --> G[Security: cert-manager issuers]
    E --> H[Storage: Rook-Ceph/OpenEBS]
    E --> I[Observability: VictoriaMetrics]
    E --> J[Workloads: Applications]
```

**‚úÖ Handover Criteria:**
- üü¢ **flux-operator** controllers running and ready
- üü¢ **flux-instance** cluster reconciliation active
- üü¢ **GitRepository** source connected and syncing
- üü¢ **Initial Kustomizations** reconciling successfully

All ongoing configuration changes happen through Git commits, with Flux automatically applying them to the cluster. In v3, manifests are authored first; deployment and system validation follow in Stories 45‚Äë49.

---

## 7. ‚öôÔ∏è Cluster Settings & Substitution

> **üîß Centralized Configuration Management**
>
> Each cluster has a dedicated `cluster-settings.yaml` ConfigMap that contains all environment-specific values used throughout the platform.

### üìã Configuration Examples (real values from this repo)

Infra cluster (kubernetes/clusters/infra/cluster-settings.yaml)
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-settings
  namespace: flux-system
  labels:
    app.kubernetes.io/managed-by: flux
    cluster: infra
data:
  # Cluster Identity
  CLUSTER: infra
  CLUSTER_ID: "1"

  # Network Configuration
  POD_CIDR: '["10.244.0.0/16"]'
  POD_CIDR_STRING: "10.244.0.0/16"
  SERVICE_CIDR: '["10.245.0.0/16"]'
  K8S_SERVICE_HOST: "infra-k8s.monosense.io"
  K8S_SERVICE_PORT: "6443"

  # Cilium Configuration
  CILIUM_VERSION: "1.18.3"
  CLUSTERMESH_IP: "10.25.11.100"
  CILIUM_GATEWAY_LB_IP: "10.25.11.110"
  CILIUM_BGP_LOCAL_ASN: "64512"
  CILIUM_BGP_PEER_ASN: "64501"
  CILIUM_BGP_PEER_ADDRESS: "10.25.11.1/32"

  # IPAM Pool Control
  INFRA_POOL_DISABLED: "false"
  APPS_POOL_DISABLED: "true"
  CILIUM_LB_POOL_START: "10.25.11.100"
  CILIUM_LB_POOL_END: "10.25.11.119"

  # CoreDNS Configuration
  COREDNS_CLUSTER_IP: "10.245.0.10"
  COREDNS_REPLICAS: "2"

  # External Secrets Configuration
  EXTERNAL_SECRET_STORE: "onepassword"
  ONEPASSWORD_CONNECT_HOST: "http://opconnect.monosense.dev"
  ONEPASSWORD_CONNECT_TOKEN_SECRET: "onepassword-connect-token"
  CILIUM_CLUSTERMESH_SECRET_PATH: "kubernetes/infra/cilium-clustermesh"
  CERTMANAGER_CLOUDFLARE_SECRET_PATH: "kubernetes/infra/cert-manager/cloudflare"

  # Domain Configuration
  SECRET_DOMAIN: "monosense.io"

  # GitHub Actions Configuration
  GITHUB_CONFIG_URL: "https://github.com/monosense-io/k8s-gitops"

  # Rook-Ceph Configuration
  ROOK_CEPH_NAMESPACE: "rook-ceph"
  ROOK_CEPH_CLUSTER_NAME: "rook-ceph"
  ROOK_CEPH_BLOCKPOOL_NAME: "rook-ceph-block"
  ROOK_CEPH_IMAGE_TAG: v19.2.3
  ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"
  ROOK_CEPH_MON_COUNT: "3"

  # Storage Classes
  BLOCK_SC: "rook-ceph-block"  # Generic Rook-Ceph block storage (RBD)
  OPENEBS_LOCAL_SC: "openebs-local-nvme"  # Local NVMe storage

  # OpenEBS Configuration
  OPENEBS_BASEPATH: "/var/mnt/openebs"

  # Observability Configuration
  OBSERVABILITY_METRICS_RETENTION: "30d"
  OBSERVABILITY_LOGS_RETENTION: "30d"
  OBSERVABILITY_LOG_ENDPOINT_HOST: "victorialogs-vmauth.observability.svc.cluster.local"
  OBSERVABILITY_LOG_ENDPOINT_PORT: "9428"
  OBSERVABILITY_LOG_ENDPOINT_PATH: "/insert"
  OBSERVABILITY_LOG_ENDPOINT_TLS: "Off"
  OBSERVABILITY_LOG_TENANT: "infra"
  OBSERVABILITY_GRAFANA_SECRET_PATH: "kubernetes/infra/grafana-admin"

  # Global Monitoring Configuration (for cross-cluster federation)
  GLOBAL_VM_INSERT_ENDPOINT: "victoria-metrics-global-vminsert.observability.svc.cluster.local:8480"
  GLOBAL_VM_SELECT_ENDPOINT: "victoria-metrics-global-vmselect.observability.svc.cluster.local:8481"
  GLOBAL_ALERTMANAGER_ENDPOINT: "victoria-metrics-global-alertmanager.observability.svc.cluster.local:9093"

  # CloudNative-PG Configuration
  CNPG_OPERATOR_VERSION: "0.25.0"
  CNPG_POSTGRES_VERSION: "16.8"
  CNPG_STORAGE_CLASS: "openebs-local-nvme"
  CNPG_DATA_SIZE: "80Gi"
  CNPG_WAL_SIZE: "20Gi"
  CNPG_INSTANCES: "3"
  CNPG_SHARED_CLUSTER_NAME: "shared-postgres"
  CNPG_BACKUP_BUCKET: "monosense-cnpg"
  # NOTE: CNPG ScheduledBackup uses a six-field cron expression (seconds first).
  # Example below runs at 02:00:00 UTC daily.
  CNPG_BACKUP_SCHEDULE: "0 0 2 * * *"
  CNPG_MINIO_ENDPOINT_URL: "http://10.25.11.3:9000"
  CNPG_MINIO_SECRET_PATH: "kubernetes/infra/cloudnative-pg/minio"
  CNPG_SUPERUSER_SECRET_PATH: "kubernetes/infra/cloudnative-pg/superuser"

  # Dragonfly Configuration (infra)
  DRAGONFLY_STORAGE_CLASS: "openebs-local-nvme"
  DRAGONFLY_DATA_SIZE: "30Gi"           # default for shared cache; tune per-tenant if split
  DRAGONFLY_AUTH_SECRET_PATH: "kubernetes/infra/dragonfly/auth"
```

Apps cluster (kubernetes/clusters/apps/cluster-settings.yaml)
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-settings
  namespace: flux-system
  labels:
    app.kubernetes.io/managed-by: flux
    cluster: apps
data:
  # Cluster Identity
  CLUSTER: apps
  CLUSTER_ID: "2"

  # Network Configuration
  POD_CIDR: '["10.246.0.0/16"]'
  POD_CIDR_STRING: "10.246.0.0/16"
  SERVICE_CIDR: '["10.247.0.0/16"]'
  K8S_SERVICE_HOST: "apps-k8s.monosense.io"
  K8S_SERVICE_PORT: "6443"

  # Cilium Configuration
  CILIUM_VERSION: "1.18.3"
  CLUSTERMESH_IP: "10.25.11.120"
  CILIUM_GATEWAY_LB_IP: "10.25.11.121"
  CILIUM_BGP_LOCAL_ASN: "64513"
  CILIUM_BGP_PEER_ASN: "64501"
  CILIUM_BGP_PEER_ADDRESS: "10.25.11.1/32"

  # IPAM Pool Control
  INFRA_POOL_DISABLED: "true"
  APPS_POOL_DISABLED: "false"
  CILIUM_LB_POOL_START: "10.25.11.120"
  CILIUM_LB_POOL_END: "10.25.11.139"

  # CoreDNS Configuration
  COREDNS_CLUSTER_IP: "10.247.0.10"
  COREDNS_REPLICAS: "2"

  # External Secrets Configuration
  EXTERNAL_SECRET_STORE: "onepassword"
  ONEPASSWORD_CONNECT_HOST: "http://opconnect.monosense.dev"
  ONEPASSWORD_CONNECT_TOKEN_SECRET: "onepassword-connect-token"
  CILIUM_CLUSTERMESH_SECRET_PATH: "kubernetes/apps/cilium-clustermesh"
  CERTMANAGER_CLOUDFLARE_SECRET_PATH: "kubernetes/apps/cert-manager/cloudflare"

  # Domain Configuration
  SECRET_DOMAIN: "monosense.io"

  # Rook-Ceph Configuration (apps cluster)
  ROOK_CEPH_NAMESPACE: "rook-ceph"
  ROOK_CEPH_CLUSTER_NAME: "rook-ceph"
  ROOK_CEPH_BLOCKPOOL_NAME: "rook-ceph-block"
  ROOK_CEPH_IMAGE_TAG: v19.2.3
  ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"
  ROOK_CEPH_MON_COUNT: "3"

  # Storage Classes
  BLOCK_SC: "rook-ceph-block"  # Generic Rook-Ceph block storage (RBD)
  OPENEBS_LOCAL_SC: "openebs-local-nvme"  # Local NVMe storage

  # OpenEBS Configuration
  OPENEBS_BASEPATH: "/var/mnt/openebs"

  # Observability Configuration
  OBSERVABILITY_METRICS_RETENTION: "30d"
  OBSERVABILITY_LOGS_RETENTION: "30d"
  OBSERVABILITY_LOG_ENDPOINT_HOST: "victorialogs-vmauth.observability.svc.cluster.local"
  OBSERVABILITY_LOG_ENDPOINT_PORT: "9428"
  OBSERVABILITY_LOG_ENDPOINT_PATH: "/insert"
  OBSERVABILITY_LOG_ENDPOINT_TLS: "Off"
  OBSERVABILITY_LOG_TENANT: "apps"
  OBSERVABILITY_GRAFANA_SECRET_PATH: "kubernetes/apps/grafana-admin"

  # Dragonfly Configuration (apps)
  # Clients only; no operator on apps. Kept for potential future per-tenant instances.
  DRAGONFLY_STORAGE_CLASS: "openebs-local-nvme"
  DRAGONFLY_DATA_SIZE: "50Gi"
  DRAGONFLY_AUTH_SECRET_PATH: "kubernetes/apps/dragonfly/auth"

  # GitLab Configuration
  GITLAB_DB_SECRET_PATH: "kubernetes/apps/gitlab/db"
  GITLAB_REDIS_SECRET_PATH: "kubernetes/apps/gitlab/redis"
  GITLAB_S3_SECRET_PATH: "kubernetes/apps/gitlab/s3"
  GITLAB_ROOT_SECRET_PATH: "kubernetes/apps/gitlab/root"
```

### üîß Variable Substitution Flow

```mermaid
graph LR
    A[cluster-settings.yaml] --> B[postBuild.substituteFrom]
    B --> C[Template Variables]
    C --> D[HelmRelease Values]
    C --> E[Manifest Substitutions]
    C --> F[ConfigMap Data]

    subgraph "All Kustomizations"
        G[infrastructure.yaml]
        H[workloads.yaml]
        I[security/]
        J[storage/]
        K[networking/]
    end

    D --> G
    E --> H
    F --> I
    F --> J
    F --> K
```

### üìù Usage Examples

| Component | Variable Used | Example |
|---|---|---|
| **Cilium BGP** | `${CILIUM_BGP_ASN}` | `localAsn: ${CILIUM_BGP_ASN}` |
| **Storage Class** | `${DEFAULT_STORAGE_CLASS}` | `storageClassName: ${DEFAULT_STORAGE_CLASS}` |
| **External Secret** | `${SECRET_STORE_PATH}` | `path: ${SECRET_STORE_PATH}/database` |
| **Victoria Metrics** | `${VICTORIA_METRICS_ENDPOINT}` | `remoteWrite: ${VICTORIA_METRICS_ENDPOINT}` |

### ‚úÖ Benefits

| Benefit | Description |
|---|---|
| **üéØ Environment Isolation** | Each cluster has independent settings |
| **üîÑ Single Source of Truth** | All variables in one ConfigMap |
| **üöÄ Zero Drift** | Template substitutions at build time |
| **üîß Easy Maintenance** | Update cluster settings in one place |
| **üõ°Ô∏è Type Safety** | Explicit variable declarations |

### üèóÔ∏è Implementation Pattern

```yaml
# Any Kustomization using cluster settings
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: my-app
spec:
  # ... other spec
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
        optional: false
```

### üîí Substitution Ownership (Important)
- Define substitutions in the Flux `Kustomization` via `postBuild.substitute`/`postBuild.substituteFrom` that references `ConfigMap/cluster-settings`.
- HelmRelease values must use `${VAR}` placeholders (e.g., `${K8S_SERVICE_HOST}`, `${K8S_SERVICE_PORT}`, `${CILIUM_VERSION}`) and must not hard‚Äëcode cluster‚Äëspecific values.
- This keeps manifests portable and ensures all environment specifics live in `cluster-settings`.

## 8. üîê Secrets Management (1Password Only)

> **üõ°Ô∏è Zero-Trust Secrets Architecture**
>
> We implement a defense-in-depth approach to secrets management, ensuring no plaintext secrets ever touch our Git repository.

### üîß Approach

All secrets are managed exclusively via External Secrets with 1Password Connect. No SOPS is used in this platform.

### üèóÔ∏è Architecture Flow

```mermaid
graph TD
    A[1Password Vault] --> B[1Password Connect]
    B --> C[External Secrets Operator]
    C --> D[Kubernetes Secrets]
    D --> E[Applications]

    J[Bootstrap Secret (1Password token)] --> C
    K[Cluster Settings] --> C
```

### üéØ Implementation Strategy

#### **Primary: External Secrets + 1Password Connect**

**üîß Bootstrap Phase:**
```yaml
# bootstrap/resources.yaml - One-time setup
apiVersion: v1
kind: Secret
metadata:
  name: onepassword-connect
  namespace: external-secrets
type: Opaque
data:
  token: <base64-encoded-1password-token>
```

**üîÑ Runtime Secret Sync:**
```yaml
# Example ExternalSecret
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: database-credentials
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: onepassword-store
    kind: SecretStore
  target:
    name: database-credentials
  dataFrom:
    - extract:
        key: kubernetes/infra/database
```

#### Notes
- Bootstrap writes the 1Password Connect token Secret and creates External Secret stores. Thereafter, all workload/application secrets are pulled at reconcile time.

### üõ°Ô∏è Security Standards

| Standard | Requirement | Implementation |
|---|---|---|
| **üö´ Zero Plaintext** | No secrets in clear text | External Secrets (1Password) only |
| **üîÑ Automatic Rotation** | Regular secret updates | 1Password Connect with 1-hour refresh |
| **üö® Alerting** | Failed sync notifications | Flux alerts on decryption/ES failures |
| **üìù Documentation** | Rotation runbooks | Standardized procedures for all secret types |
| **üîê Access Control** | Least privilege access | 1Password RBAC, Kubernetes RBAC |

### üìä Secret Categories

| Category | Storage Method | Rotation Frequency | Example |
|---|---|---|---|
| **üóÑÔ∏è Database Credentials** | External Secrets | 90 days | PostgreSQL passwords |
| **üîë TLS Certificates** | External Secrets | Auto-renewal | cert-managed certs |
| **üîå API Keys** | External Secrets | 30-90 days | External service tokens |
| **üöÄ Bootstrap Secrets** | External Secrets | 1Password token rotation via 1Password |
| **üë• Service Account Keys** | External Secrets | 365 days | CI/CD deployment keys |

### ‚ö° Performance Optimizations

| Optimization | Description | Impact |
|---|---|---|
| **üîÑ Refresh Interval** | 1-hour refresh for most secrets | Reduces 1Password API calls |
| **üì¶ Secret Caching** | External Secrets operator caching | Faster pod startup |
| **üéØ Selective Sync** | Only sync needed secrets per namespace | Reduced memory footprint |
| **üöÄ Bulk Operations** | Batch secret operations where possible | Improved sync performance |

## 9. üåê Networking (Cilium)

> **üîó High-Performance eBPF Networking**
>
> Cilium provides the foundation for our multi-cluster connectivity with advanced security and observability capabilities.

### üèóÔ∏è Architecture Overview (v3 sequence)

```mermaid
graph TB
    subgraph "Story 01‚Äë13: Networking Manifests"
        A[Cilium Core (HelmRelease values)]
        B[kubeProxyReplacement: true]
        C[WireGuard Encryption]
        D[BGP Control Plane]
        E[Gateway API]
        F[ClusterMesh]
        G[IPAM Pools]
    end

    subgraph "Stories 42‚Äë50: Bootstrap + Validation"
        X[Bootstrap minimal core]
        Y[Flux applies manifests]
        Z[Validation stories 45‚Äë49]
    end

    A --> X
    B --> X
    C --> X
    D --> Y
    E --> Y
    F --> Y
    G --> Y

    subgraph "Security & Observability"
        H[Network Policies]
        I[Hubble Monitoring]
        J[Service Mesh]
    end

    D --> H
    E --> I
    F --> J
```

### üîß Core Installation (Bootstrap)

| Component | Method | Key Settings | Purpose |
|---|---|---|---|
| **üåê Cilium Agent** | Helmfile (DaemonSet) | `kubeProxyReplacement: true` | eBPF data plane, replaces kube-proxy |
| **‚öôÔ∏è Cilium Operator** | Helmfile (Deployment) | WireGuard encryption enabled | Control plane management |
| **üîí WireGuard** | Built-in | Transparent encryption | Node-to-node encryption |

### üìÅ Networking Manifests Layout

Located under `kubernetes/infrastructure/networking/cilium/*`:

| Feature | Path | Component | Function |
|---|---|---|---|
| **üîó BGP Peering** | `bgp/` | `CiliumBGPPeeringPolicy` | Pod/LB IP advertisement |
| **üåâ Gateway API** | `gateway/` | `GatewayClass` + `Gateways` | North-south traffic management |
| **üîó ClusterMesh** | `clustermesh/` | `ExternalSecret` | Cross-cluster connectivity |
| **üìä IPAM** | `ipam/` | `CiliumLoadBalancerIPPool` | L2/LB IP pool management |

Gateway policy note: This platform uses Cilium Gateway exclusively; Envoy Gateway is not part of this design.

### üîë API Endpoint Selection for Cilium (kube‚Äëproxy‚Äëfree)
- Default (Option A): use the cluster control‚Äëplane DNS on port 6443.
  - `K8S_SERVICE_HOST`: `infra-k8s.monosense.io` / `apps-k8s.monosense.io` (per cluster)
  - `K8S_SERVICE_PORT`: `6443`
- Optional overlay: you may point to Talos KubePrism (`127.0.0.1:7445`) by changing the two variables in `cluster-settings` ‚Äî no code changes required.
- Set both values explicitly whenever `kubeProxyReplacement: true` is enabled so hostNetwork agents can reach the apiserver reliably.

Minimal HelmRelease values (excerpt):
```yaml
values:
  cluster:
    name: ${CLUSTER}
    id: ${CLUSTER_ID}
  ipv4NativeRoutingCIDR: ${POD_CIDR_STRING}
  kubeProxyReplacement: true
  k8sServiceHost: ${K8S_SERVICE_HOST}
  k8sServicePort: ${K8S_SERVICE_PORT}
  encryption:
    enabled: true
    type: wireguard
  hubble:
    enabled: true
    relay:
      enabled: true
    ui:
      enabled: false
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
```

Scope boundaries:
- The Cilium ‚Äúcore‚Äù HelmRelease configures the base dataplane (kube‚Äëproxy replacement, encryption, Hubble, metrics).
- BGP Control Plane and Gateway API are authored in their dedicated directories (`bgp/`, `gateway/`) and reconciled by Flux separately.

### üî¢ LoadBalancer IP Allocation

**Network Topology:** Both clusters share the same L2 subnet (`10.25.11.0/24`) and peer with BGP router at `10.25.11.1` (ASN 64501).

**IP Pool Allocation Strategy:**

| Range | Cluster | Purpose | Reserved IPs | Pool Size |
|---|---|---|---|---|
| **10.25.11.100-119** | Infra | LoadBalancer Services | `.100` (ClusterMesh), `.110` (Gateway) | 20 IPs |
| **10.25.11.120-139** | Apps | LoadBalancer Services | `.120` (ClusterMesh), `.121` (Gateway) | 20 IPs |

**Key Design Principles:**
- ‚úÖ **Pool Isolation**: Pools use `disabled` flag via cluster-settings to prevent cross-cluster allocation
- ‚úÖ **Clean Segmentation**: No IP overlap, contiguous ranges per cluster
- ‚úÖ **Predictable Allocation**: ClusterMesh at start of each pool, Gateway follows
- ‚úÖ **Room for Growth**: ~17 IPs available per cluster for future LoadBalancer services
- ‚úÖ **BGP-Friendly**: Each cluster advertises only its own pool range via BGP Control Plane

**Specific IP Assignments:**

| Service | Infra Cluster | Apps Cluster | Purpose |
|---|---|---|---|
| **ClusterMesh API** | `10.25.11.100` | `10.25.11.120` | Cross-cluster service discovery |
| **Gateway API** | `10.25.11.110` | `10.25.11.121` | North-south HTTP(S) traffic |
| **Available Pool** | `.111-.119` | `.122-.139` | Future LoadBalancer services |

**Pool Isolation Mechanism:**
```yaml
# Controlled via cluster-settings ConfigMap
# Infra cluster:
INFRA_POOL_DISABLED: "false"
APPS_POOL_DISABLED: "true"

# Apps cluster:
INFRA_POOL_DISABLED: "true"
APPS_POOL_DISABLED: "false"
```

This ensures that when shared infrastructure manifests deploy to both clusters, each cluster only allocates from its designated pool.

### ü™û Spegel: Cluster-Local OCI Registry Mirror

**Installation**: Authored as manifests pre‚Äëbootstrap; applied during Stories 45‚Äë49

Spegel provides distributed P2P image caching across all cluster nodes, significantly improving image pull performance and reducing external registry bandwidth.

| Aspect | Configuration | Notes |
|---|---|---|
| **üì¶ Deployment** | DaemonSet (kube-system namespace) | Runs on all nodes for local mirror access |
| **üîå Registry Endpoint** | HostPort `29999` | Local mirror accessible at `localhost:29999` on each node |
| **‚öôÔ∏è Talos Integration** | `containerdRegistryConfigPath: /etc/cri/conf.d/hosts` | Dynamic containerd registry configuration (no machine config changes required) |
| **üíæ Storage** | In-memory cache (stateless) | No persistent volumes required |
| **üìä Observability** | ServiceMonitor + Grafana dashboard | Cache hit rate, pull performance metrics |
| **üîÑ Upstream Fallback** | Automatic | Falls back to upstream registry if image not in local cache |

**How It Works**:
1. **First Pull**: Node pulls image from upstream registry (ghcr.io, docker.io, etc.) ‚Äî normal speed
2. **Cache**: Spegel caches image locally on that node
3. **Subsequent Pulls**: Other nodes pull from Spegel mirror (P2P) ‚Äî ~10x faster, no external bandwidth
4. **Benefits**: Faster deployments, reduced egress costs, improved resilience (works during upstream outages)

**Talos Compatibility**: Fully supported via containerd's dynamic registry configuration. Spegel updates `/etc/cri/conf.d/hosts` at runtime (no Talos machine config modifications required).

**Bootstrap Integration**: Included in `bootstrap/helmfile.d/01-core.yaml.gotmpl` after CoreDNS to accelerate subsequent component installations (cert-manager, Flux, etc.).

### üöÄ Key Benefits

| Benefit | Description | Impact |
|---|---|---|
| **‚ö° High Performance** | eBPF-based data plane | Near bare-metal network performance |
| **üîí Security** | WireGuard + Network Policies | Defense-in-depth network security |
| **üîó Multi-Cluster** | ClusterMesh integration | Seamless cross-cluster service discovery |
| **üìä Observability** | Hubble + Prometheus integration | Full network visibility and monitoring |
| **üéõÔ∏è Flexibility** | Gateway API + BGP support | Advanced traffic routing capabilities |
| **ü™û Image Caching** | Spegel P2P registry mirror | Faster pulls, reduced external bandwidth, improved resilience |

## 10. üíæ Storage Architecture

> **üóÑÔ∏è High-Performance Distributed Storage**
>
> Our multi-cluster storage strategy provides optimal performance and isolation while maintaining operational simplicity.

### üèóÔ∏è Storage Architecture Overview

```mermaid
graph TB
    subgraph "Infrastructure Cluster"
        A1[Rook-Ceph Operator]
        A2[Rook-Ceph Cluster]
        A3[Block/File/Object Storage]
        A4[OpenEBS LocalPV]
        A1 --> A2
        A2 --> A3
        A4 --> A3
    end

    subgraph "Application Cluster"
        B1[Rook-Ceph Operator]
        B2[Rook-Ceph Cluster]
        B3[App-focused Storage]
        B4[OpenEBS LocalPV - Default]
        B1 --> B2
        B2 --> B3
        B4 --> B3
    end

    A3 -.-> |Cross-cluster services| B3
    B3 -.-> |High-performance local| B3
```

### üìä Cluster Storage Comparison

| Cluster | Storage Solution | Primary Use | Performance Target | Key Features |
|---|---|---|---|---|
| **üè≠ Infra** | üóÑÔ∏è Rook-Ceph + OpenEBS | Platform services, databases | High throughput NVMe | Block/file/object, monitoring |
| **üéØ Apps** | üóÑÔ∏è Dedicated Rook-Ceph + OpenEBS | Application workloads | Multi-GB/s local NVMe | Isolated storage, default local PV |

### üéØ Why Dedicated Storage for Apps Cluster?

| Challenge | Solution | Benefit |
|---|---|---|
| **üö´ Network Bottleneck** - 1 Gbps router cap | **Local Ceph cluster** on apps side | Eliminates cross-cluster I/O bottleneck |
| **‚ö° Performance** - Latency sensitive workloads | **In-cluster replication** and backfill | Multi-GB/s NVMe-backed performance |
| **üîß Operational Simplicity** | **Aligned Ceph versions** (v19.2.3) | Simplified management and tooling |
| **üõ°Ô∏è Isolation** | Dedicated storage per cluster | Prevents noisy neighbor problems |

### üóÑÔ∏è Storage Classes & Usage

| Storage Class | Cluster | Type | Use Case | Performance |
|---|---|---|---|---|
| **rook-ceph-block** | Infra + Apps | Block storage | Databases, stateful apps | High IOPS, low latency |
| **rook-ceph-fs** | Infra | File storage | Shared file systems | Concurrent access |
| **openebs-local-nvme** | Infra + Apps | Local storage | High-performance workloads | NVMe speed, local only |
| **rook-ceph-object** | Infra | Object storage | S3-compatible storage | Scalable object access |

### üìà Performance Optimizations

| Optimization | Implementation | Impact |
|---|---|---|
| **üîß NVMe Device Class** | `ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"` | Optimized SSD performance |
| **üìä Local NVMe Priority** | OpenEBS as default for apps | Maximum I/O performance |
| **üîÑ Replication Strategy** | Local cluster replication | Minimal cross-cluster traffic |
| **üìà Monitoring** | Ceph metrics + alerts | Performance visibility |

### üõ†Ô∏è Operational Benefits

| Feature | Description | Operational Impact |
|---|---|---|
| **üîß Version Alignment** | Same Ceph v19.2.3 across clusters | Simplified upgrades, tooling consistency |
| **üìä Integrated Monitoring** | Ceph + Prometheus + Grafana | Full storage observability |
| **üö® Automated Alerts** | Storage health and performance alerts | Proactive issue detection |
| **üìñ Standardized Runbooks** | Consistent operational procedures | Reduced operational complexity |

### üéõÔ∏è Configuration Highlights

```yaml
# From cluster-settings.yaml
ROOK_CEPH_IMAGE_TAG: v19.2.3
ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"
BLOCK_SC: "rook-ceph-block"  # Generic block storage
OPENEBS_LOCAL_SC: "openebs-local-nvme"  # Local NVMe storage
OPENEBS_BASEPATH: "/var/mnt/openebs"
```

## 11. üìä Observability Strategy

> **üîç Centralized Monitoring & Logging**
>
> Our observability architecture provides comprehensive visibility across both clusters with centralized storage and distributed collection.

### üèóÔ∏è Architecture Overview

```mermaid
graph TB
    subgraph "Infrastructure Cluster (Central)"
        A1[VictoriaMetrics Global]
        A2[VictoriaLogs]
        A3[Grafana]
        A4[Alertmanager]
        A5[Flux Alert Providers]
    end

    subgraph "Application Cluster (Leaf)"
        B1[vmagent]
        B2[kube-state-metrics]
        B3[node-exporter]
        B4[Fluent Bit]
        B5[OpenTelemetry Collector]
    end

    subgraph "Data Flow"
        C1[Metrics Collection]
        C2[Log Collection]
        C3[Trace Collection]
    end

    B1 --> C1
    B2 --> C1
    B3 --> C1
    B4 --> C2
    B5 --> C3

    C1 --> A1
    C2 --> A2
    C3 --> A1

    A1 --> A3
    A1 --> A4
    A5 --> A4
```

### üéØ Cluster Strategy

| Cluster | Role | Components | Data Flow |
|---|---|---|---|
| **üè≠ Infra** | **Central Storage & Visualization** | VictoriaMetrics global, VictoriaLogs, Grafana, Alertmanager | Stores all cluster data, provides dashboards/alerts |
| **üéØ Apps** | **Lightweight Collection** | vmagent, kube-state-metrics, node-exporter, Fluent Bit | Forwards all data to infra cluster |

### üì± Apps Cluster: Leaf Observability Pack

#### üîÑ Metrics Collection (Pull + Forward)

| Component | Purpose | Data Flow | Resource Usage |
|---|---|---|---|
| **üì° vmagent** | Discovers and scrapes targets | Remote write to infra vminsert via vmauth | 100-300m CPU / 256-512Mi RAM |
| **üìä kube-state-metrics** | Kubernetes object metrics | Scraped by vmagent | Lightweight |
| **üñ•Ô∏è node-exporter** | OS/host metrics (CPU, memory, disk I/O, network) | Scraped by vmagent | Essential for host visibility |
| **üåê Cilium/Hubble** | Network/L7 metrics (optional) | ServiceMonitors ‚Üí vmagent | Network visibility |

#### üìù Logs Collection (Push)

| Component | Function | Configuration | Resource Usage |
|---|---|---|---|
| **üìã Fluent Bit** | Ships container/kubelet/audit logs | Compression, batching, `cluster=apps` labels | 50-200m CPU / 128-256Mi RAM |
| **üîó vmauth** | Multi-tenant routing | Insert endpoint with TLS/auth | Minimal overhead |

#### üîç Traces Collection (Optional)

| Component | Function | Integration | Notes |
|---|---|---|---|
| **üîç OpenTelemetry Collector** | Receives OTLP from applications | Exports to infra tracing backend | DaemonSet or agent mode |

### ü§î Why Keep node-exporter with vmagent?

| Component | Role | Complementarity |
|---|---|---|
| **üì° vmagent** | Prometheus-compatible scraper/forwarder | Discovers and pulls metrics, doesn't generate host metrics |
| **üñ•Ô∏è node-exporter** | Canonical OS/host signals source | Provides CPU, filesystem saturation, network, thermal data |
| **üí° Synergy** | Complete visibility | kubelet/cAdvisor insufficient for host-level detail |

### ‚öôÔ∏è Configuration Details (Apps ‚Üí Infra)

#### üîó Endpoints (from cluster-settings)

```yaml
# Metrics Configuration
GLOBAL_VM_INSERT_ENDPOINT: "victoria-metrics-global-vminsert.observability.svc.cluster.local:8480"
OBSERVABILITY_LOG_ENDPOINT_HOST: "victorialogs-vmauth.observability.svc.cluster.local"
OBSERVABILITY_LOG_ENDPOINT_PORT: "9428"
OBSERVABILITY_LOG_TENANT: "apps"
```

#### üõ°Ô∏è NetworkPolicy Configuration

| Direction | Allowed Traffic | Ports | Purpose |
|---|---|---|---|
| **Egress** | DNS + kube-apiserver | 53, 443 | Cluster functionality |
| **Egress** | infra vmauth/vminsert | 8480/8427 | Metrics forwarding |
| **Egress** | VictoriaLogs insert | 9428 | Log forwarding |
| **Default** | Deny all other traffic | - | Security |

#### üîê Security Configuration

| Component | Secret Management | Access Pattern |
|---|---|---|
| **vmagent** | External Secrets | Client credentials + CA roots |
| **Fluent Bit** | External Secrets | TLS certs for log shipping |
| **OTel Collector** | External Secrets | Tracing backend access |

### üö´ What We DON'T Run on Apps Cluster

| Component | Reason | Alternative |
|---|---|---|
| **üìä VictoriaMetrics TSDB** | Storage consolidation | Remote write to infra |
| **üö® VMAlert/Alertmanager** | Centralized alerting | Infra cluster handles all alerts |
| **üìà Grafana** | Single visualization layer | Access infra Grafana via network |
| **üíæ Long-term storage** | Cost efficiency | Central storage on infra |

### üìã CRD Requirements for Apps Cluster

#### Required CRD Bundles

| CRD Set | Purpose | Examples |
|---|---|---|
| **üìä VictoriaMetrics Operator** | VM resource definitions | VMAgent, VMServiceScrape, VMRule, VMAuth, VMUser |
| **üîÑ Prometheus Operator (Compatibility)** | Upstream chart support | ServiceMonitor, PodMonitor, PrometheusRule |

#### üöÄ Bootstrap Method

```bash
# Phase 0: Install CRDs on both clusters
helmfile -f bootstrap/helmfile.d/00-crds.yaml -e apps template \
  | yq ea 'select(.kind == "CustomResourceDefinition")' \
  | kubectl apply -f -
```

### üìà Scaling & Performance

| Metric | Starting Point | Scaling Guidance |
|---|---|---|
| **vmagent CPU** | 100-300m | Scale with scrape cardinality |
| **vmagent Memory** | 256-512Mi | Scale with metric volume |
| **Fluent Bit CPU** | 50-200m | Scale with log throughput |
| **Fluent Bit Memory** | 128-256Mi | Scale with buffer size |
| **Network Bandwidth** | Depends on log/metric volume | Monitor compression ratios |

### ‚úÖ Benefits of This Architecture

| Benefit | Description | Impact |
|---|---|---|
| **üí∞ Cost Efficiency** | Single storage backend | Reduced infrastructure costs |
| **‚ö° Performance** | Local aggregation, remote forwarding | Low latency collection |
| **üîß Simplicity** | Centralized management | Easier operations |
| **üõ°Ô∏è Security** | Controlled egress paths | Reduced attack surface |
| **üìà Scalability** | Distributed collection | Linear scaling capability |


## 12. üîÑ CI/CD & Policy

| Component | Implementation | Details |
| :--- | :--- | :--- |
| **üîç CI Validation** | Pipeline checks | `kubeconform` (strict), `kustomize build` for each cluster entry, `flux build`/`flux diff` |
| **üõ°Ô∏è Policy Management** | Admission control | Start with audit-mode ValidatingAdmissionPolicy or Kyverno; then enforce baseline/restricted policies |
| **üîê Image Security** | Provenance verification | Add image provenance (cosign/notation) where applicable |
| **ü§ñ Image Automation** | Selected apps | `ImageRepository`, `ImagePolicy`, `ImageUpdateAutomation` writing to staging branches |

## 13. üè¢ Multi‚ÄëTenancy (optional)

| Aspect | Implementation | Details |
| :--- | :--- | :--- |
| **üìÅ Team Structure** | Directory layout | `workloads/tenants/<team>`: Namespace + ServiceAccount + RBAC |
| **üîß Resource Management** | Team-scoped resources | `Kustomization`/`HelmRelease` with `serviceAccountName` specified |
| **üö´ Isolation Rules** | Namespace boundaries | Enforce: no cross-namespace refs; lock Flux flags (`--no-cross-namespace-refs`, `--no-remote-bases`) |
| **üìà Scaling Strategy** | Performance optimization | Scale via controller sharding and `--watch-label-selector` if needed |

## 14. üîß Operations & Runbooks (abridged)

| Operation | Command/Process | Description |
| :--- | :--- | :--- |
| **üöÄ Fresh Cluster** | `task bootstrap:talos` ‚Üí `task bootstrap:apps` ‚Üí `flux get ks -A` | Complete cluster bootstrap sequence |
| **üîÑ Bootstrap Re-run** | CRDs ‚Üí `helmfile sync` ‚Üí suspend Flux | Safe re-run of bootstrap phases with Flux suspension if necessary |
| **‚è∏Ô∏è Flux Control** | `flux suspend\|resume kustomization <name>` | Pause/resume specific Kustomizations |
| **üîô Rollbacks** | `helm rollback` / Git revert | Bootstrap charts: `helm rollback`; GitOps resources: Git revert + Flux reconcile |
| **‚¨ÜÔ∏è Node Upgrades** | `.taskfiles/talos/upgrade-node` | Talos node upgrades with drain logic; verify Cilium + storage DaemonSets availability |

## 15. üìÖ Phased Implementation Plan (Sprints)

| Sprint | Focus | Key Tasks |
| :--- | :--- | :--- |
| **Sprint 0** | üèóÔ∏è Foundations | Lock controller flags; decide bootstrap ownership; add CI scaffolding |
| **Sprint 1** | üìÅ Repo Skeleton | Create/normalize cluster entries; add `bootstrap/` and `.taskfiles/bootstrap`; ensure values reuse between Helmfile and Flux |
| **Sprint 2** | üîê Secrets & Decryption | Finalize External Secrets with 1Password for all secrets (bootstrap/runtime) |
| **Sprint 3** | ‚öôÔ∏è Platform Controllers | External Secrets, cert-manager CRDs/issuers, CNPG (if used); health checks and ordering |
| **Sprint 4** | üåê Networking Validation | Deploy and validate Cilium core + BGP + Gateway API + ClusterMesh per validation stories |
| **Sprint 5** | üíæ Storage | Infra Ceph cluster; Apps client/operator (optional); PVC tests and monitoring |
| **Sprint 6** | üìä Observability | VM global stack in infra; remote write from apps; Flux Alerts/Receivers |
| **Sprint 7** | üîÑ CI/CD & Policy | kubeconform/kustomize/flux build; policy audit‚Üíenforce; image automation (staging) |
| **Sprint 8** | üöÄ Workloads Migration | Normalize app bases/overlays; migrate an anchor app end‚Äëto‚Äëend; rollback test |
| **Sprint 9** | üè¢ Tenancy & RBAC (optional) | Team namespaces + RBAC; per‚Äëteam Kustomizations; isolation verification |
| **Sprint 10** | üõ°Ô∏è Reliability, DR, Hardening | Backups/restore drills; PodSecurity; image provenance; finalize runbooks |

## 16. üìù Decisions & Rationale

| Decision | Rationale |
| :--- | :--- |
| **üîß Flux Bootstrap Method** | Flux is bootstrapped via Helmfile, not self‚Äëmanaged in this repo (simpler, deterministic bootstrap) |
| **üö´ Remove Aggregator** | Remove aggregator `ks.yaml` to avoid duplication; wire clusters directly to directories |
| **üåê Cilium Management** | Author all Cilium core and related features as manifests first; bootstrap later; Flux manages ongoing reconciliation |
| **üìÅ Source Strategy** | Prefer local Git/OCI sources; avoid remote bases and cross‚Äënamespace references |

## 17. ‚ö†Ô∏è Risks & Mitigations

| Risk | Mitigation Strategy |
| :--- | :--- |
| **üìã CRD Ordering Issues** | Two‚Äëphase bootstrap; `--include-crds`, postRenderer filter |
| **üîê Secret Store Outages** | Alert on ES sync; ensure 1Password Connect is HA |
| **üåê Network Disruption** | Guard BGP/Gateway changes behind Kustomization toggles; staged rollouts |
| **‚öôÔ∏è Controller Overload** | Shard controllers, use label selectors, tune reconcile intervals |

## 18. ‚úÖ Acceptance Criteria & Metrics

### üî¨ Technical Criteria
- **üìä Health Status**: 100% of Kustomizations healthy; zero missing `dependsOn`; CI green on kubeconform/kustomize/flux build
- **‚ö° Performance**: Mean reconciliation time within target; alert coverage for Flux, cert-manager, storage, and Cilium

### üìà Process Metrics
- **üîÑ Throughput**: PR throughput/predictability; change failure rate
- **üõ°Ô∏è Reliability**: Successful restore drills; rollback MTTR |

---

## 19. üõ†Ô∏è Workloads & Versions

> **üìÖ Version Snapshot: 2025-10-20**
>
> The tables below list the platform workloads we deploy per cluster and the versions we will pin at bootstrap. Versions reflect the latest stable charts/releases available on October 20, 2025. We keep them explicit to ensure reproducible installs; upgrades follow our normal PR process.

### üè≠ Infrastructure Cluster - Platform Services

| Component | üè∑Ô∏è Version | üì¶ Namespace | üîß Install Method | üìù Purpose & Notes |
|---|---|---|---|---|
| **üåê Cilium** | `1.18.3` | `kube-system` | Helm (OCI) | Core CNI; features managed via Flux (BGP/Gateway/ClusterMesh) |
| **üîç CoreDNS** | `1.38.0` | `kube-system` | Helm (OCI) | Cluster DNS resolution |
| **üîê External Secrets** | `0.20.3` | `external-secrets` | Helm (repo) | 1Password Connect integration |
| **üîí cert-manager** | `v1.19.1` | `cert-manager` | Helm (OCI) | Cluster issuers + ACME automation |
| **üóÑÔ∏è Rook‚ÄëCeph Operator** | `latest` | `rook-ceph` | Helm (repo) | Storage operator; Ceph v19.2.3 pinned |
| **üóÑÔ∏è Rook‚ÄëCeph Cluster** | `latest` | `rook-ceph` | Helm (repo) | CephCluster + pools/SCs; Ceph v19.2.3 |
| **üíæ OpenEBS LocalPV** | `4.3.x` | `openebs-system` | Helm (repo) | High-performance NVMe storage |
| **üìä VictoriaMetrics** | `0.61.8` | `observability` | Helm (repo/OCI) | Metrics stack (infra only) |
| **üìù VictoriaLogs** | `0.0.17` | `observability` | Helm (OCI) | Centralized logging |
| **üìã Fluent Bit** | `0.53.0` | `observability` | Helm (repo) | Log shipping to VictoriaLogs |
| **üì¶ Harbor Registry** | `1.18.0` | `harbor` | Helm (repo) | Container registry; app v2.14.0 |
| **üöÄ Actions Runner** | `0.12.0` | `actions-runner-system` | Helm (OCI) | GitHub ARC controller |
| **üêâ Dragonfly Operator** | `1.3.0` | `dragonfly-operator-system` | Helm (OCI) | Manages Dragonfly CRs and Services |
| **üêâ Dragonfly Cluster** | `v1.17.0` (image) | `dragonfly-system` | Kustomize (CR) | Shared Redis‚Äëcompatible cache; cross‚Äëcluster via Cilium Global Service |

### üéØ Application Cluster - Workloads & Services

| Component | üè∑Ô∏è Version | üì¶ Namespace | üîß Install Method | üìù Purpose & Notes |
|---|---|---|---|---|
| **üåê Cilium** | `1.18.3` | `kube-system` | Helm (OCI) | Core CNI; features managed via Flux |
| **üîç CoreDNS** | `1.38.0` | `kube-system` | Helm (OCI) | Cluster DNS resolution |
| **üîê External Secrets** | `0.20.3` | `external-secrets` | Helm (repo) | 1Password Connect integration |
| **üîí cert-manager** | `v1.19.1` | `cert-manager` | Helm (OCI) | Cluster issuers + ACME automation |
| **üóÑÔ∏è Rook‚ÄëCeph Operator** | `latest` | `rook-ceph` | Helm (repo) | Storage operator; Ceph v19.2.3 pinned |
| **üóÑÔ∏è Rook‚ÄëCeph Cluster** | `latest` | `rook-ceph` | Helm (repo) | Dedicated apps storage; Ceph v19.2.3 |
| **üíæ OpenEBS LocalPV** | `4.3.x` | `openebs-system` | Helm (repo) | Default storage; openebs-local-nvme |
| **üì¨ Kafka Operator** | `0.48.0` | `messaging` | Helm (repo) | Strimzi Kafka platform |
| **üóÇÔ∏è Schema Registry** | `latest` | `messaging` | Kustomize | Confluent Schema Registry |
| **üêò PostgreSQL Operator** | `0.26.0` | `cnpg-system` | Helm (repo) | CloudNativePG for app databases |
| **üìã Fluent Bit** | `0.53.0` | `observability` | Helm (repo) | Ships logs/metrics to infra |
| **üöÄ Actions Runner** | `0.12.0` | `actions-runner-system` | Helm (OCI) | GitHub ARC + scale sets |

### üéõÔ∏è Optional/Edge Components

| Component | üè∑Ô∏è Version | ‚ö†Ô∏è Status | üìù Notes |
|---|---|---|---|
| **ü™û Spegel** | `0.4.0` | ‚úÖ Enabled | Cluster-local OCI registry mirror; Talos-compatible via `/etc/cri/conf.d/hosts` (P2P image caching reduces external bandwidth) |
<!-- Envoy Gateway removed: this platform uses Cilium Gateway exclusively -->

### üìã Version Management Strategy

```mermaid
graph TD
    A[Version Pinning] --> B[Reproducible Installs]
    A --> C[No Accidental Drifts]
    A --> D[Controlled Upgrades]

    E[PR Process] --> F[kubeconform checks]
    E --> G[kustomize build]
    E --> H[flux build validation]
    E --> I[Rollout plan]

    J[Rook-Ceph] --> K[Ceph Image Pinned]
    J --> L[Operator Chart Tracking]
    J --> M[Upgrade Documentation]
```

### üéØ Version Stewardship Policies

| Policy | Implementation | Impact |
|---|---|---|
| **üìå Pin All Versions** | Helm chart versions + image tags in repo | Prevents accidental upgrades |
| **üîÑ PR-Based Upgrades** | All version changes require PR + validation | Controlled rollout process |
| **üìä Validation Pipeline** | kubeconform + kustomize + flux build checks | Ensures compatibility |
| **üìù Change Tracking** | Document exact chart versions in changelogs | Full audit trail |
| **üóÑÔ∏è Special Handling** | Rook‚ÄëCeph: Ceph image v19.2.3 pinned separately | Complex dependency management |

### ‚ö° Upgrade Process Flow

1. **üìã Planning** - Create upgrade plan with compatibility matrix
2. **üîß Testing** - Validate in staging environment
3. **üìù PR Creation** - Include all validation checks
4. **‚úÖ Review** - Architecture team approval
5. **üöÄ Deployment** - Automated via Flux
6. **üìä Monitoring** - Post-upgrade health verification
7. **üìñ Documentation** - Update changelog and runbooks

## 20. üîß CRD & API Standardization Matrix

> **üìã Custom Resource Definitions (CRDs)**
>
> This section defines the exact CRD versions and API groups we standardize on across our multi-cluster GitOps infrastructure. All CRDs are installed via bootstrap Phase 0 and managed through GitOps manifests.

### üèóÔ∏è Core Infrastructure CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üåê Cilium CNI** | `1.18.3` | `cilium.io/v2` | `CiliumNetworkPolicy`, `CiliumBGPPeeringPolicy`, `CiliumLoadBalancerIPPool`, `CiliumClusterwideNetworkPolicy`, `CiliumAuthPolicy`, `CiliumClusterMesh`, `CiliumExternalWorkload`, `CiliumEndpoint`, `CiliumIdentity`, `CiliumNode` | eBPF networking, security policies, BGP, ClusterMesh |
| **üîÑ Flux CD v2** | `2.4+` | `kustomize.toolkit.fluxcd.io/v1`<br>`source.toolkit.fluxcd.io/v1`<br>`helm.toolkit.fluxcd.io/v2`<br>`notification.toolkit.fluxcd.io/v1`<br>`image.toolkit.fluxcd.io/v1beta2` | `Kustomization`, `GitRepository`, `HelmRepository`, `HelmRelease`, `OCIRepository`, `Receiver`, `Alert`, `Provider`, `ImageRepository`, `ImagePolicy`, `ImageUpdateAutomation` | GitOps orchestration and deployment automation |
| **üîí cert-manager** | `v1.19.1` | `cert-manager.io/v1` | `Certificate`, `Issuer`, `ClusterIssuer`, `CertificateRequest`, `ACMEChallenge`, `ACMEOrder`, `ACMEChallengeSolver` | TLS certificate management and automation |
| **üîê External Secrets** | `0.20.3` | `external-secrets.io/v1beta1` | `ExternalSecret`, `SecretStore`, `ClusterSecretStore`, `PushSecret` | External secret management and synchronization |

### üíæ Storage & Database CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üóÑÔ∏è Rook-Ceph** | `latest` (Ceph `v19.2.3`) | `ceph.rook.io/v1` | `CephCluster`, `CephBlockPool`, `CephFilesystem`, `CephObjectStore`, `CephObjectStoreUser`, `CephObjectRealm`, `CephObjectZone`, `CephObjectZoneGroup`, `CephRBDMirror`, `CephFilesystemMirror`, `CephNFS`, `CephClient`, `CephRBDMirror` | Distributed storage, block/file/object storage |
| **üíæ OpenEBS LocalPV** | `4.3.x` | `localvolumes.openebs.io/v1`<br>`lvm.openebs.io/v1`<br>`zfs.openebs.io/v1` | `LocalVolume`, `LocalVolumeSet`, `LVMVolume`, `LVMVolumeSnapshot`, `ZFSVolume`, `ZFSVolumeSnapshot` | High-performance local storage, NVMe optimization |
| **üêò CloudNativePG** | `1.25.0` | `postgresql.cnpg.io/v1` | `Cluster`, `Pooler`, `ScheduledBackup`, `Backup`, `Database`, `Publication`, `Subscription`, `ImageCatalog`, `BackupCatalog`, `ScheduledBackup` | PostgreSQL cluster management and high availability |
| **üêâ Dragonfly Operator** | `1.3.0` | `dragonflydb.io/v1beta1` | `Dragonfly`, `DragonflyCluster`, `DragonflyReplication` | Redis-compatible in-memory data store |

### üìä Observability & Monitoring CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üìà VictoriaMetrics** | `0.63.0` | `operator.victoriametrics.com/v1beta1` | `VMCluster`, `VMAgent`, `VMAlert`, `VMAuth`, `VMUser`, `VLSingle`, `VTCluster`, `VMServiceScrape`, `VMPodScrape`, `VMRule`, `VMProbe`, `VMScrapeConfig`, `VMAnomaly` | Metrics collection, storage, alerting, and visualization |
| **üìù VictoriaLogs** | `0.0.17` | `operator.victoriametrics.com/v1beta1` | `VLogs` | Centralized log aggregation and analysis |
| **üìã Fluent Bit** | `0.53.0` | `kustomize.toolkit.fluxcd.io/v1` (managed via Flux) | N/A (DaemonSet deployment) | Log collection and forwarding |
| **üîç Prometheus Operator** | `0.75.1` (for compatibility) | `monitoring.coreos.com/v1` | `ServiceMonitor`, `PodMonitor`, `PrometheusRule`, `Prometheus`, `Alertmanager`, `ScrapeConfig`, `Probe` | Metrics collection compatibility layer |

### üåê Networking & Gateway CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üåâ Gateway API** | `v1.0.0` (GA) | `gateway.networking.k8s.io/v1` | `GatewayClass`, `Gateway`, `HTTPRoute`, `GRPCRoute`, `TLSRoute`, `TCPRoute`, `UDPRoute`, `ReferenceGrant`, `BackendTLSPolicy`, `GatewayPolicy` (experimental) | Modern API for load balancing and traffic routing |
| **üö™ Cilium Gateway** | `1.18.3` | `cilium.io/v2alpha1` | `CiliumGateway`, `CiliumGatewayConfiguration` | eBPF-based gateway implementation |

### üîÑ Messaging & Streaming CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üì¨ Strimzi Kafka** | `0.48.0` | `kafka.strimzi.io/v1beta2` | `Kafka`, `KafkaTopic`, `KafkaUser`, `KafkaBridge`, `KafkaConnect`, `KafkaMirrorMaker2`, `KafkaNodePool`, `KafkaConnector`, `KafkaRebalance` | Apache Kafka cluster management and streaming platform |

### üöÄ CI/CD & Automation CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üèÉ GitHub Actions ARC** | `0.12.0` | `actions.summerwind.dev/v1alpha1` | `Runner`, `RunnerDeployment`, `RunnerReplicaSet`, `HorizontalRunnerAutoscaler`, `RunnerSet`, `SelfHostedRunner`, `EnterpriseRunner`, `OrganizationRunner`, `RepositoryRunner` | GitHub Actions self-hosted runners management |

### üîê Security & Policy CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üõ°Ô∏è PodSecurity** | `v1.2.0` | `policy/v1` (built-in) | `PodSecurityPolicy` (deprecated), `PodSecurityAdmission` | Pod security standards and enforcement |
| **üîë SPIRE** | `0.12.0` | `spireid.org/v1alpha1` | `ClusterFederatedTrustDomain`, `ClusterSPIFFEID`, `NodeSPIFFEID` | Zero-trust identity and workload authentication |

### üì¶ Package Management CRDs

| Operator | Version | API Group | Custom Resources | Purpose |
|---|---|---|---|---|
| **üîß Helm Toolkit** | `v2.7+` | `helm.toolkit.fluxcd.io/v2` | `HelmRelease` (listed under Flux) | Helm chart management |
| **üìã OCI Repository** | `v2.4+` | `source.toolkit.fluxcd.io/v1` | `OCIRepository` (listed under Flux) | OCI artifact management |

### üéØ API Version Standards & Compatibility

| Component | Standard API Version | Minimum Kubernetes Version | Notes |
|---|---|---|---|
| **Core Kubernetes** | `apps/v1`, `batch/v1`, `policy/v1` | `1.29+` | GA versions for stability |
| **Gateway API** | `gateway.networking.k8s.io/v1` | `1.29+` | Requires Gateway API support |
| **Flux CD** | `toolkit.fluxcd.io/v1` and `v2` | `1.28+` | Latest stable APIs |
| **Cilium** | `cilium.io/v2` | `1.28+` | Stable Cilium APIs |
| **cert-manager** | `cert-manager.io/v1` | `1.28+` | GA APIs only |
| **CNPG** | `postgresql.cnpg.io/v1` | `1.27+` | Stable database APIs |
| **Rook-Ceph** | `ceph.rook.io/v1` | `1.27+` | Mature storage APIs |
| **VictoriaMetrics** | `operator.victoriametrics.com/v1beta1` | `1.27+` | Beta APIs with operator support |
| **Strimzi** | `kafka.strimzi.io/v1beta2` | `1.27+` | Well-established Kafka APIs |
| **External Secrets** | `external-secrets.io/v1beta1` | `1.27+` | Stable secret management |

### üîß CRD Installation Strategy

#### **Phase 0: Bootstrap CRDs**
```bash
# Install all CRDs before operators
helmfile -f bootstrap/helmfile.d/00-crds.yaml -e <cluster> template \
  | yq ea 'select(.kind == "CustomResourceDefinition")' \
  | kubectl apply -f -
```

#### **Phase 1: Operator Installation**
```bash
# Install operators with CRD creation disabled
helmfile -f bootstrap/helmfile.d/01-core.yaml.gotmpl -e <cluster> sync
```

### üìã CRD Management Best Practices

| Practice | Implementation | Impact |
|---|---|---|
| **üîÑ Version Pinning** | Explicit CRD versions in bootstrap | Prevents accidental upgrades |
| **‚úÖ Validation** | `kubeconform` + `kustomize build` in CI | Ensures manifest compatibility |
| **üìä Monitoring** | CRD readiness checks in Flux Kustomizations | Guarantees proper installation |
| **üîß Upgrades** | Phased CRD updates before operators | Maintains system stability |
| **üìù Documentation** | Centralized API matrix tracking | Clear version standards |

### üö® CRD Health Checks & Validation

| Check Type | Command | Success Criteria |
|---|---|---|
| **CRD Installation** | `kubectl get crds | grep -E "(cilium|flux|cert-manager|rook|cnpg|victoria-metrics)"` | All expected CRDs present |
| **API Readiness** | `kubectl api-resources | grep -E "(cilium|flux|cert-manager|rook|cnpg|victoria-metrics)"` | All API groups registered |
| **Operator Health** | `kubectl get pods -n <namespace> -l app.kubernetes.io/name=<operator>` | All operators running |
| **Flux Reconciliation** | `flux get kustomizations -A` | All Kustomizations Ready |
| **CRD Compatibility** | `kubectl get crd <crd-name> -o jsonpath='{.spec.versions[*].name}'` | Expected versions available |

### üîÆ Future CRD Considerations

| Emerging CRD | Status | Evaluation Criteria |
|---|---|---|
| **Kyverno Policies** | Consider | Security policy engine alternatives |
| **Karpenter** | Evaluate | Node lifecycle management |
| **Crossplane** | Monitor | Cloud resource management |
| **KEDA** | Potential | Event-driven autoscaling |
| **Backstage** | Future | Developer portal integration |

**üéØ Standardization Principle**: We prioritize GA APIs over beta/alpha versions for production stability, while adopting mature beta APIs only when necessary for critical functionality.

## 21. üîó Cilium ClusterMesh + SPIRE (Zero‚ÄëTrust, Multi‚ÄëCluster)

> **üõ°Ô∏è Secure Multi-Cluster Identity & Connectivity**
>
> This section describes how we achieve secure, multi‚Äëcluster connectivity and identity with Cilium ClusterMesh and SPIRE, and how we operate it day‚Äëto‚Äëday.

### üéØ Zero-Trust Goals

| Goal | Description | Implementation |
| :--- | :--- | :--- |
| **üåê Seamless Connectivity** | L3/L4/L7 connectivity between clusters with native service discovery | Cilium ClusterMesh + Global Services |
| **üîê Identity-Based Auth** | Mutual authentication and mTLS based on SPIFFE identities, not IPs | SPIRE + Cilium Auth Policies |
| **üõ°Ô∏è Zero-Trust Policy** | Permit only authenticated/authorized traffic; deny by default | CiliumNetworkPolicy + CiliumAuthPolicy |
| **üìä Full Observability** | Complete flow visibility and metrics across clusters | Hubble + Hubble Relay |

### üèóÔ∏è Component Architecture

```mermaid
graph TB
    subgraph "Infrastructure Cluster"
        C1[Cilium Agent/Operator]
        CM1[ClusterMesh API Server]
        SPIRE1[SPIRE Server + Agents]
        HB1[Hubble + Relay]
    end

    subgraph "Application Cluster"
        C2[Cilium Agent/Operator]
        CM2[ClusterMesh API Server]
        SPIRE2[SPIRE Server + Agents]
        HB2[Hubble + Relay]
    end

    subgraph "External Services"
        ES[External Secrets]
        DNS[DNS Records]
        LB[Load Balancers]
    end

    CM1 <--> |Clustermesh| CM2
    SPIRE1 <--> |Federated Trust| SPIRE2
    HB1 <--> |Flow Data| HB2

    ES --> CM1
    ES --> CM2
    DNS --> LB
    LB --> CM1
    LB --> CM2
```

### üìã Component Roles & Responsibilities

| Component | Role | Key Features |
| :--- | :--- | :--- |
| **üåê Cilium (agent/operator)** | CNI + Data Plane | eBPF acceleration, Gateway API, BGP control plane |
| **üîó ClusterMesh API Server** | Cross-cluster control plane | Service discovery, identity exchange, LoadBalancer exposed |
| **üõ°Ô∏è SPIRE (Server + Agent)** | Identity management | SPIFFE SVID issuance, short-lived certs, pod-level identity |
| **üìä Hubble + Relay** | Observability | Network flow visibility, metrics, security monitoring |
| **üîê External Secrets** | Secret distribution | Secure ClusterMesh material distribution across clusters |

### 20.3 üîß Control‚ÄëPlane Topology

| Configuration | Value/Setting | Purpose |
| :--- | :--- | :--- |
| **üè∑Ô∏è Cluster Identity** | `cluster.name`: `infra` or `apps` | Unique cluster identification |
| **üî¢ Cluster ID** | `cluster.id`: `1` (infra), `2` (apps) | Numeric ID unique per cluster |
| **üåê ClusterMesh API** | `clustermesh.useAPIServer: true` | Enable ClusterMesh API server |
| **üîÑ Load Balancer** | `clustermesh.apiserver.service.type: LoadBalancer` | Expose API server via LB with DNS |
| **üåê DNS Records** | `infra-cilium-apiserver.<domain>` ‚Üí infra LB<br>`apps-cilium-apiserver.<domain>` ‚Üí apps LB | ClusterMesh endpoint resolution |
| **üîê Secret Management** | ExternalSecret `kube-system/cilium-clustermesh` | Per‚Äëpeer CA, client/server certs, endpoints |

### 20.4 üõ°Ô∏è Workload Identity & mTLS with SPIRE

| Component | Function | Configuration Details |
| :--- | :--- | :--- |
| **üîê Authentication Provider** | SPIRE as Cilium auth provider | SPIRE server + agents installed; agents run on each node |
| **üé´ Identity Issuance** | SVID minting for pods | Based on k8s selectors; SPIFFE ID format: `spiffe://monosense.io/ns/<ns>/sa/<serviceaccount>` |
| **‚è∞ Rotation Management** | SVID TTL and rotation | Configurable; default frequent rotation (hours) to minimize key longevity |
| **üîó Defense in Depth** | Multi-layer encryption | WireGuard (node‚Äëto‚Äënode) + SPIRE (workload identity + mTLS) |

### 20.5 üõ°Ô∏è Policy Model

| Policy Aspect | üîß Implementation | üìã Purpose |
| :--- | :--- | :--- |
| **üÜî Identity‚ÄëCentric Auth** | `CiliumNetworkPolicy`/`CiliumClusterwideNetworkPolicy` + `CiliumAuthPolicy` | Authorization based on workload identity, not IP |
| **üîê mTLS Requirements** | Require mTLS between specific identities/namespaces | Secure communication with SPIFFE authentication |
| **üö´ Default Deny** | Baseline policy denies all traffic by default | Security-first approach with explicit allow rules |
| **üéØ L4/L7 Control** | Allow only intended directions and ports | Precise traffic control and attack surface reduction |

#### üîê SPIRE Authentication Example

**Require SPIRE authentication for traffic to a namespace:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumAuthPolicy
metadata:
  name: require-mtls-to-synergyflow
  namespace: synergyflow
spec:
  selectors:
    - namespace: synergyflow
  requiredAuthentication:
    - type: spiffe
```

#### üåç Global Service Example

**Global service for cross‚Äëcluster failover:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: schema-registry
  namespace: messaging
  annotations:
    io.cilium/global-service: "true"
spec:
  selector:
    app.kubernetes.io/name: schema-registry
  ports:
    - name: http
      port: 8081
      targetPort: 8081
```

### 20.6 üöÄ Day‚Äë0/Day‚Äë1 Setup (per cluster)

| Phase | üîß Configuration Details | ‚úÖ Outcome |
| :--- | :--- | :--- |
| **üîß Phase 1**<br>*(Cilium Installation)* | **Helmfile installs Cilium with:**<br>‚Ä¢ `cluster.name`, `cluster.id`, `ipv4NativeRoutingCIDR`<br>‚Ä¢ `clustermesh.useAPIServer: true`<br>‚Ä¢ `apiserver.service.type: LoadBalancer`<br>‚Ä¢ `authentication.mutual.spire.enabled: true`<br>‚Ä¢ `authentication.mutual.spire.install.enabled: true`<br>‚Ä¢ `encryption.type: wireguard` (node‚Äëto‚Äënode)<br>‚Ä¢ Hubble Relay/UI enabled with ServiceMonitor | Cilium core components installed with ClusterMesh and SPIRE capabilities |
| **üåê Phase 2**<br>*(DNS Configuration)* | **DNS records created for:**<br>‚Ä¢ Each cluster's clustermesh‚Äëapiserver LB IP<br>‚Ä¢ Values stored in `cluster-settings` | ClusterMesh endpoints resolvable via DNS |
| **üîê Phase 3**<br>*(Secret Distribution)* | **ExternalSecret syncs:**<br>‚Ä¢ `kube-system/cilium-clustermesh` from secret store<br>‚Ä¢ Path specified in `cluster-settings` | ClusterMesh connection secrets available in each cluster |
| **üîó Phase 4**<br>*(Mesh Establishment)* | **Cilium agents:**<br>‚Ä¢ Observe the secret<br>‚Ä¢ Connect to remote cluster's API server<br>‚Ä¢ Establish the mesh | Cross-cluster connectivity established and operational |

### 20.7 üîß Operations

| Operation Area | Commands/Checks | Purpose |
| :--- | :--- | :--- |
| **üåê Connectivity** | `cilium status` ‚Ä¢ `cilium clustermesh status` ‚Ä¢ `hubble status` ‚Ä¢ `hubble observe --follow` | Verify peering, cluster visibility, and authenticated flows |
| **üõ°Ô∏è Identity & Auth** | `kubectl -n spire get spireentries` ‚Ä¢ SPIRE CLI ‚Ä¢ Review `CiliumAuthPolicy` | Inspect SPIRE entries, validate policies, audit in non‚Äëprod |
| **‚¨ÜÔ∏è Upgrades** | Sequential cluster upgrade ‚Ä¢ Verify WG tunnel ‚Ä¢ Check SPIRE health ‚Ä¢ Monitor Hubble metrics | Safe upgrade process with health verification |

### 20.8 üö® Failure Modes & Troubleshooting

| Failure Mode | Troubleshooting Steps |
| :--- | :--- |
| **üîó Peering Down** | Check LB/DNS for clustermesh‚Äëapiserver, verify Secret freshness, review agent logs for TLS errors |
| **üõ°Ô∏è Auth Failures** | Verify SPIRE SVID issuance (selectors), check clock skew, validate `CiliumAuthPolicy` matches traffic |
| **üåê Routing Issues** | Ensure BGP peering is Established, verify Pod/LB CIDR route advertisement consistency |

### 20.9 üõ°Ô∏è Security Posture

| Security Principle | Implementation |
| :--- | :--- |
| **üîê Zero‚ÄëTrust Design** | Every connection authenticated (SPIRE) + encrypted (WireGuard) + authorized (Cilium policies) |
| **‚è∞ Credential Management** | Short‚Äëlived SVIDs reduce blast radius; automated rotation |
| **üîí Secret Security** | ClusterMesh secrets never stored in Git; pulled at reconcile time from secret store |

## 22. Security & Network Policy Baseline

This section defines the cluster‚Äëwide network security posture and the policy building blocks teams use. We default‚Äëdeny all traffic and then explicitly allow the minimum required flows. Policies use a mix of Kubernetes `NetworkPolicy` and Cilium `CiliumNetworkPolicy`/`CiliumAuthPolicy` to enable SPIFFE/mTLS authorization.

### 22.1 üéØ Objectives

| Objective | Implementation Strategy |
| :--- | :--- |
| **üö´ Default Deny** | Deny all traffic by default; allow only minimum necessary ingress/egress |
| **üõ°Ô∏è Identity‚ÄëAware Auth** | SPIRE (SPIFFE) + Cilium Auth for mTLS between workloads |
| **üß© Composable Policies** | Small, reusable allow patterns (DNS, API server, observability, gateway ingress, FQDN egress) |

### 22.2 üìã Namespace Baseline

Apply these policies in every application namespace:

| Policy | Purpose | Implementation |
| :--- | :--- | :--- |
| **üö´ Default Deny** | Block all traffic by default | Kubernetes `NetworkPolicy` with `podSelector: {}` |
| **üåê DNS Access** | Allow DNS resolution | Cilium `CiliumNetworkPolicy` targeting kube-dns/CoreDNS |
| **üîë API Server Access** | Allow k8s API communication | Cilium policy with `toEntities: [kube-apiserver]` |

**üö´ Default Deny Policy:**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes: [Ingress, Egress]
```

**üåê DNS Allow Policy:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-dns
spec:
  endpointSelector: {}
  egress:
    - toEndpoints:
        - matchLabels:
            k8s:io.kubernetes.pod.namespace: kube-system
            k8s-app: coredns
      toPorts:
        - ports:
            - port: "53"
              protocol: UDP
            - port: "53"
              protocol: TCP
```

**üîë API Server Policy:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-kube-apiserver
spec:
  endpointSelector: {}
  egress:
    - toEntities: [kube-apiserver]
      toPorts:
        - ports: [{ port: "443", protocol: TCP }]
```

### 22.3 üåç FQDN Egress Allowlists

| Use Case | üîß Implementation | üìã Allowed Destinations |
| :--- | :--- | :--- |
| **üì¶ Package Repositories** | Workload needs outbound internet access | Strict allowlist using `toFQDNs` |
| **üê≥ OCI Registries** | Container image pulls | Pre-approved registry domains only |
| **üîí Security Principle** | Default deny, explicit allow | Minimum required egress only |

**üìã Example Policy - FQDN Egress Allowlist:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: egress-allowlist-tofqdns
spec:
  endpointSelector: {}
  egress:
    - toFQDNs:
        - matchName: registry-1.docker.io
        - matchName: ghcr.io
        - matchName: quay.io
      toPorts:
        - ports:
            - { port: "80", protocol: TCP }
            - { port: "443", protocol: TCP }
```

### 22.4 üö™ Ingress via Gateway Only

| Security Principle | üîß Implementation | üìã Traffic Control |
| :--- | :--- | :--- |
| **üõ°Ô∏è Gateway-Only Access** | Expose applications through Cilium Gateway only | Single ingress point for all external traffic |
| **üîí Source Verification** | Only accept traffic from Cilium Gateway dataplane pods | Prevents direct access to application pods |
| **üåê Port Control** | Restrict to standard HTTP/HTTPS ports | Consistent port policies across services |

**üìã Example Policy - Gateway-Only Ingress:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-from-gateway
spec:
  endpointSelector: {}
  ingress:
    - fromEndpoints:
        - matchLabels:
            k8s:io.kubernetes.pod.namespace: kube-system
            io.cilium.gateway: "true"
      toPorts:
        - ports: [{ port: "80", protocol: TCP }, { port: "443", protocol: TCP }]
```

### 22.5 üìä Observability Paths

| Observability Type | üîß Allowed Traffic | üìã Implementation |
| :--- | :--- | :--- |
| **üìà Metrics Collection** | Scrape to metrics endpoints from vmagent/victoria | Use namespace/label selectors in NetworkPolicies |
| **üìù Log Shipping** | Allow logs egress to VictoriaLogs insert endpoint | Cross‚Äënamespace: prefer Cilium policies over IP allowlists |
| **üîç Policy Preference** | Avoid brittle IP allowlists | Use identity-based policies for reliability |

### 22.6 üõ°Ô∏è SPIFFE/mTLS Authorization

| Security Requirement | üîß Implementation | üìã Purpose |
| :--- | :--- | :--- |
| **üÜî Identity Verification** | Require SPIRE identities for sensitive paths | Zero-trust authentication based on workload identity |
| **üîê mTLS Enforcement** | Use `CiliumAuthPolicy` for mutual TLS | Encrypted communication with identity verification |
| **üéØ Access Control** | Pair with L4/L7 `CiliumNetworkPolicy` | Granular control over ports and HTTP paths |

**üìã Example Policy - SPIFFE/mTLS Authorization:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumAuthPolicy
metadata:
  name: require-spiffe-to-api
  namespace: my-namespace
spec:
  selectors:
    - namespace: my-namespace
      identities:
        - spiffe://monosense.io/ns/my-namespace/sa/backend
  requiredAuthentication:
    - type: spiffe
```

**üîó Policy Layering:**
- **üõ°Ô∏è Authentication**: SPIFFE identity verification via `CiliumAuthPolicy`
- **üéØ Authorization**: L4/L7 traffic control via `CiliumNetworkPolicy`

### 22.7 ‚úÖ Application Policy Checklist

| ‚úÖ Requirement | üìã Description |
| :--- | :--- |
| **üö´ Default Deny** | Apply default deny policy (Ingress + Egress) |
| **üåê DNS Access** | Allow DNS resolution |
| **üîë API Access** | Allow egress to kube‚Äëapiserver if needed (client‚Äëgo, discovery) |
| **üåç External Egress** | Restrict external egress with `toFQDNs` |
| **üö™ Ingress Control** | Only allow ingress from Gateway (and same‚Äënamespace where required) |
| **üõ°Ô∏è mTLS Between Services** | Add SPIFFE `CiliumAuthPolicy` for mTLS between internal services |
| **üìä Observability** | Add observability egress/ingress as needed |

### 22.8 ‚ö†Ô∏è Exceptions

| Exception Type | Management |
| :--- | :--- |
| **üìù Temporary Allowances** | Document with owner and expiry date |
| **üìÅ Policy Tracking** | Separate policies per namespace |
| **üìã Central Logging** | Track in central `EXCEPTIONS.md` file |

### 22.9 üîç Validation & Monitoring

| Validation Method | Purpose |
| :--- | :--- |
| **üî≠ Flow Observation** | Use `hubble observe` to confirm policy hits and TLS status |
| **üö® Alerting** | Add Prometheus alerts for denied flows above threshold |
| **üõ°Ô∏è Identity Monitoring** | Monitor SPIRE SVID issuance failures |

### 22.10 ‚¨ÜÔ∏è Upgrades & Migration

| Migration Strategy | Implementation |
| :--- | :--- |
| **üîç Audit Mode** | Introduce policies in audit mode first, validate flows |
| **‚úÖ Enforcement** | Enforce policies after validation |
| **üìù Version Control** | Version policy docs with app releases to avoid drift |

## 23. Multi‚ÄëCluster Mesh Options ‚Äî Decision Matrix (as of 2025‚Äë10‚Äë20)

This section compares Cilium ClusterMesh with alternative meshes frequently used for cross‚Äëcluster traffic (Istio, Linkerd, Kuma, Consul). It focuses on what matters operationally for Talos bare‚Äëmetal clusters.

### 23.1 Quick Comparison

| Dimension | üåê Cilium ClusterMesh | üèóÔ∏è Istio (sidecar) | üöÄ Istio Ambient (multicluster) | ‚ö° Linkerd | üåç Kuma | üîó Consul |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **üåê Network prereqs** | L3 pod/node reachability, non‚Äëoverlapping PodCIDRs | Gateways between clusters (no pod L3 required) | Gateways (alpha status for multicluster) | Gateways + Service mirror | Multi‚Äëzone; zone/global CP; gateways | Mesh gateways; WAN/peering or cluster peering |
| **üõ°Ô∏è Identity/mTLS across clusters** | In‚Äëcluster SPIRE mTLS GA; cross‚Äëcluster: app‚Äëlevel TLS recommended | Mature SPIFFE/mTLS across clusters | Sidecar‚Äëless model; multicluster alpha (early) | Mature automatic mTLS across clusters | Built‚Äëin mTLS via Envoy | Built‚Äëin mTLS via Envoy |
| **‚öôÔ∏è L7 features** | L3/L4 eBPF, L7 via Cilium Service Mesh (HTTP), policy; cross‚Äëcluster L7 limited | Deep L7 (traffic policy, resiliency, locality) | Emerging L7 (waypoints/ztunnel) | Simpler L7; SMI integrations | Envoy feature set; policy via CRDs | Envoy feature set; policy via Consul |
| **üîç Service discovery** | Global Service annotations; MCS support (beta) | ServiceExport/Import or east‚Äëwest gateways; strong locality/failover | Same goals as sidecar path (alpha) | Service mirror + export | Zone/global discovery | Catalog + peering |
| **üìà Scale & topology** | 255 clusters (default), 511 with tuning; KVStoreMesh for scale | Proven large‚Äëscale; multiple topologies (multi‚Äëprimary, primary‚Äëremote) | Early for multicluster | Proven multi‚Äëcluster via gateways | Multi‚Äëzone hybrid K8s/VM | WAN/peering, hybrid |
| **üéõÔ∏è Control‚Äëplane complexity** | Low (Cilium only) once underlay solved | Medium/High (Istio CP per cluster + gateways) | Medium (new components) | Low/Medium | Medium (global + zone CPs) | Medium (Consul servers + gateways) |
| **üéØ Best for** | Highest performance with eBPF; same policy model across clusters; simple east‚Äëwest LB | Rich L7 + cross‚Äëcluster mTLS now | Sidecar‚Äëless future; not production for multicluster yet | Lightweight secure multicluster | Hybrid environments, policy‚Äërich | Hybrid and multi‚Äëplatform estates |

### 23.2 üéØ When to Choose Which

| Solution | ‚úÖ Choose When... | ‚ùå Avoid When... |
| :--- | :--- | :--- |
| **üåê Cilium ClusterMesh** | L3 connectivity available; primary L3/L4 policy needs; simple service failover; want highest eBPF performance | Need mature cross‚Äëcluster mTLS today; require advanced L7 traffic policies |
| **üèóÔ∏è Istio (sidecar)** | Need mature cross‚Äëcluster mTLS + advanced L7 policies (retries, locality-aware failover, progressive delivery) | Want simple architecture; prefer sidecar‚Äëless approach |
| **‚ö° Linkerd** | Want lightweight, opinionated mesh; cross‚Äëcluster mTLS via gateways and service mirroring | Need advanced L7 features; require complex routing |
| **üåç Kuma/Consul** | Expect hybrid environments (K8s + VMs); need multi‚Äëzone/global control‚Äëplane | Pure K8s environment; want simpler setup |

### 23.3 üõ£Ô∏è Recommended Path for Our Talos Greenfield

| Approach | üéØ Strategy | üîß Key Configuration |
| :--- | :--- | :--- |
| **üåê Primary** | Cilium ClusterMesh (now) + app‚Äëlevel TLS cross‚Äëcluster | Unique PodCIDRs; `clustermesh.useAPIServer: true`; WireGuard + SPIRE; Global Services; namespace policies |
| **üîÑ Fallback** | Istio (sidecar) for selected namespaces only | Scope to specific apps; multi‚Äëprimary with east‚Äëwest gateways; policy layering with Cilium baseline |

**üåê Primary Path Rationale:**
- **‚úÖ Cilium Standardization**: Already standardized on Cilium, need high performance and unified eBPF policy
- **üéØ Use Case**: East‚Äëwest service reachability and failover
- **üåê Network**: Non‚Äëoverlapping PodCIDRs and L3 routing (BGP or static) between clusters

**üîÑ Fallback Strategy:**
- **üìé Scoped Deployment**: Only for apps requiring immediate mesh‚Äëlevel cross‚Äëcluster mTLS
- **üèóÔ∏è Architecture**: Cilium as CNI baseline, Istio for L7/mTLS and traffic shaping

**üö™ Exit Criteria to Revisit:**
- **üõ°Ô∏è Cilium SPIRE Integration**: End‚Äëto‚Äëend ClusterMesh support with single/federated trust domain
- **üìã Production Guidance**: Documented production-ready implementation
- **üîÑ Migration Plan**: Pilot to remove app‚Äëlevel TLS and consolidate on Cilium mTLS |

### 23.4 üìã Migration & Validation Plan

| Phase | üîÑ Step | ‚úÖ Validation |
| :--- | :--- | :--- |
| **üåê Phase 1** | **Underlay Setup**<br>‚Ä¢ Allocate PodCIDRs<br>‚Ä¢ Enable BGP/static routes<br>‚Ä¢ Verify pod‚Äëto‚Äëpod ping | Cross‚Äëcluster L3 connectivity |
| **üöÄ Phase 2** | **Cilium Bootstrap**<br>‚Ä¢ Bootstrap Cilium per cluster<br>‚Ä¢ Expose ClusterMesh API server via LoadBalancer<br>‚Ä¢ Create DNS records | ClusterMesh API accessibility |
| **üîê Phase 3** | **Secret Distribution**<br>‚Ä¢ ExternalSecrets sync `cilium-clustermesh` secret<br>‚Ä¢ Validate secret store connectivity | ClusterMesh secret availability |
| **üåç Phase 4** | **Service Enablement**<br>‚Ä¢ Enable Global Services (or MCS) for 1‚Äì2 anchor services<br>‚Ä¢ Validate failover/locality behavior | Cross‚Äëcluster service discovery |
| **üõ°Ô∏è Phase 5** | **Security Hardening**<br>‚Ä¢ Enforce namespace default‚Äëdeny<br>‚Ä¢ Allow DNS + kube‚Äëapiserver<br>‚Ä¢ Add FQDN egress allowlists<br>‚Ä¢ Require app‚Äëlevel TLS for cross‚Äëcluster flows | Policy enforcement and TLS validation |
| **üìä Phase 6** | **Observability**<br>‚Ä¢ Validate Hubble flows across clusters<br>‚Ä¢ Add alerts for denied cross‚Äëcluster flows<br>‚Ä¢ Monitor mesh peering health | Full visibility and monitoring |
| **üîÑ Phase 7** | **Istio Fallback (if needed)**<br>‚Ä¢ Deploy sidecar mesh to specific namespaces<br>‚Ä¢ Export services<br>‚Ä¢ Validate mTLS and traffic policies end‚Äëto‚Äëend | Istio integration validation |

Appendix A: Snippets

Kustomization with health checks
```yaml
spec:
  wait: true
  timeout: 10m
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: external-secrets
      namespace: external-secrets
```

## Appendix B: Consolidated Guides

### B.1 üöÄ Bootstrap Details (Phased Helmfile)

| Phase | üì¶ Components | üîß Commands |
| :--- | :--- | :--- |
| **Phase 0**<br>*(CRDs only)* | cert-manager CRDs<br>external-secrets CRDs<br>victoria-metrics-operator CRDs bundle<br>prometheus-operator CRDs | `helmfile -f bootstrap/helmfile.d/00-crds.yaml -e <cluster> template \| yq ea 'select(.kind == "CustomResourceDefinition")' \| kubectl apply -f -` |
| **Phase 1**<br>*(Core)* | Cilium<br>CoreDNS<br>Spegel (optional)<br>cert-manager (crds disabled)<br>external-secrets (crds disabled)<br>Flux Operator + Flux Instance | `helmfile -f bootstrap/helmfile.d/01-core.yaml.gotmpl -e <cluster> sync` |

### B.2 üîß Cilium Bootstrap Fixes (Summary)

| Issue | üîß Solution | ‚úÖ Verification |
| :--- | :--- | :--- |
| **üèóÔ∏è Controller Conflicts** | Author all manifests before bootstrap; let Flux adopt after bootstrap | Avoids controller conflict and ordering issues |
| **üåê GitOps Management** | BGP, gateway, clustermesh, IPAM via Flux | Clear separation of concerns |
| **‚úÖ Health Checks** | `kubectl -n kube-system get ds cilium` | Verify Cilium DaemonSet status |
| **‚úÖ Operator Status** | `kubectl -n kube-system get deploy cilium-operator` | Verify Cilium Operator deployment |

### B.3 üêò CloudNativePG (CNPG) Quick Deployment Guide

| Configuration | üîß Details | ‚úÖ Verification |
| :--- | :--- | :--- |
| **üìç Operator Location** | Present in infra cluster | Storage classes defined by cluster-settings |
| **üèóÔ∏è Typical Cluster** | 3 instances; WAL 20Gi; data 80Gi | StorageClass `openebs-local-nvme` |
| **üîç Health Checks** | `kubectl -n cnpg-system get deploy` | Verify CNPG operator deployment |
| **üìä Cluster Status** | `kubectl -n cnpg-system get cluster` | Verify PostgreSQL cluster status |

### B.4 üìä Observability Strategy & Implementation

| Cluster | üîß Stack Components | üì° Endpoints |
| :--- | :--- | :--- |
| **üè≠ Infra** | VictoriaMetrics Global (vmcluster) + VictoriaLogs + vmauth + Alertmanager + Grafana | Central storage and visualization |
| **üéØ Apps** | vmagent + kube-state-metrics + node-exporter + Fluent Bit | Lightweight collection and forwarding |
| **üìù Remote Write** | `victoria-metrics-global-vminsert.observability.svc.cluster.local:8480` | Metrics forwarding endpoint |
| **üîç Query** | `victoria-metrics-global-vmselect.observability.svc.cluster.local:8481` | Query and visualization endpoint |

#### B.4.1 Observability Model (Appendix)

- Metrics (Prometheus-compatible)
  - Infra runs the global VictoriaMetrics vmcluster (vmselect/vminsert/vmstorage) with vmauth and vmalert.
  - Apps runs vmagent that scrapes local targets (kube-state-metrics, node-exporter, Cilium) and remote-writes to infra.
  - Cluster settings provide endpoints and storage classes: `${GLOBAL_VM_INSERT_ENDPOINT}`, `${GLOBAL_VM_SELECT_ENDPOINT}`, `${GLOBAL_ALERTMANAGER_ENDPOINT}`, `${BLOCK_SC}`.
- Logs
  - Infra runs VictoriaLogs (vmstorage/vmselect/vminsert) with vmauth; ServiceMonitor enabled.
  - Fluent Bit DaemonSet on each cluster ships container/system logs over HTTP JSON to vmauth `/insert` with header `X-Scope-OrgID=${OBSERVABILITY_LOG_TENANT}`.
  - Cluster settings provide endpoint host/port/path/TLS toggle and tenant: `${OBSERVABILITY_LOG_ENDPOINT_HOST}`, `${OBSERVABILITY_LOG_ENDPOINT_PORT}`, `${OBSERVABILITY_LOG_ENDPOINT_PATH}`, `${OBSERVABILITY_LOG_ENDPOINT_TLS}`, `${OBSERVABILITY_LOG_TENANT}`.
- Access & RBAC
  - Grafana admin credentials from `${OBSERVABILITY_GRAFANA_SECRET_PATH}`; dashboards loaded via sidecar.
  - NetworkPolicies in `kubernetes/components/networkpolicy/monitoring` allow vm* internal flows and scrapes.
  - PDBs in `kubernetes/components/pdb/victoria-metrics-pdb` protect vmselect/vminsert/vmstorage.

### B.5 üöÄ Workload Notes

| Workload | üîß Components | üõ°Ô∏è Security & Management |
| :--- | :--- | :--- |
| **ü¶ä GitLab (apps)** | CNPG pooler, Dragonfly, External Secrets | Reconciled by Flux |
| **üîë Keycloak (identity)** | Operator‚Äëmanaged SSO; external CNPG | Cilium policies; observability egress only; SPIFFE/mTLS in‚Äëcluster |

### B.9 üêâ DragonflyDB (Operator & Cluster) ‚Äî Quick Guide

| Topic | üîß Details | ‚úÖ Verification |
| :--- | :--- | :--- |
| **üìç Operator Delivery** | Flux HelmRelease + OCIRepository `ghcr.io/dragonflydb/dragonfly-operator/helm`; `install/upgrade.crds: CreateReplace`; replicas `2` + PDB | `kubectl -n dragonfly-operator-system get deploy,crd` |
| **üóÑÔ∏è Cluster CR** | `kubernetes/workloads/platform/databases/dragonfly/dragonfly.yaml`; 3 replicas; PVC on `${DRAGONFLY_STORAGE_CLASS}` with `${DRAGONFLY_DATA_SIZE}`; `--dir=/data`; auth from ExternalSecret | `kubectl -n dragonfly-system get dragonflies.dragonflydb.io,pods,pvc,svc` |
| **üåê Cross‚ÄëCluster Access** | `Service` annotated `service.cilium.io/global: "true"` (and `shared: "true"`) for DNS/routing via ClusterMesh | From apps: resolve `dragonfly.dragonfly-system.svc.cluster.local` and TCP connect 6379 |
| **üõ°Ô∏è Network Policy** | Deny‚Äëby‚Äëdefault; allow from `gitlab-system`, `harbor`, selected app namespaces; allow `observability` to scrape metrics | Flows visible in Hubble; app smoke tests pass |
| **üìä Observability** | ServiceMonitor enabled; PrometheusRule includes availability, memory/disk thresholds, replication lag, command rate | `dragonfly_*` metrics present; sample alert firing |
| **üîÅ Replication & Persistence** | Prefer master‚Äëonly snapshots (if supported by chosen tag) to reduce replica IO; validate behavior on primary restart | Snapshot logs on primary only; replicas stay responsive |
| **üè∑Ô∏è Tenancy** | Start with shared CR; consider per‚Äëtenant CRs (e.g., `dragonfly-gitlab`) for isolation and tailored resources | Draft CR examples; do not flip consumers in this story |

Implementation reference: STORY-DB-DRAGONFLY-OPERATOR-CLUSTER.md


### B.8 üîë Keycloak (Operator) ‚Äî Quick Guide

| Configuration | üîß Details | ‚úÖ Verification |
| :--- | :--- | :--- |
| **üìç Operator** | Install via OLM `Subscription` (prefer) or apply official bundle manifests | `kubectl -n keycloak-system get deploy,crd` |
| **üóÑÔ∏è Database** | External CNPG via `keycloak-pooler` (session mode), secret `keycloak-db-credentials` | Keycloak Ready; tables created |
| **üåê Exposure** | Use Ingress (CR‚Äëmanaged) or Gateway API HTTPRoute; TLS via cert‚Äëmanager secret `sso-tls` | `/.well-known/openid-configuration` reachable over HTTPS |
| **üìà Monitoring** | Service/ServiceMonitor for Prometheus; add Grafana dashboard | `up{job="keycloak"}` present; HTTP 2xx rates |
| **‚¨ÜÔ∏è Upgrades** | OLM channel with Manual approval in prod | CSV transitions Succeeded |

References: Keycloak Operator install/upgrade and basic deployment docs. ÓàÄciteÓàÇturn0search5ÓàÇturn0search6ÓàÅ

### B.6 üìã Historical RCA (Highlights)

| Issue | üîß Resolution | ‚úÖ Outcome |
| :--- | :--- | :--- |
| **üåê Cilium Ownership Conflicts** | Bootstrap split implementation | Clear ownership separation |
| **üìã CRD Timing Issues** | Phase 0 CRD installation | Proper dependency ordering |

### B.7 üèóÔ∏è Platform Infrastructure Implementation (Consolidated)

| Component | üìç Location | üìã Status |
| :--- | :--- | :--- |
| **‚öôÔ∏è Talos Configs** | Consolidated in relevant architecture sections | Integrated |
| **üåê Cilium IPAM/BGP** | Networking section | Actionable |
| **üîê RBAC** | Security section | Defined |
| **üîÑ CI/CD Templates** | Observability & Bootstrap sections | Streamlined |
| **üõ°Ô∏è Network/Security Policies** | Consolidated into architecture | Centralized |

**üìù Implementation Notes:**
- Detailed examples have been consolidated into relevant architecture sections
- Remaining deep dive templates intentionally omitted to keep document actionable
- **Live manifests under `kubernetes/` are the source of truth** |

HelmRelease (OCI-based chart)
```yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
spec:
  chart:
    spec:
      chart: harbor
      version: 1.16.0
      sourceRef:
        kind: HelmRepository
        name: harbor
        namespace: flux-system
  values: { ... }
```

---

## 24. üåê DNS, ExternalDNS, and Cloudflare Tunnel

Public and private DNS automation with two ExternalDNS controllers (Cloudflare and RFC2136/BIND), integrated with Cloudflare Tunnel for zero‚Äëorigin‚Äëexposure ingress.

### Objectives
- Automate DNS for public apps under `SECRET_DOMAIN` (Cloudflare managed).
- Provide LAN‚Äëonly DNS for internal names (BIND via RFC2136) without leaking private records to public zones.
- Terminate edge via Cloudflare Tunnel (cloudflared) to avoid exposing load balancers to the Internet.

### Target Pattern (Summary)
- Run two ExternalDNS instances with disjoint domain filters and TXT registries:
  - `external-dns-cloudflare` ‚Üí provider `cloudflare`, `--domain-filter=${SECRET_DOMAIN}`, `--registry=txt`, `--txt-owner-id=k8s-public`, `--txt-prefix=k8s.`; sources: `gateway-httproute`.
  - `external-dns-rfc2136` ‚Üí provider `rfc2136` to a local BIND server (zone like `home.arpa` or `monosense.lan`), `--domain-filter=<private-zone>`, `--registry=txt`, `--txt-owner-id=k8s-private`.
- Deploy `cloudflared` in the cluster (2 replicas) with a named tunnel. Use QUIC transport, readiness on `/ready`, and a ServiceMonitor for metrics.
- Create a single public CNAME anchor (for example `external.${SECRET_DOMAIN}`) that points to your tunnel domain `<TUNNEL-UUID>.cfargotunnel.com` and is proxied.
- Publish individual app hostnames as CNAMEs to that anchor (e.g., `app.${SECRET_DOMAIN} ‚Üí external.${SECRET_DOMAIN}`) using ExternalDNS from Gateway API routes. This decouples per‚Äëapp DNS from tunnel lifecycle.

### Cloudflare (Public DNS)
- Use API Token auth with least privilege: `Zone:Read` and `Zone:DNS:Edit` for the specific zone; store via External Secrets at `${CERTMANAGER_CLOUDFLARE_SECRET_PATH}` or an `external-dns-cloudflare` secret.
- Set `--cloudflare-proxied` to route through Cloudflare (orange‚Äëcloud) and enable WAF/CDN on public records.
- Use TXT registry with a unique owner ID to avoid contention with any other writers. Prefer `--txt-prefix=k8s.` so TXT records are namespaced and easy to identify.
- Use Gateway API source (`--source=gateway-httproute`) and annotate the parent Gateway (or HTTPRoute) with:
  - `external-dns.alpha.kubernetes.io/hostname: app.${SECRET_DOMAIN}` for the desired FQDN.
  - `external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN}` so the record becomes a CNAME to the tunnel anchor rather than a raw A/AAAA.

Example (Gateway RBAC sketch for ExternalDNS):
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: external-dns-cloudflare
rules:
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["gateway.networking.k8s.io"]
    resources: ["gateways", "httproutes"]
    verbs: ["get", "list", "watch"]
```

### Cloudflared (Cloudflare Tunnel)
- Use a named tunnel with token or credentials JSON stored in External Secrets; do not bake secrets into manifests.
- Recommended runtime settings: `TUNNEL_TRANSPORT_PROTOCOL=quic`, enable post‚Äëquantum where available, disable auto‚Äëupdate in production (managed by GitOps), enable metrics and readiness on a dedicated port, and run ‚â•2 replicas.
- Keep tunnel config minimal: map `*.${SECRET_DOMAIN}` to the cluster‚Äôs external Gateway service (for example `https://envoy-external.networking.svc.cluster.local`) and set `originServerName: external.${SECRET_DOMAIN}` for TLS consistency.
- Create the one‚Äëtime anchor record `external.${SECRET_DOMAIN}` ‚Üí `<TUNNEL-UUID>.cfargotunnel.com` with Cloudflare‚Äôs DNS (can be created via `cloudflared tunnel route dns` or the API). ExternalDNS will then publish per‚Äëapp CNAMEs pointing at this anchor.

Sketch (cloudflared config):
```yaml
ingress:
  - hostname: "*.${SECRET_DOMAIN}"
    service: https://envoy-external.networking.svc.cluster.local
    originRequest:
      http2Origin: true
      originServerName: external.${SECRET_DOMAIN}
  - service: http_status:404
```

### BIND (Private DNS via RFC2136)
- Run a BIND server for a private zone (e.g., `monosense.lan` or `home.arpa`).
- Enable TSIG‚Äëauthenticated dynamic updates for the zone and grant access to ExternalDNS only (e.g., `hmac-sha256` key, `allow-update { key externaldns-key; };`).
- Configure ExternalDNS RFC2136 provider with:
  - `--provider=rfc2136`, `--rfc2136-host=<bind-ip>`, `--rfc2136-port=53`, `--rfc2136-zone=<private-zone>`
  - `--rfc2136-tsig-secret=<base64>`, `--rfc2136-tsig-keyname=externaldns-key`, `--rfc2136-tsig-axfr`, `--rfc2136-tsig-secret-alg=hmac-sha256`
  - `--domain-filter=<private-zone>`, `--registry=txt`, `--txt-owner-id=k8s-private`

### Failure Domains and Safety
- Separate controller instances and TXT owner IDs per zone to prevent record ownership conflicts.
- Per‚Äëcontroller domain filters ensure no cross‚Äëwrites (Cloudflare vs. BIND).
- Proxied CNAME pattern isolates tunnel lifecycle from app DNS and supports apex flattening where needed.

### Observability
- Add Prometheus rules for `external_dns_controller_last_sync_timestamp_seconds` to detect stale syncs.
- Scrape cloudflared metrics and alert on tunnel disconnects or degraded readiness.

### Implementation Notes (Repo Alignment)
- Follow the `app-template` HelmRelease pattern used elsewhere in this repo.
- Place manifests under `kubernetes/apps/networking/` as:
  - `cloudflared/` (Deployment + config + ExternalSecret + ServiceMonitor)
  - `external-dns/cloudflare/` (ExternalDNS for public zone)
  - `external-dns/rfc2136/` (ExternalDNS for private zone)
- Use External Secrets for API tokens and TSIG material.
