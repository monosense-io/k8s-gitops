# ğŸ—ï¸ Multi-Cluster GitOps Architecture (v4)

<div align="center">

![Status](https://img.shields.io/badge/Status-Implementing-blue)
![Owner](https://img.shields.io/badge/Owner-Platform_Engineering-orange)
![Last Updated](https://img.shields.io/badge/Updated-2025--10--20-green)
![Version](https://img.shields.io/badge/Version-4.0-purple)

**Modern cloud-native platform built on Talos Linux â€¢ GitOps-powered â€¢ Multi-cluster**

</div>

---

## ğŸ“‹ Table of Contents

1. [ğŸ“– Context & Goals](#1-context--goals)
2. [ğŸ¯ Design Principles](#2-design-principles)
3. [ğŸŒ Target Topology](#3-target-topology)
4. [ğŸ“ Repository Layout](#4-repository-layout)
5. [âš™ï¸ Flux Model & Convergence](#5-flux-model--convergence)
6. [ğŸš€ Bootstrap Architecture](#6-bootstrap-architecture)
7. [ğŸ”§ Cluster Settings & Substitution](#7-cluster-settings--substitution)
8. [ğŸ” Secrets Management](#8-secrets-management)
9. [ğŸŒ Networking (Cilium)](#9-networking-cilium)
10. [ğŸ’¾ Storage](#10-storage)
11. [ğŸ“Š Observability](#11-observability)
12. [ğŸ”„ CI/CD & Policy](#12-cicd--policy)
13. [ğŸ¢ Multi-Tenancy](#13-multi-tenancy)
14. [ğŸ”§ Operations & Runbooks](#14-operations--runbooks)
15. [ğŸ“… Phased Implementation](#15-phased-implementation)
16. [ğŸ“ Decisions & Rationale](#16-decisions--rationale)
17. [âš ï¸ Risks & Mitigations](#17-risks--mitigations)
18. [âœ… Acceptance Criteria](#18-acceptance-criteria--metrics)
19. [ğŸ› ï¸ Workloads & Versions](#19-workloads--versions)
20. [ğŸ”— Cilium ClusterMesh + SPIRE](#20-cilium-clustermesh--spire)
21. [ğŸ›¡ï¸ Security & Network Policy](#21-security--network-policy-baseline)
22. [ğŸ”„ Multi-Cluster Mesh Options](#22-multi-cluster-mesh-options---decision-matrix)

---

## 1. ğŸ“– Context & Goals

> **ğŸ›ï¸ Overview**
>
> We run two Talos-based Kubernetes clusters â€“ an infrastructure cluster ("infra") and an application cluster ("apps"). We manage the platform via GitOps using Flux, Helm, and Kustomize. This document defines the target multi-cluster architecture, repository structure, bootstrap approach, security posture, and phased rollout plan.

### ğŸ¯ Objectives

| âœ… Primary Goals | âŒ Non-Goals |
| :--- | :--- |
| âš¡ **Repeatable, fast cluster bring-up** with minimal manual steps | ğŸ”§ Managing Talos OS lifecycle (covered by Talos docs/scripts) |
| ğŸ”„ **Clear separation of concerns** between infrastructure and workloads | ğŸ“š Full application SRE runbooks (app teams own those) |
| âš™ï¸ **Deterministic ordering** using Flux `Kustomization.dependsOn`, health checks, and timeouts | |
| ğŸ” **Zero plaintext secrets** via External Secrets (1Password Connect) | |
| ğŸ“ˆ **Scalable to additional environments** (apps-dev, apps-stg, apps-prod) and optional multi-tenancy | |

## 2. ğŸ¯ Design Principles

> **ğŸ›ï¸ Core Philosophy**
>
> Our architecture follows GitOps best practices with security and reliability at the forefront.

| Principle | Description | Impact |
| :--- | :--- | :--- |
| ğŸ”„ **Git as Single Source of Truth** | Controllers converge the desired state from Git repository | Eliminates configuration drift |
| ğŸ—ï¸ **Hierarchical Flux Structure** | One entry Kustomization per cluster, fanning into ordered Kustomizations with `dependsOn` | Ensures deterministic deployment order |
| ğŸ”’ **Hermetic Builds** | Avoid remote bases; prefer local Git/OCI sources. No cross-namespace references for multi-tenant areas | Improves security and reproducibility |
| âœ… **Complete Kustomization Settings** | All `Kustomization` have `prune: true`, `wait: true`, `timeout`, `healthChecks`/`healthCheckExprs` | Ensures reliable deployments |
| ğŸ” **Secure Secrets Management** | External Secrets with 1Password Connect for all secrets (bootstrap and runtime) | Zero plaintext secrets in Git |
| ğŸš€ **Minimal Bootstrap** | Bootstrap installs only what is required to let Flux take over; day-2 config lives in Git managed by Flux | Fast, reliable cluster bring-up |

---

## 3. ğŸŒ Target Topology

> **ğŸ—ï¸ Multi-Cluster Architecture**
>
> Our platform consists of two specialized clusters designed for optimal performance and separation of concerns.

### ğŸ­ Cluster Overview

| Cluster | Purpose | Key Components | Network Features |
| :--- | :--- | :--- | :--- |
| **ğŸ­ infra** | Platform services & storage | ğŸ—„ï¸ Rook-Ceph â€¢ ğŸ˜ CloudNativePG â€¢ ğŸ“Š VictoriaMetrics/Logs â€¢ ğŸ” Security â€¢ ğŸ“¦ Registry | Core Cilium + BGP + Gateway API |
| **ğŸ¯ apps** | Application workloads | âš¡ User applications â€¢ ğŸš€ CI/CD runners â€¢ ğŸ“¬ Messaging systems | Core Cilium + ClusterMesh connectivity |

### ğŸŒ Network Architecture

```mermaid
graph TB
    subgraph "Infrastructure Cluster"
        C1[Cilium CNI]
        BGP1[BGP Peering]
        GW1[Gateway API]
        Storage[Storage Services]
        DB[Database Services]
    end

    subgraph "Application Cluster"
        C2[Cilium CNI]
        CM[ClusterMesh]
        Apps[Application Workloads]
    end

    C1 <--> CM
    C2 <--> CM
    Storage -.-> |Cross-cluster services| Apps
    DB -.-> |Database access| Apps
```

**Key Network Features:**
- ğŸŒ **Cilium as CNI** - Core installed via Helmfile during bootstrap
- ğŸ”„ **Dayâ€‘2 Features** - BGP, Gateway API, ClusterMesh secrets, IPAM pools managed by Flux
- ğŸ”— **ClusterMesh** - Secure cross-cluster connectivity for service discovery

### ğŸ’¾ Storage Architecture

| Cluster | Storage Solution | Use Case | Performance |
| :--- | :--- | :--- | :--- |
| **ğŸ­ infra** | ğŸ—„ï¸ Rook-Ceph + OpenEBS LocalPV | Block/file storage, databases | High throughput NVMe |
| **ğŸ¯ apps** | ğŸ—„ï¸ Dedicated Rook-Ceph + OpenEBS LocalPV (default) | Application storage, local workloads | Multiâ€‘GB/s NVMe local |

**Why Dedicated Storage for Apps Cluster?**
- ğŸš« **Avoids 1 Gbps router bottleneck** - Keeps Ceph traffic local to apps cluster
- âš¡ **Better performance** - Local replication/backfill within cluster
- ğŸ”§ **Operational simplicity** - Aligned Ceph versions across clusters

### ğŸ“Š Observability Strategy

| Cluster | Stack Components | Data Flow |
| :--- | :--- | :--- |
| **ğŸ­ infra** | ğŸ“ˆ VictoriaMetrics global â€¢ ğŸ“ VictoriaLogs â€¢ ğŸ“Š Exporters â€¢ ğŸš¨ Flux Alerts/Receivers | Full observability stack |
| **ğŸ¯ apps** | ğŸ“¡ vmagent â€¢ ğŸ“Š kube-state-metrics â€¢ ğŸ“‹ node-exporter â€¢ ğŸ“¤ Fluent Bit | **Remote write/export to infra only** |

**Benefits:**
- ğŸ’° **Cost efficient** - Single observability stack
- ğŸš€ **High performance** - Local aggregation, remote forwarding
- ğŸ”§ **Simplified operations** - Centralized monitoring and logging

## 4. ğŸ“ Repository Layout (Endâ€‘State)

> **ğŸ›ï¸ Flux-Optimized Structure**
>
> We keep the established `kubernetes/` layout and align it with Flux best practices for optimal GitOps workflows.

### ğŸ“‚ Directory Structure

```
ğŸ“¦ k8s-gitops/
â”£â” ğŸ“ kubernetes/
â”ƒ  â”£â” ğŸ“ clusters/
â”ƒ  â”ƒ  â”£â” ğŸ­ infra/
â”ƒ  â”ƒ  â”ƒ  â”£â” ğŸ”„ flux-system/               # GitRepository + cluster Kustomizations (entrypoint)
â”ƒ  â”ƒ  â”ƒ  â”£â” âš™ï¸ cluster-settings.yaml      # ConfigMap; used by postBuild.substituteFrom
â”ƒ  â”ƒ  â”ƒ  â”£â” ğŸ—ï¸ infrastructure.yaml        # Ordered platform Kustomizations for infra cluster
â”ƒ  â”ƒ  â”ƒ  â”—â” ğŸš€ workloads.yaml             # Platform workloads (observability, registry, etc.)
â”ƒ  â”ƒ  â”£â” ğŸ¯ apps/
â”ƒ  â”ƒ  â”ƒ  â”£â” ğŸ”„ flux-system/
â”ƒ  â”ƒ  â”ƒ  â”£â” âš™ï¸ cluster-settings.yaml
â”ƒ  â”ƒ  â”ƒ  â”£â” ğŸ—ï¸ infrastructure.yaml        # Platform needed on apps cluster (e.g., external-secrets, issuers)
â”ƒ  â”ƒ  â”ƒ  â”—â” ğŸš€ workloads.yaml             # Tenants/platform apps on apps cluster
â”ƒ  â”ƒ  â”—â” ğŸŒ apps-dev/, apps-stg/, apps-prod/ (optional overlays)
â”ƒ  â”£â” ğŸ“ infrastructure/
â”ƒ  â”ƒ  â”£â” ğŸŒ networking/                  # dayâ€‘2 Cilium features (bgp, gateway, clustermesh, ipam)
â”ƒ  â”ƒ  â”£â” ğŸ” security/                    # external-secrets, cert-manager issuers, RBAC bundles
â”ƒ  â”ƒ  â”£â” ğŸ’¾ storage/                     # rook-ceph, openebs (infra cluster only)
â”ƒ  â”ƒ  â”—â” ğŸ”„ gitops/                      # legacy flux-operator/instance charts (reference only)
â”ƒ  â”£â” ğŸ“ workloads/
â”ƒ  â”ƒ  â”£â” ğŸ—ï¸ platform/                    # platform apps (observability, registry, CICD, databases)
â”ƒ  â”ƒ  â”—â” ğŸ‘¥ tenants/                     # optional multiâ€‘tenant applications
â”£â” ğŸ“ bootstrap/
â”ƒ  â”£â” ğŸ“„ helmfile.d/00-crds.yaml       # CRD-only phase
â”ƒ  â”£â” ğŸ“„ helmfile.d/01-apps.yaml       # ordered bootstrap charts (ciliumâ†’corednsâ†’spegelâ†’cert-managerâ†’flux-operatorâ†’flux-instance)
â”ƒ  â”£â” ğŸ“„ helmfile.d/templates/values.yaml.gotmpl  # reads HelmRelease .spec.values to keep one source of truth
â”ƒ  â”—â” ğŸ“„ resources.yaml                # namespaces + initial Secret(s) for secret-store
â”—â” ğŸ“ .taskfiles/
   â”£â” ğŸ“„ bootstrap/Taskfile.yaml       # task bootstrap:talos, bootstrap:apps
   â”—â” ğŸ“„ talos/Taskfile.yaml           # node lifecycle helpers
```

### ğŸ¯ Key Design Decisions

| Decision | Rationale | Impact |
| :--- | :--- | :--- |
| **ğŸš« Removed aggregator `ks.yaml`** | Avoid duplicating cluster wiring | Cleaner, more direct Flux reconciliations |
| **ğŸ”§ Helmfile bootstrap** | Predictable, idempotent installation | Reliable cluster bring-up |
| **ğŸ“ Cluster-specific settings** | `cluster-settings.yaml` per cluster | Environment-specific configuration |
| **ğŸ”„ Git as source of truth** | Flux reconciles directories directly | No configuration drift |

### ğŸ”§ Bootstrap vs Dayâ€‘2 Management

| Phase | Tool | Responsibility |
| :--- | :--- | :--- |
| **ğŸš€ Bootstrap** | Helmfile + Task | Core infrastructure installation (Cilium, Flux) |
| **ğŸ“… Dayâ€‘2** | Flux | All ongoing configuration management via Git |

---

## 5. âš™ï¸ Flux Model & Convergence

> **ğŸ”„ Declarative GitOps Engine**
>
> Flux ensures reliable, ordered deployment of infrastructure and workloads with built-in health checking.

### ğŸ¯ Entry Kustomization (Per Cluster)

Each cluster has a single entry point that orchestrates the entire platform:

```yaml
# kubernetes/clusters/<cluster>/flux-system/kustomization.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cluster-<name>
  namespace: flux-system
spec:
  interval: 10m
  path: ./kubernetes/clusters/<name>
  prune: true
  wait: true
  timeout: 15m
  sourceRef:
    kind: GitRepository
    name: flux-system
```

**Key Features:**
- ğŸ“ **Reconciles cluster directory** - Includes `cluster-settings.yaml`, `infrastructure.yaml`, and `workloads.yaml`
- ğŸ”§ **Variable substitution** - Uses `postBuild.substituteFrom` to inject cluster-specific settings
- âœ… **Health validation** - Built-in health checks ensure reliable deployments

### ğŸ”„ Ordering & Dependencies

```mermaid
graph TD
    A[flux-repositories] --> B[infrastructure]
    B --> C[workloads]

    B --> D[networking]
    B --> E[security]
    B --> F[storage]

    D --> G[workloads]
    E --> G
    F --> G
```

**Dependency Chain:**
1. **ğŸ”„ flux-repositories** - Helm repositories and OCI sources
2. **ğŸ—ï¸ infrastructure** - Core platform components (networking, security, storage)
3. **ğŸš€ workloads** - Applications and services

### âœ… Health Checking Strategy

| Component | Health Check | Timeout | Success Criteria |
|---|---|---|---|
| **ClusterIssuer** | Ready condition | 5m | Certificate authority ready |
| **DaemonSet** | Available pods | 10m | All nodes running pods |
| **Deployment** | Available replicas | 5m | Desired replicas ready |
| **StatefulSet** | Ready replicas | 15m | All replicas ready |
| **PVC** | Bound status | 2m | Volume successfully bound |

**Configuration Example:**
```yaml
spec:
  dependsOn:
    - name: flux-repositories
  interval: 10m
  path: ./kubernetes/infrastructure
  prune: true
  wait: true
  timeout: 10m
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: external-secrets
      namespace: external-secrets
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
```

Example (trimmed)
```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: cluster-infra-infrastructure
  namespace: flux-system
spec:
  dependsOn:
    - name: flux-repositories
  interval: 10m
  path: ./kubernetes/infrastructure
  prune: true
  wait: false
  sourceRef:
    kind: GitRepository
    name: flux-system
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
```

## 6. ğŸš€ Bootstrap Architecture (Helmfile + Task)

> **ğŸ—ï¸ Two-Phase Bootstrap Strategy**
>
> We use Helmfile for predictable, idempotent bootstrap before Flux controllers exist. This ensures consistent cluster bring-up every time.

### ğŸ¯ Why Helmfile?

| Benefit | Description | Impact |
|---|---|---|
| **ğŸ”„ Idempotent** | Same result every run | Reliable bootstrap process |
| **âš¡ Fast** | Direct cluster installation | No waiting for controllers |
| **ğŸ“ Consistent values** | Reuses same values as Flux `HelmRelease` | Zero configuration drift |
| **ğŸ”§ Two-phase approach** | CRDs first, then apps | Proper dependency ordering |

### ğŸ”„ Bootstrap Phases

```mermaid
graph LR
    A[Phase 0: CRDs] --> B[Phase 1: Core Apps]
    B --> C[Phase 2: Flux Handover]
    C --> D[Day-2: GitOps]

    subgraph "Phase 0"
        A1[00-crds.yaml]
        A2[Extract CRDs only]
        A3[Apply CRDs first]
    end

    subgraph "Phase 1"
        B1[01-apps.yaml]
        B2[Ordered releases]
        B3[Cilium â†’ CoreDNS â†’ Spegel â†’ cert-manager â†’ Flux]
    end
```

#### Phase 0: CRD Foundation (`bootstrap/helmfile.d/00-crds.yaml`)

```bash
helmfile -f bootstrap/helmfile.d/00-crds.yaml -e <cluster> template \
  | yq ea 'select(.kind == "CustomResourceDefinition")' \
  | kubectl apply -f -
```

**Key Features:**
- ğŸ”§ **`helmDefaults.args: [--include-crds, --no-hooks]`** - Clean CRD extraction
- ğŸ“‹ **PostRenderer with `yq`** - Filters CRDs only
- âœ… **Prerequisite validation** - Ensures CRDs exist before consumers

#### Phase 1: Core Applications (`bootstrap/helmfile.d/01-core.yaml.gotmpl`)

**ğŸ”— Dependency Chain:**
```
Cilium (CNI) â†’ CoreDNS (DNS) â†’ Spegel (Image Mirror) â†’ cert-manager (TLS) â†’ flux-operator â†’ flux-instance
```

**ğŸ¯ Key Features:**
- ğŸ“‹ **Ordered releases with `needs`** - Proper startup sequence
- ğŸ”„ **Template values from Git** - `values.yaml.gotmpl` reads `HelmRelease` specs
- ğŸ“ **Single source of truth** - Same values for bootstrap and Flux

### ğŸ” Early Secrets & Namespaces

**Bootstrap Resources (`bootstrap/resources.yaml`):**

| Resource | Purpose | Created When |
|---|---|---|
| **external-secrets namespace** | Secret management operator | Bootstrap Phase 1 |
| **1Password Connect Secret** | External secrets access | Before Flux starts |
| **1Password Connect token** | Bootstrap access token for External Secrets | Bootstrap Phase 0 |

### ğŸ› ï¸ Bootstrap Tasks (Taskfile Canonical)

| Task | Command | Function |
|---|---|---|
| **ğŸ”§ Talos Bootstrap (Phase âˆ’1)** | `task bootstrap:talos` | Apply first control plane, `talosctl bootstrap`, remaining CPs, export kubeconfig |
| **ğŸ“¦ Prereqs (Phase 0)** | `task :bootstrap:phase:0 CLUSTER=<cluster>` | Namespaces and initial secrets (e.g., 1Password) |
| **ğŸ”§ CRDs (Phase 1)** | `task :bootstrap:phase:1 CLUSTER=<cluster>` | Install CRDs only (extracted from charts) |
| **ğŸš€ Core (Phase 2)** | `task :bootstrap:phase:2 CLUSTER=<cluster>` | Cilium, CoreDNS, certâ€‘manager (CRDs disabled), External Secrets (CRDs disabled), Flux |
| **âœ… Validate (Phase 3)** | `task :bootstrap:phase:3 CLUSTER=<cluster>` | Readiness, Flux health, status summary |

### âœ… Phase Guards
- Phase 0 must emit only CustomResourceDefinition kinds; audit with kinds filter.
- Phase 1 installs controllers with CRD installation disabled in chart values (CRDs were preâ€‘installed in Phase 0).

### ğŸ§­ Handover Criteria (Authoritative)
- fluxâ€‘operator Ready; fluxâ€‘instance Ready; GitRepository source connected; all initial Kustomizations Ready; `kustomize build` + `kubeconform` clean for the cluster root.

### ğŸ§© Talos Roleâ€‘Aware Convention (optional)
```
talos/
 â””â”€ <cluster>/
     â”œâ”€ controlplane/   # first CP used for etcd bootstrap; then remaining CPs
     â””â”€ worker/         # workers joined after API is responding
```
Behavior:
- Prefer `controlplane/*.yaml` first; then remaining CPs in `<cluster>/*.yaml`.
- After API is Ready, apply `worker/*.yaml` using `:talos:apply-node ... MACHINE_TYPE=worker`.
- Safe detector: if first CP already healthy (`talosctl get machineconfig` and `etcd status` OK), skip `talosctl bootstrap`.

### ğŸ§ª CI Dryâ€‘Run (nonâ€‘blocking to start)
- Run `task bootstrap:dry-run CLUSTER=infra` in validation CI to surface template/values drift. Emit a short summary. Can become gating later.

### â±ï¸ Timeâ€‘toâ€‘Ready Targets (baseline)
- Talos control plane â‰¤ 7 minutes; CRDs â‰¤ 2 minutes; Core â‰¤ 6 minutes; total â‰¤ 20 minutes per cluster.

### ğŸ”„ Handover to GitOps

```mermaid
graph TD
    A[Bootstrap Complete] --> B[flux-operator Ready]
    B --> C[flux-instance Ready]
    C --> D[Flux Takes Over]
    D --> E[Day-2 GitOps Management]

    E --> F[Networking: BGP/Gateway/ClusterMesh]
    E --> G[Security: cert-manager issuers]
    E --> H[Storage: Rook-Ceph/OpenEBS]
    E --> I[Observability: VictoriaMetrics]
    E --> J[Workloads: Applications]
```

**âœ… Handover Criteria:**
- ğŸŸ¢ **flux-operator** controllers running and ready
- ğŸŸ¢ **flux-instance** cluster reconciliation active
- ğŸŸ¢ **GitRepository** source connected and syncing
- ğŸŸ¢ **Initial Kustomizations** reconciling successfully

**ğŸ”„ Day-2 Management:**
All ongoing configuration changes happen through Git commits, with Flux automatically applying them to the cluster.

---

## 7. âš™ï¸ Cluster Settings & Substitution

> **ğŸ”§ Centralized Configuration Management**
>
> Each cluster has a dedicated `cluster-settings.yaml` ConfigMap that contains all environment-specific values used throughout the platform.

### ğŸ“‹ Configuration Examples (real values from this repo)

Infra cluster (kubernetes/clusters/infra/cluster-settings.yaml)
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-settings
  namespace: flux-system
  labels:
    app.kubernetes.io/managed-by: flux
    cluster: infra
data:
  # Cluster Identity
  CLUSTER: infra
  CLUSTER_ID: "1"

  # Network Configuration
  POD_CIDR: '["10.244.0.0/16"]'
  POD_CIDR_STRING: "10.244.0.0/16"
  SERVICE_CIDR: '["10.245.0.0/16"]'
  K8S_SERVICE_HOST: "infra-k8s.monosense.io"

  # Cilium Configuration
  CLUSTERMESH_IP: "10.25.11.100"
  CILIUM_GATEWAY_LB_IP: "10.25.11.120"
  CILIUM_BGP_LOCAL_ASN: "64512"
  CILIUM_BGP_PEER_ASN: "64501"
  CILIUM_BGP_PEER_ADDRESS: "10.25.11.1/32"

  # CoreDNS Configuration
  COREDNS_CLUSTER_IP: "10.245.0.10"
  COREDNS_REPLICAS: "2"

  # External Secrets Configuration
  EXTERNAL_SECRET_STORE: "onepassword"
  ONEPASSWORD_CONNECT_HOST: "http://opconnect.monosense.dev"
  ONEPASSWORD_CONNECT_TOKEN_SECRET: "onepassword-connect-token"
  CILIUM_CLUSTERMESH_SECRET_PATH: "kubernetes/infra/cilium-clustermesh"
  CERTMANAGER_CLOUDFLARE_SECRET_PATH: "kubernetes/infra/cert-manager/cloudflare"

  # Domain Configuration
  SECRET_DOMAIN: "monosense.io"

  # GitHub Actions Configuration
  GITHUB_CONFIG_URL: "https://github.com/monosense-io/k8s-gitops"

  # Rook-Ceph Configuration
  ROOK_CEPH_NAMESPACE: "rook-ceph"
  ROOK_CEPH_CLUSTER_NAME: "rook-ceph"
  ROOK_CEPH_BLOCKPOOL_NAME: "rook-ceph-block"
  ROOK_CEPH_IMAGE_TAG: v19.2.3
  ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"
  ROOK_CEPH_MON_COUNT: "3"
  CEPH_BLOCK_STORAGE_CLASS: "rook-ceph-block"

  # OpenEBS Configuration
  OPENEBS_BASEPATH: "/var/mnt/openebs"
  OPENEBS_STORAGE_CLASS: "openebs-local-nvme"

  # Observability Configuration
  OBSERVABILITY_BLOCK_SC: "rook-ceph-block"
  OBSERVABILITY_METRICS_RETENTION: "30d"
  OBSERVABILITY_LOGS_RETENTION: "14d"
  OBSERVABILITY_LOG_ENDPOINT_HOST: "victorialogs-vmauth.observability.svc.cluster.local"
  OBSERVABILITY_LOG_ENDPOINT_PORT: "9428"
  OBSERVABILITY_LOG_ENDPOINT_PATH: "/insert"
  OBSERVABILITY_LOG_ENDPOINT_TLS: "Off"
  OBSERVABILITY_LOG_TENANT: "infra"
  OBSERVABILITY_GRAFANA_SECRET_PATH: "kubernetes/infra/grafana-admin"

  # Global Monitoring Configuration (for cross-cluster federation)
  GLOBAL_VM_INSERT_ENDPOINT: "victoria-metrics-global-vminsert.observability.svc.cluster.local:8480"
  GLOBAL_VM_SELECT_ENDPOINT: "victoria-metrics-global-vmselect.observability.svc.cluster.local:8481"
  GLOBAL_ALERTMANAGER_ENDPOINT: "victoria-metrics-global-alertmanager.observability.svc.cluster.local:9093"

  # CloudNative-PG Configuration
  CNPG_OPERATOR_VERSION: "0.25.0"
  CNPG_POSTGRES_VERSION: "16.8"
  CNPG_STORAGE_CLASS: "openebs-local-nvme"
  CNPG_DATA_SIZE: "80Gi"
  CNPG_WAL_SIZE: "20Gi"
  CNPG_INSTANCES: "3"
  CNPG_SHARED_CLUSTER_NAME: "shared-postgres"
  CNPG_BACKUP_BUCKET: "monosense-cnpg"
  CNPG_BACKUP_SCHEDULE: "0 2 * * *"
  CNPG_MINIO_ENDPOINT_URL: "http://10.25.11.3:9000"
  CNPG_MINIO_SECRET_PATH: "kubernetes/infra/cloudnative-pg/minio"
  CNPG_SUPERUSER_SECRET_PATH: "kubernetes/infra/cloudnative-pg/superuser"

  # Dragonfly Configuration
  DRAGONFLY_STORAGE_CLASS: "openebs-local-nvme"
  DRAGONFLY_DATA_SIZE: "30Gi"
  DRAGONFLY_AUTH_SECRET_PATH: "kubernetes/infra/dragonfly/auth"
```

Apps cluster (kubernetes/clusters/apps/cluster-settings.yaml)
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-settings
  namespace: flux-system
  labels:
    app.kubernetes.io/managed-by: flux
    cluster: apps
data:
  # Cluster Identity
  CLUSTER: apps
  CLUSTER_ID: "2"

  # Network Configuration
  POD_CIDR: '["10.246.0.0/16"]'
  POD_CIDR_STRING: "10.246.0.0/16"
  SERVICE_CIDR: '["10.247.0.0/16"]'
  K8S_SERVICE_HOST: "apps-k8s.monosense.io"

  # Cilium Configuration
  CLUSTERMESH_IP: "10.25.11.101"
  CILIUM_GATEWAY_LB_IP: "10.25.11.121"
  CILIUM_BGP_LOCAL_ASN: "64513"
  CILIUM_BGP_PEER_ASN: "64501"
  CILIUM_BGP_PEER_ADDRESS: "10.25.11.1/32"

  # CoreDNS Configuration
  COREDNS_CLUSTER_IP: "10.247.0.10"
  COREDNS_REPLICAS: "2"

  # External Secrets Configuration
  EXTERNAL_SECRET_STORE: "onepassword"
  ONEPASSWORD_CONNECT_HOST: "http://opconnect.monosense.dev"
  ONEPASSWORD_CONNECT_TOKEN_SECRET: "onepassword-connect-token"
  CILIUM_CLUSTERMESH_SECRET_PATH: "kubernetes/apps/cilium-clustermesh"
  CERTMANAGER_CLOUDFLARE_SECRET_PATH: "kubernetes/apps/cert-manager/cloudflare"

  # Domain Configuration
  SECRET_DOMAIN: "monosense.io"

  # Rook-Ceph Configuration (apps cluster)
  ROOK_CEPH_NAMESPACE: "rook-ceph"
  ROOK_CEPH_CLUSTER_NAME: "rook-ceph"
  ROOK_CEPH_BLOCKPOOL_NAME: "rook-ceph-block"
  ROOK_CEPH_IMAGE_TAG: v19.2.3
  ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"
  ROOK_CEPH_MON_COUNT: "3"
  CEPH_BLOCK_STORAGE_CLASS: "rook-ceph-block"

  # OpenEBS Configuration
  OPENEBS_BASEPATH: "/var/mnt/openebs"
  OPENEBS_STORAGE_CLASS: "openebs-local-nvme"

  # Observability Configuration
  OBSERVABILITY_BLOCK_SC: "rook-ceph-block"
  OBSERVABILITY_METRICS_RETENTION: "30d"
  OBSERVABILITY_LOGS_RETENTION: "14d"
  OBSERVABILITY_LOG_ENDPOINT_HOST: "victorialogs-vmauth.observability.svc.cluster.local"
  OBSERVABILITY_LOG_ENDPOINT_PORT: "9428"
  OBSERVABILITY_LOG_ENDPOINT_PATH: "/insert"
  OBSERVABILITY_LOG_ENDPOINT_TLS: "Off"
  OBSERVABILITY_LOG_TENANT: "apps"
  OBSERVABILITY_GRAFANA_SECRET_PATH: "kubernetes/apps/grafana-admin"

  # Dragonfly Configuration
  DRAGONFLY_STORAGE_CLASS: "openebs-local-nvme"
  DRAGONFLY_DATA_SIZE: "50Gi"
  DRAGONFLY_AUTH_SECRET_PATH: "kubernetes/apps/dragonfly/auth"

  # GitLab Configuration
  GITLAB_DB_SECRET_PATH: "kubernetes/apps/gitlab/db"
  GITLAB_REDIS_SECRET_PATH: "kubernetes/apps/gitlab/redis"
  GITLAB_S3_SECRET_PATH: "kubernetes/apps/gitlab/s3"
  GITLAB_ROOT_SECRET_PATH: "kubernetes/apps/gitlab/root"
```

### ğŸ”§ Variable Substitution Flow

```mermaid
graph LR
    A[cluster-settings.yaml] --> B[postBuild.substituteFrom]
    B --> C[Template Variables]
    C --> D[HelmRelease Values]
    C --> E[Manifest Substitutions]
    C --> F[ConfigMap Data]

    subgraph "All Kustomizations"
        G[infrastructure.yaml]
        H[workloads.yaml]
        I[security/]
        J[storage/]
        K[networking/]
    end

    D --> G
    E --> H
    F --> I
    F --> J
    F --> K
```

### ğŸ“ Usage Examples

| Component | Variable Used | Example |
|---|---|---|
| **Cilium BGP** | `${CILIUM_BGP_ASN}` | `localAsn: ${CILIUM_BGP_ASN}` |
| **Storage Class** | `${DEFAULT_STORAGE_CLASS}` | `storageClassName: ${DEFAULT_STORAGE_CLASS}` |
| **External Secret** | `${SECRET_STORE_PATH}` | `path: ${SECRET_STORE_PATH}/database` |
| **Victoria Metrics** | `${VICTORIA_METRICS_ENDPOINT}` | `remoteWrite: ${VICTORIA_METRICS_ENDPOINT}` |

### âœ… Benefits

| Benefit | Description |
|---|---|
| **ğŸ¯ Environment Isolation** | Each cluster has independent settings |
| **ğŸ”„ Single Source of Truth** | All variables in one ConfigMap |
| **ğŸš€ Zero Drift** | Template substitutions at build time |
| **ğŸ”§ Easy Maintenance** | Update cluster settings in one place |
| **ğŸ›¡ï¸ Type Safety** | Explicit variable declarations |

### ğŸ—ï¸ Implementation Pattern

```yaml
# Any Kustomization using cluster settings
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: my-app
spec:
  # ... other spec
  postBuild:
    substituteFrom:
      - kind: ConfigMap
        name: cluster-settings
        optional: false
```

## 8. ğŸ” Secrets Management (1Password Only)

> **ğŸ›¡ï¸ Zero-Trust Secrets Architecture**
>
> We implement a defense-in-depth approach to secrets management, ensuring no plaintext secrets ever touch our Git repository.

### ğŸ”§ Approach

All secrets are managed exclusively via External Secrets with 1Password Connect. No SOPS is used in this platform.

### ğŸ—ï¸ Architecture Flow

```mermaid
graph TD
    A[1Password Vault] --> B[1Password Connect]
    B --> C[External Secrets Operator]
    C --> D[Kubernetes Secrets]
    D --> E[Applications]

    J[Bootstrap Secret (1Password token)] --> C
    K[Cluster Settings] --> C
```

### ğŸ¯ Implementation Strategy

#### **Primary: External Secrets + 1Password Connect**

**ğŸ”§ Bootstrap Phase:**
```yaml
# bootstrap/resources.yaml - One-time setup
apiVersion: v1
kind: Secret
metadata:
  name: onepassword-connect
  namespace: external-secrets
type: Opaque
data:
  token: <base64-encoded-1password-token>
```

**ğŸ”„ Runtime Secret Sync:**
```yaml
# Example ExternalSecret
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: database-credentials
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: onepassword-store
    kind: SecretStore
  target:
    name: database-credentials
  dataFrom:
    - extract:
        key: kubernetes/infra/database
```

#### Notes
- Bootstrap writes the 1Password Connect token Secret and creates External Secret stores. Thereafter, all workload/application secrets are pulled at reconcile time.

### ğŸ›¡ï¸ Security Standards

| Standard | Requirement | Implementation |
|---|---|---|
| **ğŸš« Zero Plaintext** | No secrets in clear text | External Secrets (1Password) only |
| **ğŸ”„ Automatic Rotation** | Regular secret updates | 1Password Connect with 1-hour refresh |
| **ğŸš¨ Alerting** | Failed sync notifications | Flux alerts on decryption/ES failures |
| **ğŸ“ Documentation** | Rotation runbooks | Standardized procedures for all secret types |
| **ğŸ” Access Control** | Least privilege access | 1Password RBAC, Kubernetes RBAC |

### ğŸ“Š Secret Categories

| Category | Storage Method | Rotation Frequency | Example |
|---|---|---|---|
| **ğŸ—„ï¸ Database Credentials** | External Secrets | 90 days | PostgreSQL passwords |
| **ğŸ”‘ TLS Certificates** | External Secrets | Auto-renewal | cert-managed certs |
| **ğŸ”Œ API Keys** | External Secrets | 30-90 days | External service tokens |
| **ğŸš€ Bootstrap Secrets** | External Secrets | 1Password token rotation via 1Password |
| **ğŸ‘¥ Service Account Keys** | External Secrets | 365 days | CI/CD deployment keys |

### âš¡ Performance Optimizations

| Optimization | Description | Impact |
|---|---|---|
| **ğŸ”„ Refresh Interval** | 1-hour refresh for most secrets | Reduces 1Password API calls |
| **ğŸ“¦ Secret Caching** | External Secrets operator caching | Faster pod startup |
| **ğŸ¯ Selective Sync** | Only sync needed secrets per namespace | Reduced memory footprint |
| **ğŸš€ Bulk Operations** | Batch secret operations where possible | Improved sync performance |

## 9. ğŸŒ Networking (Cilium)

> **ğŸ”— High-Performance eBPF Networking**
>
> Cilium provides the foundation for our multi-cluster connectivity with advanced security and observability capabilities.

### ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    subgraph "Bootstrap Phase (Helmfile)"
        A[Cilium Core]
        B[kubeProxyReplacement: true]
        C[WireGuard Encryption]
    end

    subgraph "Day-2 Management (Flux)"
        D[BGP Peering]
        E[Gateway API]
        F[ClusterMesh]
        G[IPAM Pools]
    end

    A --> D
    A --> E
    A --> F
    A --> G

    subgraph "Security & Observability"
        H[Network Policies]
        I[Hubble Monitoring]
        J[Service Mesh]
    end

    D --> H
    E --> I
    F --> J
```

### ğŸ”§ Core Installation (Bootstrap)

| Component | Method | Key Settings | Purpose |
|---|---|---|---|
| **ğŸŒ Cilium Agent** | Helmfile (DaemonSet) | `kubeProxyReplacement: true` | eBPF data plane, replaces kube-proxy |
| **âš™ï¸ Cilium Operator** | Helmfile (Deployment) | WireGuard encryption enabled | Control plane management |
| **ğŸ”’ WireGuard** | Built-in | Transparent encryption | Node-to-node encryption |

### ğŸ“… Dayâ€‘2 Features (Flux Managed)

Located in `kubernetes/infrastructure/networking/cilium/*`:

| Feature | Path | Component | Function |
|---|---|---|---|
| **ğŸ”— BGP Peering** | `bgp/` | `CiliumBGPPeeringPolicy` | Pod/LB IP advertisement |
| **ğŸŒ‰ Gateway API** | `gateway/` | `GatewayClass` + `Gateways` | North-south traffic management |
| **ğŸ”— ClusterMesh** | `clustermesh/` | `ExternalSecret` | Cross-cluster connectivity |
| **ğŸ“Š IPAM** | `ipam/` | `CiliumIPAMPool` | L2/LB IP pool management |

Gateway policy note: This platform uses Cilium Gateway exclusively; Envoy Gateway is not part of this design.

### ğŸš€ Key Benefits

| Benefit | Description | Impact |
|---|---|---|
| **âš¡ High Performance** | eBPF-based data plane | Near bare-metal network performance |
| **ğŸ”’ Security** | WireGuard + Network Policies | Defense-in-depth network security |
| **ğŸ”— Multi-Cluster** | ClusterMesh integration | Seamless cross-cluster service discovery |
| **ğŸ“Š Observability** | Hubble + Prometheus integration | Full network visibility and monitoring |
| **ğŸ›ï¸ Flexibility** | Gateway API + BGP support | Advanced traffic routing capabilities |

## 10. ğŸ’¾ Storage Architecture

> **ğŸ—„ï¸ High-Performance Distributed Storage**
>
> Our multi-cluster storage strategy provides optimal performance and isolation while maintaining operational simplicity.

### ğŸ—ï¸ Storage Architecture Overview

```mermaid
graph TB
    subgraph "Infrastructure Cluster"
        A1[Rook-Ceph Operator]
        A2[Rook-Ceph Cluster]
        A3[Block/File/Object Storage]
        A4[OpenEBS LocalPV]
        A1 --> A2
        A2 --> A3
        A4 --> A3
    end

    subgraph "Application Cluster"
        B1[Rook-Ceph Operator]
        B2[Rook-Ceph Cluster]
        B3[App-focused Storage]
        B4[OpenEBS LocalPV - Default]
        B1 --> B2
        B2 --> B3
        B4 --> B3
    end

    A3 -.-> |Cross-cluster services| B3
    B3 -.-> |High-performance local| B3
```

### ğŸ“Š Cluster Storage Comparison

| Cluster | Storage Solution | Primary Use | Performance Target | Key Features |
|---|---|---|---|---|
| **ğŸ­ Infra** | ğŸ—„ï¸ Rook-Ceph + OpenEBS | Platform services, databases | High throughput NVMe | Block/file/object, monitoring |
| **ğŸ¯ Apps** | ğŸ—„ï¸ Dedicated Rook-Ceph + OpenEBS | Application workloads | Multi-GB/s local NVMe | Isolated storage, default local PV |

### ğŸ¯ Why Dedicated Storage for Apps Cluster?

| Challenge | Solution | Benefit |
|---|---|---|
| **ğŸš« Network Bottleneck** - 1 Gbps router cap | **Local Ceph cluster** on apps side | Eliminates cross-cluster I/O bottleneck |
| **âš¡ Performance** - Latency sensitive workloads | **In-cluster replication** and backfill | Multi-GB/s NVMe-backed performance |
| **ğŸ”§ Operational Simplicity** | **Aligned Ceph versions** (v19.2.3) | Simplified management and tooling |
| **ğŸ›¡ï¸ Isolation** | Dedicated storage per cluster | Prevents noisy neighbor problems |

### ğŸ—„ï¸ Storage Classes & Usage

| Storage Class | Cluster | Type | Use Case | Performance |
|---|---|---|---|---|
| **rook-ceph-block** | Infra + Apps | Block storage | Databases, stateful apps | High IOPS, low latency |
| **rook-ceph-fs** | Infra | File storage | Shared file systems | Concurrent access |
| **openebs-local-nvme** | Infra + Apps | Local storage | High-performance workloads | NVMe speed, local only |
| **rook-ceph-object** | Infra | Object storage | S3-compatible storage | Scalable object access |

### ğŸ“ˆ Performance Optimizations

| Optimization | Implementation | Impact |
|---|---|---|
| **ğŸ”§ NVMe Device Class** | `ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"` | Optimized SSD performance |
| **ğŸ“Š Local NVMe Priority** | OpenEBS as default for apps | Maximum I/O performance |
| **ğŸ”„ Replication Strategy** | Local cluster replication | Minimal cross-cluster traffic |
| **ğŸ“ˆ Monitoring** | Ceph metrics + alerts | Performance visibility |

### ğŸ› ï¸ Operational Benefits

| Feature | Description | Operational Impact |
|---|---|---|
| **ğŸ”§ Version Alignment** | Same Ceph v19.2.3 across clusters | Simplified upgrades, tooling consistency |
| **ğŸ“Š Integrated Monitoring** | Ceph + Prometheus + Grafana | Full storage observability |
| **ğŸš¨ Automated Alerts** | Storage health and performance alerts | Proactive issue detection |
| **ğŸ“– Standardized Runbooks** | Consistent operational procedures | Reduced operational complexity |

### ğŸ›ï¸ Configuration Highlights

```yaml
# From cluster-settings.yaml
ROOK_CEPH_IMAGE_TAG: v19.2.3
ROOK_CEPH_OSD_DEVICE_CLASS: "ssd"
CEPH_BLOCK_STORAGE_CLASS: "rook-ceph-block"
OPENEBS_STORAGE_CLASS: "openebs-local-nvme"
OPENEBS_BASEPATH: "/var/mnt/openebs"
```

## 11. ğŸ“Š Observability Strategy

> **ğŸ” Centralized Monitoring & Logging**
>
> Our observability architecture provides comprehensive visibility across both clusters with centralized storage and distributed collection.

### ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    subgraph "Infrastructure Cluster (Central)"
        A1[VictoriaMetrics Global]
        A2[VictoriaLogs]
        A3[Grafana]
        A4[Alertmanager]
        A5[Flux Alert Providers]
    end

    subgraph "Application Cluster (Leaf)"
        B1[vmagent]
        B2[kube-state-metrics]
        B3[node-exporter]
        B4[Fluent Bit]
        B5[OpenTelemetry Collector]
    end

    subgraph "Data Flow"
        C1[Metrics Collection]
        C2[Log Collection]
        C3[Trace Collection]
    end

    B1 --> C1
    B2 --> C1
    B3 --> C1
    B4 --> C2
    B5 --> C3

    C1 --> A1
    C2 --> A2
    C3 --> A1

    A1 --> A3
    A1 --> A4
    A5 --> A4
```

### ğŸ¯ Cluster Strategy

| Cluster | Role | Components | Data Flow |
|---|---|---|---|
| **ğŸ­ Infra** | **Central Storage & Visualization** | VictoriaMetrics global, VictoriaLogs, Grafana, Alertmanager | Stores all cluster data, provides dashboards/alerts |
| **ğŸ¯ Apps** | **Lightweight Collection** | vmagent, kube-state-metrics, node-exporter, Fluent Bit | Forwards all data to infra cluster |

### ğŸ“± Apps Cluster: Leaf Observability Pack

#### ğŸ”„ Metrics Collection (Pull + Forward)

| Component | Purpose | Data Flow | Resource Usage |
|---|---|---|---|
| **ğŸ“¡ vmagent** | Discovers and scrapes targets | Remote write to infra vminsert via vmauth | 100-300m CPU / 256-512Mi RAM |
| **ğŸ“Š kube-state-metrics** | Kubernetes object metrics | Scraped by vmagent | Lightweight |
| **ğŸ–¥ï¸ node-exporter** | OS/host metrics (CPU, memory, disk I/O, network) | Scraped by vmagent | Essential for host visibility |
| **ğŸŒ Cilium/Hubble** | Network/L7 metrics (optional) | ServiceMonitors â†’ vmagent | Network visibility |

#### ğŸ“ Logs Collection (Push)

| Component | Function | Configuration | Resource Usage |
|---|---|---|---|
| **ğŸ“‹ Fluent Bit** | Ships container/kubelet/audit logs | Compression, batching, `cluster=apps` labels | 50-200m CPU / 128-256Mi RAM |
| **ğŸ”— vmauth** | Multi-tenant routing | Insert endpoint with TLS/auth | Minimal overhead |

#### ğŸ” Traces Collection (Optional)

| Component | Function | Integration | Notes |
|---|---|---|---|
| **ğŸ” OpenTelemetry Collector** | Receives OTLP from applications | Exports to infra tracing backend | DaemonSet or agent mode |

### ğŸ¤” Why Keep node-exporter with vmagent?

| Component | Role | Complementarity |
|---|---|---|
| **ğŸ“¡ vmagent** | Prometheus-compatible scraper/forwarder | Discovers and pulls metrics, doesn't generate host metrics |
| **ğŸ–¥ï¸ node-exporter** | Canonical OS/host signals source | Provides CPU, filesystem saturation, network, thermal data |
| **ğŸ’¡ Synergy** | Complete visibility | kubelet/cAdvisor insufficient for host-level detail |

### âš™ï¸ Configuration Details (Apps â†’ Infra)

#### ğŸ”— Endpoints (from cluster-settings)

```yaml
# Metrics Configuration
GLOBAL_VM_INSERT_ENDPOINT: "victoria-metrics-global-vminsert.observability.svc.cluster.local:8480"
OBSERVABILITY_LOG_ENDPOINT_HOST: "victorialogs-vmauth.observability.svc.cluster.local"
OBSERVABILITY_LOG_ENDPOINT_PORT: "9428"
OBSERVABILITY_LOG_TENANT: "apps"
```

#### ğŸ›¡ï¸ NetworkPolicy Configuration

| Direction | Allowed Traffic | Ports | Purpose |
|---|---|---|---|
| **Egress** | DNS + kube-apiserver | 53, 443 | Cluster functionality |
| **Egress** | infra vmauth/vminsert | 8480/8427 | Metrics forwarding |
| **Egress** | VictoriaLogs insert | 9428 | Log forwarding |
| **Default** | Deny all other traffic | - | Security |

#### ğŸ” Security Configuration

| Component | Secret Management | Access Pattern |
|---|---|---|
| **vmagent** | External Secrets | Client credentials + CA roots |
| **Fluent Bit** | External Secrets | TLS certs for log shipping |
| **OTel Collector** | External Secrets | Tracing backend access |

### ğŸš« What We DON'T Run on Apps Cluster

| Component | Reason | Alternative |
|---|---|---|
| **ğŸ“Š VictoriaMetrics TSDB** | Storage consolidation | Remote write to infra |
| **ğŸš¨ VMAlert/Alertmanager** | Centralized alerting | Infra cluster handles all alerts |
| **ğŸ“ˆ Grafana** | Single visualization layer | Access infra Grafana via network |
| **ğŸ’¾ Long-term storage** | Cost efficiency | Central storage on infra |

### ğŸ“‹ CRD Requirements for Apps Cluster

#### Required CRD Bundles

| CRD Set | Purpose | Examples |
|---|---|---|
| **ğŸ“Š VictoriaMetrics Operator** | VM resource definitions | VMAgent, VMServiceScrape, VMRule, VMAuth, VMUser |
| **ğŸ”„ Prometheus Operator (Compatibility)** | Upstream chart support | ServiceMonitor, PodMonitor, PrometheusRule |

#### ğŸš€ Bootstrap Method

```bash
# Phase 0: Install CRDs on both clusters
helmfile -f bootstrap/helmfile.d/00-crds.yaml -e apps template \
  | yq ea 'select(.kind == "CustomResourceDefinition")' \
  | kubectl apply -f -
```

### ğŸ“ˆ Scaling & Performance

| Metric | Starting Point | Scaling Guidance |
|---|---|---|
| **vmagent CPU** | 100-300m | Scale with scrape cardinality |
| **vmagent Memory** | 256-512Mi | Scale with metric volume |
| **Fluent Bit CPU** | 50-200m | Scale with log throughput |
| **Fluent Bit Memory** | 128-256Mi | Scale with buffer size |
| **Network Bandwidth** | Depends on log/metric volume | Monitor compression ratios |

### âœ… Benefits of This Architecture

| Benefit | Description | Impact |
|---|---|---|
| **ğŸ’° Cost Efficiency** | Single storage backend | Reduced infrastructure costs |
| **âš¡ Performance** | Local aggregation, remote forwarding | Low latency collection |
| **ğŸ”§ Simplicity** | Centralized management | Easier operations |
| **ğŸ›¡ï¸ Security** | Controlled egress paths | Reduced attack surface |
| **ğŸ“ˆ Scalability** | Distributed collection | Linear scaling capability |


## 12. ğŸ”„ CI/CD & Policy

| Component | Implementation | Details |
| :--- | :--- | :--- |
| **ğŸ” CI Validation** | Pipeline checks | `kubeconform` (strict), `kustomize build` for each cluster entry, `flux build`/`flux diff` |
| **ğŸ›¡ï¸ Policy Management** | Admission control | Start with audit-mode ValidatingAdmissionPolicy or Kyverno; then enforce baseline/restricted policies |
| **ğŸ” Image Security** | Provenance verification | Add image provenance (cosign/notation) where applicable |
| **ğŸ¤– Image Automation** | Selected apps | `ImageRepository`, `ImagePolicy`, `ImageUpdateAutomation` writing to staging branches |

## 13. ğŸ¢ Multiâ€‘Tenancy (optional)

| Aspect | Implementation | Details |
| :--- | :--- | :--- |
| **ğŸ“ Team Structure** | Directory layout | `workloads/tenants/<team>`: Namespace + ServiceAccount + RBAC |
| **ğŸ”§ Resource Management** | Team-scoped resources | `Kustomization`/`HelmRelease` with `serviceAccountName` specified |
| **ğŸš« Isolation Rules** | Namespace boundaries | Enforce: no cross-namespace refs; lock Flux flags (`--no-cross-namespace-refs`, `--no-remote-bases`) |
| **ğŸ“ˆ Scaling Strategy** | Performance optimization | Scale via controller sharding and `--watch-label-selector` if needed |

## 14. ğŸ”§ Operations & Runbooks (abridged)

| Operation | Command/Process | Description |
| :--- | :--- | :--- |
| **ğŸš€ Fresh Cluster** | `task bootstrap:talos` â†’ `task bootstrap:apps` â†’ `flux get ks -A` | Complete cluster bootstrap sequence |
| **ğŸ”„ Bootstrap Re-run** | CRDs â†’ `helmfile sync` â†’ suspend Flux | Safe re-run of bootstrap phases with Flux suspension if necessary |
| **â¸ï¸ Flux Control** | `flux suspend\|resume kustomization <name>` | Pause/resume specific Kustomizations |
| **ğŸ”™ Rollbacks** | `helm rollback` / Git revert | Bootstrap charts: `helm rollback`; GitOps resources: Git revert + Flux reconcile |
| **â¬†ï¸ Node Upgrades** | `.taskfiles/talos/upgrade-node` | Talos node upgrades with drain logic; verify Cilium + storage DaemonSets availability |

## 15. ğŸ“… Phased Implementation Plan (Sprints)

| Sprint | Focus | Key Tasks |
| :--- | :--- | :--- |
| **Sprint 0** | ğŸ—ï¸ Foundations | Lock controller flags; decide bootstrap ownership; add CI scaffolding |
| **Sprint 1** | ğŸ“ Repo Skeleton | Create/normalize cluster entries; add `bootstrap/` and `.taskfiles/bootstrap`; ensure values reuse between Helmfile and Flux |
| **Sprint 2** | ğŸ” Secrets & Decryption | Finalize External Secrets with 1Password for all secrets (bootstrap/runtime) |
| **Sprint 3** | âš™ï¸ Platform Controllers | External Secrets, cert-manager CRDs/issuers, CNPG (if used); health checks and ordering |
| **Sprint 4** | ğŸŒ Networking Dayâ€‘2 | Flux-manage BGP, Gateway API, ClusterMesh secret; health checks for Cilium |
| **Sprint 5** | ğŸ’¾ Storage | Infra Ceph cluster; Apps client/operator (optional); PVC tests and monitoring |
| **Sprint 6** | ğŸ“Š Observability | VM global stack in infra; remote write from apps; Flux Alerts/Receivers |
| **Sprint 7** | ğŸ”„ CI/CD & Policy | kubeconform/kustomize/flux build; policy auditâ†’enforce; image automation (staging) |
| **Sprint 8** | ğŸš€ Workloads Migration | Normalize app bases/overlays; migrate an anchor app endâ€‘toâ€‘end; rollback test |
| **Sprint 9** | ğŸ¢ Tenancy & RBAC (optional) | Team namespaces + RBAC; perâ€‘team Kustomizations; isolation verification |
| **Sprint 10** | ğŸ›¡ï¸ Reliability, DR, Hardening | Backups/restore drills; PodSecurity; image provenance; finalize runbooks |

## 16. ğŸ“ Decisions & Rationale

| Decision | Rationale |
| :--- | :--- |
| **ğŸ”§ Flux Bootstrap Method** | Flux is bootstrapped via Helmfile, not selfâ€‘managed in this repo (simpler, deterministic bootstrap) |
| **ğŸš« Remove Aggregator** | Remove aggregator `ks.yaml` to avoid duplication; wire clusters directly to directories |
| **ğŸŒ Cilium Management** | Keep Cilium core via Helmfile; manage dayâ€‘2 via Flux to separate infra provisioning from policy/config |
| **ğŸ“ Source Strategy** | Prefer local Git/OCI sources; avoid remote bases and crossâ€‘namespace references |

## 17. âš ï¸ Risks & Mitigations

| Risk | Mitigation Strategy |
| :--- | :--- |
| **ğŸ“‹ CRD Ordering Issues** | Twoâ€‘phase bootstrap; `--include-crds`, postRenderer filter |
| **ğŸ” Secret Store Outages** | Alert on ES sync; ensure 1Password Connect is HA |
| **ğŸŒ Network Disruption** | Guard BGP/Gateway changes behind Kustomization toggles; staged rollouts |
| **âš™ï¸ Controller Overload** | Shard controllers, use label selectors, tune reconcile intervals |

## 18. âœ… Acceptance Criteria & Metrics

### ğŸ”¬ Technical Criteria
- **ğŸ“Š Health Status**: 100% of Kustomizations healthy; zero missing `dependsOn`; CI green on kubeconform/kustomize/flux build
- **âš¡ Performance**: Mean reconciliation time within target; alert coverage for Flux, cert-manager, storage, and Cilium

### ğŸ“ˆ Process Metrics
- **ğŸ”„ Throughput**: PR throughput/predictability; change failure rate
- **ğŸ›¡ï¸ Reliability**: Successful restore drills; rollback MTTR |

---

## 19. ğŸ› ï¸ Workloads & Versions

> **ğŸ“… Version Snapshot: 2025-10-20**
>
> The tables below list the platform workloads we deploy per cluster and the versions we will pin at bootstrap. Versions reflect the latest stable charts/releases available on October 20, 2025. We keep them explicit to ensure reproducible installs; upgrades follow our normal PR process.

### ğŸ­ Infrastructure Cluster - Platform Services

| Component | ğŸ·ï¸ Version | ğŸ“¦ Namespace | ğŸ”§ Install Method | ğŸ“ Purpose & Notes |
|---|---|---|---|---|
| **ğŸŒ Cilium** | `1.18.2` | `kube-system` | Helm (OCI) | Core CNI; dayâ€‘2 features via Flux (bgp/gateway/clustermesh) |
| **ğŸ” CoreDNS** | `1.38.0` | `kube-system` | Helm (OCI) | Cluster DNS resolution |
| **ğŸ” External Secrets** | `0.20.3` | `external-secrets` | Helm (repo) | 1Password Connect integration |
| **ğŸ”’ cert-manager** | `v1.19.1` | `cert-manager` | Helm (OCI) | Cluster issuers + ACME automation |
| **ğŸ—„ï¸ Rookâ€‘Ceph Operator** | `latest` | `rook-ceph` | Helm (repo) | Storage operator; Ceph v19.2.3 pinned |
| **ğŸ—„ï¸ Rookâ€‘Ceph Cluster** | `latest` | `rook-ceph` | Helm (repo) | CephCluster + pools/SCs; Ceph v19.2.3 |
| **ğŸ’¾ OpenEBS LocalPV** | `4.3.x` | `openebs-system` | Helm (repo) | High-performance NVMe storage |
| **ğŸ“Š VictoriaMetrics** | `0.61.8` | `observability` | Helm (repo/OCI) | Metrics stack (infra only) |
| **ğŸ“ VictoriaLogs** | `0.0.17` | `observability` | Helm (OCI) | Centralized logging |
| **ğŸ“‹ Fluent Bit** | `0.53.0` | `observability` | Helm (repo) | Log shipping to VictoriaLogs |
| **ğŸ“¦ Harbor Registry** | `1.18.0` | `harbor` | Helm (repo) | Container registry; app v2.14.0 |
| **ğŸš€ Actions Runner** | `0.12.0` | `actions-runner-system` | Helm (OCI) | GitHub ARC controller |

### ğŸ¯ Application Cluster - Workloads & Services

| Component | ğŸ·ï¸ Version | ğŸ“¦ Namespace | ğŸ”§ Install Method | ğŸ“ Purpose & Notes |
|---|---|---|---|---|
| **ğŸŒ Cilium** | `1.18.2` | `kube-system` | Helm (OCI) | Core CNI; dayâ€‘2 features via Flux |
| **ğŸ” CoreDNS** | `1.38.0` | `kube-system` | Helm (OCI) | Cluster DNS resolution |
| **ğŸ” External Secrets** | `0.20.3` | `external-secrets` | Helm (repo) | 1Password Connect integration |
| **ğŸ”’ cert-manager** | `v1.19.1` | `cert-manager` | Helm (OCI) | Cluster issuers + ACME automation |
| **ğŸ—„ï¸ Rookâ€‘Ceph Operator** | `latest` | `rook-ceph` | Helm (repo) | Storage operator; Ceph v19.2.3 pinned |
| **ğŸ—„ï¸ Rookâ€‘Ceph Cluster** | `latest` | `rook-ceph` | Helm (repo) | Dedicated apps storage; Ceph v19.2.3 |
| **ğŸ’¾ OpenEBS LocalPV** | `4.3.x` | `openebs-system` | Helm (repo) | Default storage; openebs-local-nvme |
| **ğŸ“¬ Kafka Operator** | `0.48.0` | `messaging` | Helm (repo) | Strimzi Kafka platform |
| **ğŸ—‚ï¸ Schema Registry** | `latest` | `messaging` | Kustomize | Confluent Schema Registry |
| **ğŸ˜ PostgreSQL Operator** | `0.26.0` | `cnpg-system` | Helm (repo) | CloudNativePG for app databases |
| **ğŸ“‹ Fluent Bit** | `0.53.0` | `observability` | Helm (repo) | Ships logs/metrics to infra |
| **ğŸš€ Actions Runner** | `0.12.0` | `actions-runner-system` | Helm (OCI) | GitHub ARC + scale sets |

### ğŸ›ï¸ Optional/Edge Components

| Component | ğŸ·ï¸ Version | âš ï¸ Status | ğŸ“ Notes |
|---|---|---|---|
| **ğŸª Spegel** | `0.4.0` | ğŸŸ¡ Conditional | Node image mirror; requires Talos compatibility validation |
<!-- Envoy Gateway removed: this platform uses Cilium Gateway exclusively -->

### ğŸ“‹ Version Management Strategy

```mermaid
graph TD
    A[Version Pinning] --> B[Reproducible Installs]
    A --> C[No Accidental Drifts]
    A --> D[Controlled Upgrades]

    E[PR Process] --> F[kubeconform checks]
    E --> G[kustomize build]
    E --> H[flux build validation]
    E --> I[Rollout plan]

    J[Rook-Ceph] --> K[Ceph Image Pinned]
    J --> L[Operator Chart Tracking]
    J --> M[Upgrade Documentation]
```

### ğŸ¯ Version Stewardship Policies

| Policy | Implementation | Impact |
|---|---|---|
| **ğŸ“Œ Pin All Versions** | Helm chart versions + image tags in repo | Prevents accidental upgrades |
| **ğŸ”„ PR-Based Upgrades** | All version changes require PR + validation | Controlled rollout process |
| **ğŸ“Š Validation Pipeline** | kubeconform + kustomize + flux build checks | Ensures compatibility |
| **ğŸ“ Change Tracking** | Document exact chart versions in changelogs | Full audit trail |
| **ğŸ—„ï¸ Special Handling** | Rookâ€‘Ceph: Ceph image v19.2.3 pinned separately | Complex dependency management |

### âš¡ Upgrade Process Flow

1. **ğŸ“‹ Planning** - Create upgrade plan with compatibility matrix
2. **ğŸ”§ Testing** - Validate in staging environment
3. **ğŸ“ PR Creation** - Include all validation checks
4. **âœ… Review** - Architecture team approval
5. **ğŸš€ Deployment** - Automated via Flux
6. **ğŸ“Š Monitoring** - Post-upgrade health verification
7. **ğŸ“– Documentation** - Update changelog and runbooks

## 20. ğŸ”— Cilium ClusterMesh + SPIRE (Zeroâ€‘Trust, Multiâ€‘Cluster)

> **ğŸ›¡ï¸ Secure Multi-Cluster Identity & Connectivity**
>
> This section describes how we achieve secure, multiâ€‘cluster connectivity and identity with Cilium ClusterMesh and SPIRE, and how we operate it dayâ€‘toâ€‘day.

### ğŸ¯ Zero-Trust Goals

| Goal | Description | Implementation |
| :--- | :--- | :--- |
| **ğŸŒ Seamless Connectivity** | L3/L4/L7 connectivity between clusters with native service discovery | Cilium ClusterMesh + Global Services |
| **ğŸ” Identity-Based Auth** | Mutual authentication and mTLS based on SPIFFE identities, not IPs | SPIRE + Cilium Auth Policies |
| **ğŸ›¡ï¸ Zero-Trust Policy** | Permit only authenticated/authorized traffic; deny by default | CiliumNetworkPolicy + CiliumAuthPolicy |
| **ğŸ“Š Full Observability** | Complete flow visibility and metrics across clusters | Hubble + Hubble Relay |

### ğŸ—ï¸ Component Architecture

```mermaid
graph TB
    subgraph "Infrastructure Cluster"
        C1[Cilium Agent/Operator]
        CM1[ClusterMesh API Server]
        SPIRE1[SPIRE Server + Agents]
        HB1[Hubble + Relay]
    end

    subgraph "Application Cluster"
        C2[Cilium Agent/Operator]
        CM2[ClusterMesh API Server]
        SPIRE2[SPIRE Server + Agents]
        HB2[Hubble + Relay]
    end

    subgraph "External Services"
        ES[External Secrets]
        DNS[DNS Records]
        LB[Load Balancers]
    end

    CM1 <--> |Clustermesh| CM2
    SPIRE1 <--> |Federated Trust| SPIRE2
    HB1 <--> |Flow Data| HB2

    ES --> CM1
    ES --> CM2
    DNS --> LB
    LB --> CM1
    LB --> CM2
```

### ğŸ“‹ Component Roles & Responsibilities

| Component | Role | Key Features |
| :--- | :--- | :--- |
| **ğŸŒ Cilium (agent/operator)** | CNI + Data Plane | eBPF acceleration, Gateway API, BGP control plane |
| **ğŸ”— ClusterMesh API Server** | Cross-cluster control plane | Service discovery, identity exchange, LoadBalancer exposed |
| **ğŸ›¡ï¸ SPIRE (Server + Agent)** | Identity management | SPIFFE SVID issuance, short-lived certs, pod-level identity |
| **ğŸ“Š Hubble + Relay** | Observability | Network flow visibility, metrics, security monitoring |
| **ğŸ” External Secrets** | Secret distribution | Secure ClusterMesh material distribution across clusters |

### 20.3 ğŸ”§ Controlâ€‘Plane Topology

| Configuration | Value/Setting | Purpose |
| :--- | :--- | :--- |
| **ğŸ·ï¸ Cluster Identity** | `cluster.name`: `infra` or `apps` | Unique cluster identification |
| **ğŸ”¢ Cluster ID** | `cluster.id`: `1` (infra), `2` (apps) | Numeric ID unique per cluster |
| **ğŸŒ ClusterMesh API** | `clustermesh.useAPIServer: true` | Enable ClusterMesh API server |
| **ğŸ”„ Load Balancer** | `clustermesh.apiserver.service.type: LoadBalancer` | Expose API server via LB with DNS |
| **ğŸŒ DNS Records** | `infra-cilium-apiserver.<domain>` â†’ infra LB<br>`apps-cilium-apiserver.<domain>` â†’ apps LB | ClusterMesh endpoint resolution |
| **ğŸ” Secret Management** | ExternalSecret `kube-system/cilium-clustermesh` | Perâ€‘peer CA, client/server certs, endpoints |

### 20.4 ğŸ›¡ï¸ Workload Identity & mTLS with SPIRE

| Component | Function | Configuration Details |
| :--- | :--- | :--- |
| **ğŸ” Authentication Provider** | SPIRE as Cilium auth provider | SPIRE server + agents installed; agents run on each node |
| **ğŸ« Identity Issuance** | SVID minting for pods | Based on k8s selectors; SPIFFE ID format: `spiffe://monosense.io/ns/<ns>/sa/<serviceaccount>` |
| **â° Rotation Management** | SVID TTL and rotation | Configurable; default frequent rotation (hours) to minimize key longevity |
| **ğŸ”— Defense in Depth** | Multi-layer encryption | WireGuard (nodeâ€‘toâ€‘node) + SPIRE (workload identity + mTLS) |

### 20.5 ğŸ›¡ï¸ Policy Model

| Policy Aspect | ğŸ”§ Implementation | ğŸ“‹ Purpose |
| :--- | :--- | :--- |
| **ğŸ†” Identityâ€‘Centric Auth** | `CiliumNetworkPolicy`/`CiliumClusterwideNetworkPolicy` + `CiliumAuthPolicy` | Authorization based on workload identity, not IP |
| **ğŸ” mTLS Requirements** | Require mTLS between specific identities/namespaces | Secure communication with SPIFFE authentication |
| **ğŸš« Default Deny** | Baseline policy denies all traffic by default | Security-first approach with explicit allow rules |
| **ğŸ¯ L4/L7 Control** | Allow only intended directions and ports | Precise traffic control and attack surface reduction |

#### ğŸ” SPIRE Authentication Example

**Require SPIRE authentication for traffic to a namespace:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumAuthPolicy
metadata:
  name: require-mtls-to-synergyflow
  namespace: synergyflow
spec:
  selectors:
    - namespace: synergyflow
  requiredAuthentication:
    - type: spiffe
```

#### ğŸŒ Global Service Example

**Global service for crossâ€‘cluster failover:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: schema-registry
  namespace: messaging
  annotations:
    io.cilium/global-service: "true"
spec:
  selector:
    app.kubernetes.io/name: schema-registry
  ports:
    - name: http
      port: 8081
      targetPort: 8081
```

### 20.6 ğŸš€ Dayâ€‘0/Dayâ€‘1 Setup (per cluster)

| Phase | ğŸ”§ Configuration Details | âœ… Outcome |
| :--- | :--- | :--- |
| **ğŸ”§ Phase 1**<br>*(Cilium Installation)* | **Helmfile installs Cilium with:**<br>â€¢ `cluster.name`, `cluster.id`, `ipv4NativeRoutingCIDR`<br>â€¢ `clustermesh.useAPIServer: true`<br>â€¢ `apiserver.service.type: LoadBalancer`<br>â€¢ `authentication.mutual.spire.enabled: true`<br>â€¢ `authentication.mutual.spire.install.enabled: true`<br>â€¢ `encryption.type: wireguard` (nodeâ€‘toâ€‘node)<br>â€¢ Hubble Relay/UI enabled with ServiceMonitor | Cilium core components installed with ClusterMesh and SPIRE capabilities |
| **ğŸŒ Phase 2**<br>*(DNS Configuration)* | **DNS records created for:**<br>â€¢ Each cluster's clustermeshâ€‘apiserver LB IP<br>â€¢ Values stored in `cluster-settings` | ClusterMesh endpoints resolvable via DNS |
| **ğŸ” Phase 3**<br>*(Secret Distribution)* | **ExternalSecret syncs:**<br>â€¢ `kube-system/cilium-clustermesh` from secret store<br>â€¢ Path specified in `cluster-settings` | ClusterMesh connection secrets available in each cluster |
| **ğŸ”— Phase 4**<br>*(Mesh Establishment)* | **Cilium agents:**<br>â€¢ Observe the secret<br>â€¢ Connect to remote cluster's API server<br>â€¢ Establish the mesh | Cross-cluster connectivity established and operational |

### 20.7 ğŸ”§ Operations

| Operation Area | Commands/Checks | Purpose |
| :--- | :--- | :--- |
| **ğŸŒ Connectivity** | `cilium status` â€¢ `cilium clustermesh status` â€¢ `hubble status` â€¢ `hubble observe --follow` | Verify peering, cluster visibility, and authenticated flows |
| **ğŸ›¡ï¸ Identity & Auth** | `kubectl -n spire get spireentries` â€¢ SPIRE CLI â€¢ Review `CiliumAuthPolicy` | Inspect SPIRE entries, validate policies, audit in nonâ€‘prod |
| **â¬†ï¸ Upgrades** | Sequential cluster upgrade â€¢ Verify WG tunnel â€¢ Check SPIRE health â€¢ Monitor Hubble metrics | Safe upgrade process with health verification |

### 20.8 ğŸš¨ Failure Modes & Troubleshooting

| Failure Mode | Troubleshooting Steps |
| :--- | :--- |
| **ğŸ”— Peering Down** | Check LB/DNS for clustermeshâ€‘apiserver, verify Secret freshness, review agent logs for TLS errors |
| **ğŸ›¡ï¸ Auth Failures** | Verify SPIRE SVID issuance (selectors), check clock skew, validate `CiliumAuthPolicy` matches traffic |
| **ğŸŒ Routing Issues** | Ensure BGP peering is Established, verify Pod/LB CIDR route advertisement consistency |

### 20.9 ğŸ›¡ï¸ Security Posture

| Security Principle | Implementation |
| :--- | :--- |
| **ğŸ” Zeroâ€‘Trust Design** | Every connection authenticated (SPIRE) + encrypted (WireGuard) + authorized (Cilium policies) |
| **â° Credential Management** | Shortâ€‘lived SVIDs reduce blast radius; automated rotation |
| **ğŸ”’ Secret Security** | ClusterMesh secrets never stored in Git; pulled at reconcile time from secret store |

## 21. Security & Network Policy Baseline

This section defines the clusterâ€‘wide network security posture and the policy building blocks teams use. We defaultâ€‘deny all traffic and then explicitly allow the minimum required flows. Policies use a mix of Kubernetes `NetworkPolicy` and Cilium `CiliumNetworkPolicy`/`CiliumAuthPolicy` to enable SPIFFE/mTLS authorization.

### 21.1 ğŸ¯ Objectives

| Objective | Implementation Strategy |
| :--- | :--- |
| **ğŸš« Default Deny** | Deny all traffic by default; allow only minimum necessary ingress/egress |
| **ğŸ›¡ï¸ Identityâ€‘Aware Auth** | SPIRE (SPIFFE) + Cilium Auth for mTLS between workloads |
| **ğŸ§© Composable Policies** | Small, reusable allow patterns (DNS, API server, observability, gateway ingress, FQDN egress) |

### 21.2 ğŸ“‹ Namespace Baseline

Apply these policies in every application namespace:

| Policy | Purpose | Implementation |
| :--- | :--- | :--- |
| **ğŸš« Default Deny** | Block all traffic by default | Kubernetes `NetworkPolicy` with `podSelector: {}` |
| **ğŸŒ DNS Access** | Allow DNS resolution | Cilium `CiliumNetworkPolicy` targeting kube-dns/CoreDNS |
| **ğŸ”‘ API Server Access** | Allow k8s API communication | Cilium policy with `toEntities: [kube-apiserver]` |

**ğŸš« Default Deny Policy:**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes: [Ingress, Egress]
```

**ğŸŒ DNS Allow Policy:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-dns
spec:
  endpointSelector: {}
  egress:
    - toEndpoints:
        - matchLabels:
            k8s:io.kubernetes.pod.namespace: kube-system
            k8s-app: coredns
      toPorts:
        - ports:
            - port: "53"
              protocol: UDP
            - port: "53"
              protocol: TCP
```

**ğŸ”‘ API Server Policy:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-kube-apiserver
spec:
  endpointSelector: {}
  egress:
    - toEntities: [kube-apiserver]
      toPorts:
        - ports: [{ port: "443", protocol: TCP }]
```

### 21.3 ğŸŒ FQDN Egress Allowlists

| Use Case | ğŸ”§ Implementation | ğŸ“‹ Allowed Destinations |
| :--- | :--- | :--- |
| **ğŸ“¦ Package Repositories** | Workload needs outbound internet access | Strict allowlist using `toFQDNs` |
| **ğŸ³ OCI Registries** | Container image pulls | Pre-approved registry domains only |
| **ğŸ”’ Security Principle** | Default deny, explicit allow | Minimum required egress only |

**ğŸ“‹ Example Policy - FQDN Egress Allowlist:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: egress-allowlist-tofqdns
spec:
  endpointSelector: {}
  egress:
    - toFQDNs:
        - matchName: registry-1.docker.io
        - matchName: ghcr.io
        - matchName: quay.io
      toPorts:
        - ports:
            - { port: "80", protocol: TCP }
            - { port: "443", protocol: TCP }
```

### 21.4 ğŸšª Ingress via Gateway Only

| Security Principle | ğŸ”§ Implementation | ğŸ“‹ Traffic Control |
| :--- | :--- | :--- |
| **ğŸ›¡ï¸ Gateway-Only Access** | Expose applications through Cilium Gateway only | Single ingress point for all external traffic |
| **ğŸ”’ Source Verification** | Only accept traffic from Cilium Gateway dataplane pods | Prevents direct access to application pods |
| **ğŸŒ Port Control** | Restrict to standard HTTP/HTTPS ports | Consistent port policies across services |

**ğŸ“‹ Example Policy - Gateway-Only Ingress:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-from-gateway
spec:
  endpointSelector: {}
  ingress:
    - fromEndpoints:
        - matchLabels:
            k8s:io.kubernetes.pod.namespace: kube-system
            io.cilium.gateway: "true"
      toPorts:
        - ports: [{ port: "80", protocol: TCP }, { port: "443", protocol: TCP }]
```

### 21.5 ğŸ“Š Observability Paths

| Observability Type | ğŸ”§ Allowed Traffic | ğŸ“‹ Implementation |
| :--- | :--- | :--- |
| **ğŸ“ˆ Metrics Collection** | Scrape to metrics endpoints from vmagent/victoria | Use namespace/label selectors in NetworkPolicies |
| **ğŸ“ Log Shipping** | Allow logs egress to VictoriaLogs insert endpoint | Crossâ€‘namespace: prefer Cilium policies over IP allowlists |
| **ğŸ” Policy Preference** | Avoid brittle IP allowlists | Use identity-based policies for reliability |

### 21.6 ğŸ›¡ï¸ SPIFFE/mTLS Authorization

| Security Requirement | ğŸ”§ Implementation | ğŸ“‹ Purpose |
| :--- | :--- | :--- |
| **ğŸ†” Identity Verification** | Require SPIRE identities for sensitive paths | Zero-trust authentication based on workload identity |
| **ğŸ” mTLS Enforcement** | Use `CiliumAuthPolicy` for mutual TLS | Encrypted communication with identity verification |
| **ğŸ¯ Access Control** | Pair with L4/L7 `CiliumNetworkPolicy` | Granular control over ports and HTTP paths |

**ğŸ“‹ Example Policy - SPIFFE/mTLS Authorization:**
```yaml
apiVersion: cilium.io/v2
kind: CiliumAuthPolicy
metadata:
  name: require-spiffe-to-api
  namespace: my-namespace
spec:
  selectors:
    - namespace: my-namespace
      identities:
        - spiffe://monosense.io/ns/my-namespace/sa/backend
  requiredAuthentication:
    - type: spiffe
```

**ğŸ”— Policy Layering:**
- **ğŸ›¡ï¸ Authentication**: SPIFFE identity verification via `CiliumAuthPolicy`
- **ğŸ¯ Authorization**: L4/L7 traffic control via `CiliumNetworkPolicy`

### 21.7 âœ… Application Policy Checklist

| âœ… Requirement | ğŸ“‹ Description |
| :--- | :--- |
| **ğŸš« Default Deny** | Apply default deny policy (Ingress + Egress) |
| **ğŸŒ DNS Access** | Allow DNS resolution |
| **ğŸ”‘ API Access** | Allow egress to kubeâ€‘apiserver if needed (clientâ€‘go, discovery) |
| **ğŸŒ External Egress** | Restrict external egress with `toFQDNs` |
| **ğŸšª Ingress Control** | Only allow ingress from Gateway (and sameâ€‘namespace where required) |
| **ğŸ›¡ï¸ mTLS Between Services** | Add SPIFFE `CiliumAuthPolicy` for mTLS between internal services |
| **ğŸ“Š Observability** | Add observability egress/ingress as needed |

### 21.8 âš ï¸ Exceptions

| Exception Type | Management |
| :--- | :--- |
| **ğŸ“ Temporary Allowances** | Document with owner and expiry date |
| **ğŸ“ Policy Tracking** | Separate policies per namespace |
| **ğŸ“‹ Central Logging** | Track in central `EXCEPTIONS.md` file |

### 21.9 ğŸ” Validation & Monitoring

| Validation Method | Purpose |
| :--- | :--- |
| **ğŸ”­ Flow Observation** | Use `hubble observe` to confirm policy hits and TLS status |
| **ğŸš¨ Alerting** | Add Prometheus alerts for denied flows above threshold |
| **ğŸ›¡ï¸ Identity Monitoring** | Monitor SPIRE SVID issuance failures |

### 21.10 â¬†ï¸ Upgrades & Migration

| Migration Strategy | Implementation |
| :--- | :--- |
| **ğŸ” Audit Mode** | Introduce policies in audit mode first, validate flows |
| **âœ… Enforcement** | Enforce policies after validation |
| **ğŸ“ Version Control** | Version policy docs with app releases to avoid drift |

## 22. Multiâ€‘Cluster Mesh Options â€” Decision Matrix (as of 2025â€‘10â€‘20)

This section compares Cilium ClusterMesh with alternative meshes frequently used for crossâ€‘cluster traffic (Istio, Linkerd, Kuma, Consul). It focuses on what matters operationally for Talos bareâ€‘metal clusters.

22.1 Quick Comparison

| Dimension | ğŸŒ Cilium ClusterMesh | ğŸ—ï¸ Istio (sidecar) | ğŸš€ Istio Ambient (multicluster) | âš¡ Linkerd | ğŸŒ Kuma | ğŸ”— Consul |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **ğŸŒ Network prereqs** | L3 pod/node reachability, nonâ€‘overlapping PodCIDRs | Gateways between clusters (no pod L3 required) | Gateways (alpha status for multicluster) | Gateways + Service mirror | Multiâ€‘zone; zone/global CP; gateways | Mesh gateways; WAN/peering or cluster peering |
| **ğŸ›¡ï¸ Identity/mTLS across clusters** | Inâ€‘cluster SPIRE mTLS GA; crossâ€‘cluster: appâ€‘level TLS recommended | Mature SPIFFE/mTLS across clusters | Sidecarâ€‘less model; multicluster alpha (early) | Mature automatic mTLS across clusters | Builtâ€‘in mTLS via Envoy | Builtâ€‘in mTLS via Envoy |
| **âš™ï¸ L7 features** | L3/L4 eBPF, L7 via Cilium Service Mesh (HTTP), policy; crossâ€‘cluster L7 limited | Deep L7 (traffic policy, resiliency, locality) | Emerging L7 (waypoints/ztunnel) | Simpler L7; SMI integrations | Envoy feature set; policy via CRDs | Envoy feature set; policy via Consul |
| **ğŸ” Service discovery** | Global Service annotations; MCS support (beta) | ServiceExport/Import or eastâ€‘west gateways; strong locality/failover | Same goals as sidecar path (alpha) | Service mirror + export | Zone/global discovery | Catalog + peering |
| **ğŸ“ˆ Scale & topology** | 255 clusters (default), 511 with tuning; KVStoreMesh for scale | Proven largeâ€‘scale; multiple topologies (multiâ€‘primary, primaryâ€‘remote) | Early for multicluster | Proven multiâ€‘cluster via gateways | Multiâ€‘zone hybrid K8s/VM | WAN/peering, hybrid |
| **ğŸ›ï¸ Controlâ€‘plane complexity** | Low (Cilium only) once underlay solved | Medium/High (Istio CP per cluster + gateways) | Medium (new components) | Low/Medium | Medium (global + zone CPs) | Medium (Consul servers + gateways) |
| **ğŸ¯ Best for** | Highest performance with eBPF; same policy model across clusters; simple eastâ€‘west LB | Rich L7 + crossâ€‘cluster mTLS now | Sidecarâ€‘less future; not production for multicluster yet | Lightweight secure multicluster | Hybrid environments, policyâ€‘rich | Hybrid and multiâ€‘platform estates |

### 22.2 ğŸ¯ When to Choose Which

| Solution | âœ… Choose When... | âŒ Avoid When... |
| :--- | :--- | :--- |
| **ğŸŒ Cilium ClusterMesh** | L3 connectivity available; primary L3/L4 policy needs; simple service failover; want highest eBPF performance | Need mature crossâ€‘cluster mTLS today; require advanced L7 traffic policies |
| **ğŸ—ï¸ Istio (sidecar)** | Need mature crossâ€‘cluster mTLS + advanced L7 policies (retries, locality-aware failover, progressive delivery) | Want simple architecture; prefer sidecarâ€‘less approach |
| **âš¡ Linkerd** | Want lightweight, opinionated mesh; crossâ€‘cluster mTLS via gateways and service mirroring | Need advanced L7 features; require complex routing |
| **ğŸŒ Kuma/Consul** | Expect hybrid environments (K8s + VMs); need multiâ€‘zone/global controlâ€‘plane | Pure K8s environment; want simpler setup |

### 22.3 ğŸ›£ï¸ Recommended Path for Our Talos Greenfield

| Approach | ğŸ¯ Strategy | ğŸ”§ Key Configuration |
| :--- | :--- | :--- |
| **ğŸŒ Primary** | Cilium ClusterMesh (now) + appâ€‘level TLS crossâ€‘cluster | Unique PodCIDRs; `clustermesh.useAPIServer: true`; WireGuard + SPIRE; Global Services; namespace policies |
| **ğŸ”„ Fallback** | Istio (sidecar) for selected namespaces only | Scope to specific apps; multiâ€‘primary with eastâ€‘west gateways; policy layering with Cilium baseline |

**ğŸŒ Primary Path Rationale:**
- **âœ… Cilium Standardization**: Already standardized on Cilium, need high performance and unified eBPF policy
- **ğŸ¯ Use Case**: Eastâ€‘west service reachability and failover
- **ğŸŒ Network**: Nonâ€‘overlapping PodCIDRs and L3 routing (BGP or static) between clusters

**ğŸ”„ Fallback Strategy:**
- **ğŸ“ Scoped Deployment**: Only for apps requiring immediate meshâ€‘level crossâ€‘cluster mTLS
- **ğŸ—ï¸ Architecture**: Cilium as CNI baseline, Istio for L7/mTLS and traffic shaping

**ğŸšª Exit Criteria to Revisit:**
- **ğŸ›¡ï¸ Cilium SPIRE Integration**: Endâ€‘toâ€‘end ClusterMesh support with single/federated trust domain
- **ğŸ“‹ Production Guidance**: Documented production-ready implementation
- **ğŸ”„ Migration Plan**: Pilot to remove appâ€‘level TLS and consolidate on Cilium mTLS |

### 22.4 ğŸ“‹ Migration & Validation Plan

| Phase | ğŸ”„ Step | âœ… Validation |
| :--- | :--- | :--- |
| **ğŸŒ Phase 1** | **Underlay Setup**<br>â€¢ Allocate PodCIDRs<br>â€¢ Enable BGP/static routes<br>â€¢ Verify podâ€‘toâ€‘pod ping | Crossâ€‘cluster L3 connectivity |
| **ğŸš€ Phase 2** | **Cilium Bootstrap**<br>â€¢ Bootstrap Cilium per cluster<br>â€¢ Expose ClusterMesh API server via LoadBalancer<br>â€¢ Create DNS records | ClusterMesh API accessibility |
| **ğŸ” Phase 3** | **Secret Distribution**<br>â€¢ ExternalSecrets sync `cilium-clustermesh` secret<br>â€¢ Validate secret store connectivity | ClusterMesh secret availability |
| **ğŸŒ Phase 4** | **Service Enablement**<br>â€¢ Enable Global Services (or MCS) for 1â€“2 anchor services<br>â€¢ Validate failover/locality behavior | Crossâ€‘cluster service discovery |
| **ğŸ›¡ï¸ Phase 5** | **Security Hardening**<br>â€¢ Enforce namespace defaultâ€‘deny<br>â€¢ Allow DNS + kubeâ€‘apiserver<br>â€¢ Add FQDN egress allowlists<br>â€¢ Require appâ€‘level TLS for crossâ€‘cluster flows | Policy enforcement and TLS validation |
| **ğŸ“Š Phase 6** | **Observability**<br>â€¢ Validate Hubble flows across clusters<br>â€¢ Add alerts for denied crossâ€‘cluster flows<br>â€¢ Monitor mesh peering health | Full visibility and monitoring |
| **ğŸ”„ Phase 7** | **Istio Fallback (if needed)**<br>â€¢ Deploy sidecar mesh to specific namespaces<br>â€¢ Export services<br>â€¢ Validate mTLS and traffic policies endâ€‘toâ€‘end | Istio integration validation |

Appendix A: Snippets

Kustomization with health checks
```yaml
spec:
  wait: true
  timeout: 10m
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: external-secrets
      namespace: external-secrets
```

## Appendix B: Consolidated Guides

### B.1 ğŸš€ Bootstrap Details (Phased Helmfile)

| Phase | ğŸ“¦ Components | ğŸ”§ Commands |
| :--- | :--- | :--- |
| **Phase 0**<br>*(CRDs only)* | cert-manager CRDs<br>external-secrets CRDs<br>victoria-metrics-operator CRDs bundle<br>prometheus-operator CRDs | `helmfile -f bootstrap/helmfile.d/00-crds.yaml -e <cluster> template \| yq ea 'select(.kind == "CustomResourceDefinition")' \| kubectl apply -f -` |
| **Phase 1**<br>*(Core)* | Cilium<br>CoreDNS<br>Spegel (optional)<br>cert-manager (crds disabled)<br>external-secrets (crds disabled)<br>Flux Operator + Flux Instance | `helmfile -f bootstrap/helmfile.d/01-core.yaml.gotmpl -e <cluster> sync` |

### B.2 ğŸ”§ Cilium Bootstrap Fixes (Summary)

| Issue | ğŸ”§ Solution | âœ… Verification |
| :--- | :--- | :--- |
| **ğŸ—ï¸ Controller Conflicts** | Install Cilium core by Helmfile; manage dayâ€‘2 via Flux | Avoids controller conflict and ordering issues |
| **ğŸŒ Dayâ€‘2 Management** | BGP, gateway, clustermesh, IPAM via Flux | Clear separation of concerns |
| **âœ… Health Checks** | `kubectl -n kube-system get ds cilium` | Verify Cilium DaemonSet status |
| **âœ… Operator Status** | `kubectl -n kube-system get deploy cilium-operator` | Verify Cilium Operator deployment |

### B.3 ğŸ˜ CloudNativePG (CNPG) Quick Deployment Guide

| Configuration | ğŸ”§ Details | âœ… Verification |
| :--- | :--- | :--- |
| **ğŸ“ Operator Location** | Present in infra cluster | Storage classes defined by cluster-settings |
| **ğŸ—ï¸ Typical Cluster** | 3 instances; WAL 20Gi; data 80Gi | StorageClass `openebs-local-nvme` |
| **ğŸ” Health Checks** | `kubectl -n cnpg-system get deploy` | Verify CNPG operator deployment |
| **ğŸ“Š Cluster Status** | `kubectl -n cnpg-system get cluster` | Verify PostgreSQL cluster status |

### B.4 ğŸ“Š Observability Strategy & Implementation

| Cluster | ğŸ”§ Stack Components | ğŸ“¡ Endpoints |
| :--- | :--- | :--- |
| **ğŸ­ Infra** | VictoriaMetrics Global (vmcluster) + VictoriaLogs + vmauth + Alertmanager + Grafana | Central storage and visualization |
| **ğŸ¯ Apps** | vmagent + kube-state-metrics + node-exporter + Fluent Bit | Lightweight collection and forwarding |
| **ğŸ“ Remote Write** | `victoria-metrics-global-vminsert.observability.svc.cluster.local:8480` | Metrics forwarding endpoint |
| **ğŸ” Query** | `victoria-metrics-global-vmselect.observability.svc.cluster.local:8481` | Query and visualization endpoint |

### B.5 ğŸš€ Workload Notes

| Workload | ğŸ”§ Components | ğŸ›¡ï¸ Security & Management |
| :--- | :--- | :--- |
| **ğŸ¦Š GitLab (apps)** | CNPG pooler, Dragonfly, External Secrets | Reconciled by Flux |
| **ğŸ”‘ Keycloak (identity)** | Policy enforcement, egress control | Cilium policies; observability egress only; SPIFFE/mTLS in-cluster |

### B.6 ğŸ“‹ Historical RCA (Highlights)

| Issue | ğŸ”§ Resolution | âœ… Outcome |
| :--- | :--- | :--- |
| **ğŸŒ Cilium Ownership Conflicts** | Bootstrap split implementation | Clear ownership separation |
| **ğŸ“‹ CRD Timing Issues** | Phase 0 CRD installation | Proper dependency ordering |

### B.7 ğŸ—ï¸ Platform Infrastructure Implementation (Consolidated)

| Component | ğŸ“ Location | ğŸ“‹ Status |
| :--- | :--- | :--- |
| **âš™ï¸ Talos Configs** | Consolidated in relevant architecture sections | Integrated |
| **ğŸŒ Cilium IPAM/BGP** | Networking section | Actionable |
| **ğŸ” RBAC** | Security section | Defined |
| **ğŸ”„ CI/CD Templates** | Observability & Bootstrap sections | Streamlined |
| **ğŸ›¡ï¸ Network/Security Policies** | Consolidated into architecture | Centralized |

**ğŸ“ Implementation Notes:**
- Detailed examples have been consolidated into relevant architecture sections
- Remaining deep dive templates intentionally omitted to keep document actionable
- **Live manifests under `kubernetes/` are the source of truth** |

HelmRelease (OCI-based chart)
```yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
spec:
  chart:
    spec:
      chart: harbor
      version: 1.16.0
      sourceRef:
        kind: HelmRepository
        name: harbor
        namespace: flux-system
  values: { ... }
```
