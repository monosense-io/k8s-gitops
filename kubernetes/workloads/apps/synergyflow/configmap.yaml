---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/v1/configmap.json
apiVersion: v1
kind: ConfigMap
metadata:
  name: synergyflow-config
  namespace: synergyflow
  labels:
    app.kubernetes.io/name: synergyflow
    app.kubernetes.io/component: application
    app.kubernetes.io/part-of: synergyflow
data:
  application.yml: |
    server:
      port: 8080
      shutdown: graceful
      compression:
        enabled: true
        mime-types: application/json,application/xml,text/html,text/xml,text/plain

    spring:
      application:
        name: synergyflow

      datasource:
        url: jdbc:postgresql://synergyflow-pooler-rw.cnpg-system.svc.cluster.local:5432/synergyflow
        username: ${DB_USERNAME}
        password: ${DB_PASSWORD}
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
          leak-detection-threshold: 60000

      jpa:
        hibernate:
          ddl-auto: none
        show-sql: false
        open-in-view: false
        properties:
          hibernate:
            dialect: org.hibernate.dialect.PostgreSQLDialect
            format_sql: false
            use_sql_comments: false
            jdbc:
              batch_size: 20
              time_zone: UTC

      kafka:
        bootstrap-servers: synergyflow-kafka-bootstrap.messaging.svc.cluster.local:9092
        security:
          protocol: SASL_PLAINTEXT
        properties:
          sasl:
            mechanism: SCRAM-SHA-512
            jaas:
              config: ${KAFKA_SASL_JAAS_CONFIG}
          schema:
            registry:
              url: http://schema-registry.messaging.svc.cluster.local:8081

        producer:
          key-serializer: org.apache.kafka.common.serialization.StringSerializer
          value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
          acks: all
          retries: 3
          properties:
            linger.ms: 10
            compression.type: snappy
            max.in.flight.requests.per.connection: 5
            enable.idempotence: true

        consumer:
          key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
          value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
          auto-offset-reset: earliest
          enable-auto-commit: false
          properties:
            specific.avro.reader: true
            isolation.level: read_committed

      cache:
        type: redis
        redis:
          time-to-live: 3600000

      data:
        redis:
          host: dragonfly.dragonfly.svc.cluster.local
          port: 6379
          password: ${REDIS_PASSWORD}
          timeout: 2000ms
          lettuce:
            pool:
              max-active: 20
              max-idle: 10
              min-idle: 5
              max-wait: 2000ms

    # Flowable configuration
    flowable:
      process:
        definition-cache-limit: 100
        enable-safe-xml-processing: true
        async-executor-activate: true
        async-executor-core-pool-size: 4
        async-executor-max-pool-size: 8
        async-executor-queue-size: 100

      database-schema-update: true
      database-schema: flowable

      # Enable REST API for Flowable
      rest:
        enabled: true
        api-enabled: true
        app-enabled: false

      # Metrics
      enable-process-definition-info-cache: true

    # OPA client configuration
    opa:
      url: http://localhost:8181/v1/data
      timeout: 100ms
      retry:
        max-attempts: 2
        backoff: 50ms

    # Observability
    management:
      endpoints:
        web:
          exposure:
            include: health,info,metrics,prometheus
          base-path: /actuator
      endpoint:
        health:
          show-details: when-authorized
          probes:
            enabled: true
      metrics:
        export:
          prometheus:
            enabled: true
      health:
        livenessstate:
          enabled: true
        readinessstate:
          enabled: true

    # Logging
    logging:
      level:
        root: INFO
        io.monosense.synergyflow: DEBUG
        org.flowable: INFO
        org.springframework.kafka: INFO
        org.apache.kafka: WARN
      pattern:
        console: "%d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg%n"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: opa-policy
  namespace: synergyflow
  labels:
    app.kubernetes.io/name: opa
    app.kubernetes.io/component: policy
    app.kubernetes.io/part-of: synergyflow
data:
  policy.rego: |
    package synergyflow.authz

    # Default deny
    default allow = false

    # Allow if user has required permission
    allow {
      input.user.roles[_] == required_role
      required_permission
    }

    # Role hierarchy
    admin_roles := {"SYSTEM_ADMIN", "TENANT_ADMIN"}
    agent_roles := {"AGENT", "SENIOR_AGENT", "TEAM_LEAD"}
    user_roles := {"END_USER"}

    # Permission checks
    required_permission {
      input.action == "incident.create"
      input.user.roles[_] == agent_roles[_]
    }

    required_permission {
      input.action == "incident.update"
      incident_owner_or_agent
    }

    required_permission {
      input.action == "incident.delete"
      input.user.roles[_] == admin_roles[_]
    }

    # Helper rules
    incident_owner_or_agent {
      input.resource.owner == input.user.id
    }

    incident_owner_or_agent {
      input.user.roles[_] == agent_roles[_]
    }

    required_role = role {
      role := admin_roles[_]
    }

    required_role = role {
      role := agent_roles[_]
    }
